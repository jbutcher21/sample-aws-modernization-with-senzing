[
{
	"uri": "//localhost:1313/1_introduction/index.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Workshop Overview Welcome to the One-click Entity Resolution workshop! This workshop teaches you to use AI prompts and agentic workflows to streamline the entire entity resolution process - from mapping source data, to running Senzing utilities, to analyzing resolution results through conversational AI.\nYou\u0026rsquo;ll work with realistic data examples and use tools like Amazon Q Developer and the Senzing MCP Server to experience how AI can transform hours of manual work into minutes of guided interaction.\nLearning Outcomes By completing this workshop, you will know how to:\nMap your real source data to Senzing using the 5-stage Senzing Mapping Assistant workflow Get answers to your mapping questions and learn best practices through AI Load data into Senzing and take snapshots for analysis Use conversational AI to explore entity resolved data Validate that everything is working correctly at each step Intended Audience This workshop is designed for:\nSoftware developers and data engineers Technical professionals working with data integration and ETL workflows Anyone interested in AI-assisted data transformation Anyone interested in entity resolution Prerequisites:\nBasic familiarity with IDEs (Visual Studio Code or code-server) Understanding of data formats (CSV, JSON) Experience with data transformation concepts Prior Senzing experience is not required Workshop Conventions Throughout this workshop, you\u0026rsquo;ll encounter various visual elements:\nTips provide helpful suggestions and best practices.\nInfo boxes highlight important information or context.\nWarnings alert you to common pitfalls or important considerations.\nCode Blocks:\n# Command-line instructions look like this python3 mapper.py input.csv -o output.jsonl Validation Checkpoints: ✅ Green checkmarks indicate validation steps to confirm your progress\n"
},
{
	"uri": "//localhost:1313/scratch.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Overview What is entity resolutiuon?\nshows the entity resolution image here.\nThe data mapping challenge: Data mapping is the process of translating data from a source schema to a target schema.\nSource Schema: Your organization\u0026rsquo;s data (customer records, watchlist data, etc.) Target Schema: Senzing\u0026rsquo;s entity resolution format The Challenge: Understanding how your source attributes map to Senzing\u0026rsquo;s standardized format Overview Specifically, you will be able to:\nUnderstand the Senzing entity resolution data model and requirements Use AI tools (Amazon Q Developer) to assist with data mapping workflows Analyze your source data and what it contains Apply best practices for production data mapping projects Create mapping logic that transforms source data to Senzing format Load mapped data into Senzing and validate the results Prerequisites Basic understanding of data formats (CSV, JSON) Basic Python knowledge (helpful but not required) Basic understanding of AWS core services AWS account with appropriate permissions (provided in AWS-led event) Familiarity with using an IDE (preferably code-server) Familiarity with using an AI assistant Familiarity with command-line interfaces What You\u0026rsquo;ll Build Throughout this workshop, you will create a complete, production-ready data mapping pipeline:\nSchema analysis documents profiling your source data Mapping specifications documenting field-by-field decisions Python mapper code that transforms CSV to Senzing JSON format Validated entity data ready to load into Senzing Entity resolution results showing how Senzing identifies duplicates Workshop Structure This workshop is divided into the following modules:\nModule Duration Description Module 1: Introduction 10 minutes Workshop overview and learning objectives Module 2: Setup \u0026amp; Configuration 15 minutes Access your development environment and configure Amazon Q Module 3: Understanding Senzing Mapping 30 minutes Learn the Senzing data model and mapping workflow Module 4: Hands-On - Map Customer Data 60 minutes Use AI to map customer records to Senzing format Module 5: Load and Validate 30 minutes Load your mapped data and see entity resolution in action Module 6: Bonus - Watchlist Mapping 45 minutes Optional: Map complex international watchlist data Module 7: Cleanup and Next Steps 10 minutes Clean up resources and explore what\u0026rsquo;s next Total Time: Approximately 2.5-3 hours (core modules 1-5)\nWorkshop Modules Module 1: Introduction Get oriented with the workshop goals, structure, and the data mapping challenge you\u0026rsquo;ll solve.\nModule 2: Setup and Configuration Access your development environment and authenticate with Amazon Q Developer for both the IDE and CLI.\nModule 3: Understanding Senzing Mapping Learn the Senzing entity resolution data model, the difference between Features and Payload attributes, and the 5-stage mapping workflow.\nModule 4: Hands-On Customer Mapping ⭐ Core Module Apply what you learned: analyze customer data, make mapping decisions with AI guidance, generate mapper code, and validate your output.\nModule 5: Load and Validate See your mapping in action by loading data into Senzing and exploring entity resolution results.\nModule 6: Bonus - Watchlist Mapping (Optional) Challenge yourself with more complex data including nested structures, international names, and entity relationships.\nModule 7: Cleanup and Next Steps Clean up your AWS resources and learn how to apply these skills to your own data.\nGetting Started Ready to begin? Here\u0026rsquo;s what happens next:\nRead Module 1 to understand the context and learning objectives Complete Module 2 to set up your development environment Learn the concepts in Module 3 before diving into hands-on work Map customer data in Module 4 using AI assistance Validate your work in Module 5 by loading into Senzing Self-Paced Approach: This workshop is designed for self-paced learning. Take your time, use the validation checkpoints, and reference the complete solutions if you get stuck.\nWorkshop Conventions Throughout this workshop, you\u0026rsquo;ll encounter various visual elements to help guide your learning:\nTips provide helpful suggestions and best practices.\nInfo boxes highlight important information or context.\nWarnings alert you to common pitfalls or important considerations.\nCode Blocks:\n# Command-line instructions look like this python3 mapper.py input.csv -o output.jsonl Validation Checkpoints: ✅ Green checkmarks indicate validation steps to confirm your progress\nSupport and Resources Workshop Support During Instructor-Led Events:\nAsk your instructor or workshop facilitators Use the event chat or Q\u0026amp;A features For Self-Paced Learners:\nReview the complete solutions provided in each module Check the troubleshooting sections Consult the Senzing documentation links below Senzing Resources Senzing Documentation: https://senzing.com/docs/ Senzing GitHub: https://github.com/senzing Senzing Community Support: https://senzing.com/support/ AWS Resources Amazon Q Developer Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/ AWS Builder ID: https://profile.aws.amazon.com/ AWS Workshop Studio: https://workshops.aws/ Goals By the end of this workshop, you will have:\nPractical Skills:\nMapped real data to Senzing format using AI assistance Created production-ready mapper code Validated your mappings with professional tools Deep Understanding:\nKnow the Senzing entity resolution data model Understand Features vs Payload attributes Recognize common mapping patterns and best practices Repeatable Process:\nLearned the 5-stage Senzing Mapping Assistant workflow Can apply this workflow to any data source Know how to validate and iterate on your mappings The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\nReady to get started? Let\u0026rsquo;s begin with Module 1: Introduction!\nafter setup there should be Modules for\nSome sort of the problem with mapping and why this was built? Exercise Workflow Exercise 1 - mapping the customer file Exercise 2 - mapping the watchlist file some sort of wrap up In each exercise we will have the same tasks (the overview will briefly describe the steps and tools)\nExamine the source data Provide or create the schema Use the senzing mapping assistant to map it Validate the mapping with the json analyzer Load the data in Senzing Take a snaphot Analyze the snapshot for stats and examples Review the examples with the mcp server by asking how and why questions Source Data (Your Format):\nCSV with columns like: customer_id, customer_name, dob, address, email\u0026hellip; May have organization-specific field names Mixed data quality and completeness Target Data (Senzing Format):\nJSON with standardized features: NAME, DATE_OF_BIRTH, ADDRESS, EMAIL\u0026hellip; Specific attribute names (NAME_FIRST, NAME_LAST, ADDR_LINE1\u0026hellip;) Clear distinction between matching features and payload data The Challenge Knowing which source fields map to which Senzing features, and how to transform the data correctly.\nFor each source field, you must decide:\nShould it be a Feature (used for entity matching)? Should it be Payload (stored but not matched)? Which specific Senzing attribute name is correct? Does the data need transformation (date formats, name parsing, etc.)? Example decisions:\nYour CSV has customer_name - should this map to NAME_FULL? Or split into NAME_FIRST and NAME_LAST? You have id_number - is this an SSN? Driver\u0026rsquo;s License? Passport? Your registration_date - is this legal entity registration or customer signup date? (Very different meanings!) Traditional Manual Approach Without AI assistance, data mapping typically requires:\nReading specifications - Understanding hundreds of Senzing features and attributes Making decisions - Determining appropriate mappings for each field Writing code - Building transformation logic Manual validation - Checking outputs meet requirements Iterative debugging - Finding and fixing errors Time cost: 3-6 days for a typical dataset with 15-20 fields\nThe AI-Assisted Solution This workshop teaches you to use the Senzing Mapping Assistant - a structured AI workflow that:\n✅ Reads specifications instantly - AI loads the complete Senzing spec into context ✅ Suggests mappings - AI recommends appropriate features based on your data ✅ Generates code - AI writes the mapper automatically ✅ Creates documentation - Mapping decisions documented as byproduct\nTime savings: Same work in under an hour\nImportant Caveat: AI is your assistant, not a replacement for your judgment. YOU still need to:\nUnderstand what your source fields mean Make business decisions about what should influence matching Validate that AI suggestions align with your requirements Correct AI when it misunderstands your data What You\u0026rsquo;ll Learn In this workshop, you\u0026rsquo;ll learn:\nThe Senzing entity model (Features vs Payload, entity types) How to work with AI to make mapping decisions A repeatable 5-stage workflow for any dataset Professional validation tools to verify correctness How to load and analyze entity resolution results Ready to understand Senzing\u0026rsquo;s data model?\nThe 5 Stages STAGE 1: INIT - Load Reference Materials You load the Senzing specifications and reference materials into the AI context. This gives the AI the knowledge it needs to guide you through mapping.\nWhat happens:\nLoad entity specification Load mapping examples Load validation tools Load identifier and usage type crosswalks Your role: Copy prompts to load files into AI context\nSTAGE 2: INVENTORY - Analyze Source Data The AI analyzes your source data schema to understand what fields you\u0026rsquo;re working with.\nWhat happens:\nAI reviews pre-generated schema file Identifies field names, data types, population rates Notes patterns and potential issues Your role: Provide schema file (generated by sz_schema_generator.py)\nSTAGE 3: PLANNING - Identify Entities and Sources You work with the AI to determine:\nWhat DATA_SOURCE value to use Are there multiple entity types in your data? Any special handling needed? What happens:\nAI asks clarifying questions You make business decisions Establish mapping strategy Your role: Answer questions about your data and business context\nSTAGE 4: MAPPING - Field-by-Field Decisions This is the core mapping work. For each source field, you and the AI determine:\nShould it be a Senzing Feature (used for matching)? Should it be Payload (stored but not matched)? Should it be ignored? What is the correct Senzing attribute name? What happens:\nAI suggests mappings based on field names and content You confirm or override decisions Handle edge cases (e.g., dynamic identifier types) Decide on data transformations (date formats, name parsing) Your role: Make informed decisions with AI guidance\nSTAGE 5: OUTPUTS - Generate Code and Documentation The AI generates production-ready deliverables:\nOutputs:\nMapper specification (markdown documentation of all decisions) Python mapper (executable code) Sample output (example mapped JSON records) README (usage instructions) What happens:\nAI writes the mapper based on Stage 4 decisions You run the mapper Validate output with linting and analysis tools Your role: Review generated code, run mapper, validate output\nWhy This Workflow Works Structured Decision-Making: Each stage has a clear purpose and deliverable\nValidation Gates: You verify correctness at each stage before proceeding\nIterative Refinement: Easy to go back and adjust decisions\nDocumentation Trail: Complete record of why each mapping decision was made\nProduction Ready: Output includes code, documentation, and validation\nThis isn\u0026rsquo;t just code generation - it\u0026rsquo;s AI-assisted decision-making with you in control. The AI guides you through the specifications and best practices, but YOU make the final decisions based on your understanding of the data.\nThe Full Prompt The complete Senzing Mapping Assistant prompt with detailed instructions for each stage is available at:\nworkshop/senzing/prompts/senzing_mapping_assistant.md\nYou\u0026rsquo;ll load this prompt in Module 4 when you start the hands-on customer mapping exercise.\nWhat to Expect in Module 4 In the next module, you\u0026rsquo;ll:\nLoad the Mapping Assistant into Amazon Q Developer Work through all 5 stages for the customer dataset Make real mapping decisions with AI guidance Generate a working mapper Validate your output with linting and analysis tools Estimated time: 60 minutes for complete customer mapping\nReady to learn about the validation tools you\u0026rsquo;ll use?\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/index.html",
	"title": "Setup and Configuration",
	"tags": [],
	"description": "",
	"content": "Setup and Configuration Overview Welcome to the setup phase! In this module, you\u0026rsquo;ll access your pre-configured cloud development environment and connect to the AI tools you\u0026rsquo;ll use throughout the workshop.\nAll software, tools, and workshop materials are already installed in your cloud environment - you just need to authenticate and verify everything is ready.\nSetup Steps In this module you will:\nJoin the AWS Workshop Event - Access Workshop Studio with your event code Access Your Cloud IDE - Log into your pre-configured code-server environment Authenticate Amazon Q Developer - Connect to AI assistance with AWS Builder ID Configure Senzing MCP Server - Enable AI-powered entity resolution queries Review Resources - Bookmark important documentation and support links Let\u0026rsquo;s get started with joining the workshop event!\n"
},
{
	"uri": "//localhost:1313/3_workshopoverview/index.html",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Overview Overview Before diving into hands-on mapping, let\u0026rsquo;s understand the mapping challenge, explore Senzing\u0026rsquo;s AI-assisted solution, and tour the workshop materials.\nThis module prepares you with the context needed for successful data mapping in the hands-on exercises.\nWhat You\u0026rsquo;ll Learn In this brief module, you will:\nUnderstand the Data Mapping Challenge - What mapping is and why it\u0026rsquo;s traditionally difficult Learn Senzing\u0026rsquo;s AI Assisted Solution - The 5-stage Senzing Mapping Assistant workflow Tour the Workshop Folder - Understanding where everything is located Explore with AI - Practice using AI to understand workshop materials Estimated Reading Time: 10-15 minutes\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/index.html",
	"title": "Exercise 1: Customers",
	"tags": [],
	"description": "",
	"content": "Exercise 1: Customers Map and Load Customer Data Overview In this exercise, you\u0026rsquo;ll map customer data from CSV format to Senzing\u0026rsquo;s entity resolution format using AI assistance. You\u0026rsquo;ll then load the data into Senzing and analyze the results to see how entity resolution identifies duplicate records.\nWhat You\u0026rsquo;ll Learn By completing this exercise, you will:\nExamine source data to understand its structure Use AI to generate data schemas automatically Apply the Senzing Mapping Assistant workflow to create a mapper Validate your mapping with analysis tools Load data into Senzing for entity resolution Take a snapshot and analyze resolution statistics Use the MCP server to understand resolution decisions Exercise Steps This exercise follows a structured 8-step workflow:\nExamine the source data - Understand what data you\u0026rsquo;re working with Provide or create the schema - Document the data structure Use the Senzing Mapping Assistant - AI-guided mapping workflow Validate the mapping - Verify output with JSON analyzer Load the data in Senzing - Import mapped records Take a snapshot - Capture resolution results Analyze the snapshot - Review statistics and examples Review with MCP server - Ask how and why questions Estimated Time: 30-45 minutes\nComplete each step before moving to the next. Each step builds on the previous one.\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "//localhost:1313/99_unused/4_handsonmapping/index.html",
	"title": "Hands-On: Map Customer Data with AI",
	"tags": [],
	"description": "",
	"content": "Hands-On: Map Customer Data with AI Overview In this module, you\u0026rsquo;ll use AI (Amazon Q Developer) to map the customer dataset to Senzing format using the Senzing Mapping Assistant workflow. This is a fully guided, hands-on exercise with complete validation at every step.\nWhat You\u0026rsquo;ll Do Load the Mapping Assistant - Initialize the AI-powered mapping workflow Analyze Source Schema - Let AI inventory all fields in customers.csv Plan the Mapping - Determine DATA_SOURCE and entity types Map Each Field - Decide which fields are Features vs Payload Generate Outputs - Create mapper code, documentation, and mapped data Validate Results - Use Senzing tools to verify the mapping Workshop Materials All materials are in your Cloud9 environment under workshop/:\nSource Data:\nworkspace/customers/customers.csv - 120 customer records (114 persons, 6 organizations) workspace/solutions/customers/customers_schema.md - Pre-generated schema analysis Senzing Resources:\nsenzing/prompts/senzing_mapping_assistant.md - The AI mapping workflow senzing/reference/ - Entity specification, examples, crosswalks, linter senzing/tools/ - Schema generator, JSON analyzer Complete Solution (for reference):\nsolutions/customers/ - All completed mapping files The Mapping Workflow You\u0026rsquo;ll follow the Senzing Mapping Assistant v4 5-stage process:\nSTAGE 1: INIT - Load reference files and verify tools STAGE 2: INVENTORY - Extract all source fields STAGE 3: PLANNING - Identify entities and DATA_SOURCE codes STAGE 4: MAPPING - Map each field with AI guidance STAGE 5: OUTPUTS - Generate mapper script and documentation AI-Assisted but Human-Driven: The AI guides you through decisions, but YOU make the final calls. When the AI asks questions, provide clear answers based on your understanding of the data.\nVideo Demonstration: A complete walkthrough video shows the entire mapping process from start to finish, including all prompts, decisions, and validation steps.\nKey Learning Objectives By the end of this module, you will:\n✅ Understand the 5-stage AI-assisted mapping workflow ✅ Know how to disposition fields as Features vs Payload ✅ Learn to resolve naming conflicts (e.g., CUSTOMER_SINCE_DATE) ✅ Generate production-ready mapper code ✅ Validate mapped data with Senzing tools ✅ Interpret sz_json_analyzer output Expected Outcomes Input:\ncustomers.csv (120 records, 19 fields) Output:\ncustomers_senzing.jsonl (120 records, fully validated) customers_mapper.py (Python mapper) Complete mapping documentation Validation:\n✅ Linter passes (0 errors) ✅ All features recognized by Senzing ✅ Payload attributes properly separated Ready? Let\u0026rsquo;s start mapping!\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/index.html",
	"title": "Exercise 2: Watchlist",
	"tags": [],
	"description": "",
	"content": "Exercise 2: Watchlist Map and Load Watchlist Data Overview In this exercise, you\u0026rsquo;ll map watchlist data from JSON format (FollowTheMoney schema) to Senzing\u0026rsquo;s entity resolution format. Watchlist data is more complex than customer data, featuring international names, relationships, and sanctions information.\nWhat You\u0026rsquo;ll Learn By completing this exercise, you will:\nWork with JSON source data (vs CSV from Exercise 1) Handle complex nested data structures Map international names and identifiers Process entity relationships Combine watchlist data with existing customer data Analyze cross-dataset entity resolution Exercise Steps This exercise follows the same 8-step workflow as Exercise 1:\nExamine the source data - Understand watchlist JSON structure Provide or create the schema - Document the data structure Use the Senzing Mapping Assistant - AI-guided mapping workflow Validate the mapping - Verify output with JSON analyzer Load the data in Senzing - Import mapped records Take a snapshot - Capture resolution results Analyze the snapshot - Review statistics and examples Review with MCP server - Ask how and why questions Estimated Time: 30-45 minutes\nWatchlist data presents new challenges: nested JSON structures, international names, and relationships. Apply what you learned in Exercise 1!\nThis exercise builds on Exercise 1. You\u0026rsquo;ll see how watchlist entities resolve against customer data.\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "//localhost:1313/99_unused/5_loadandvalidate/index.html",
	"title": "Load and Validate Mapped Data",
	"tags": [],
	"description": "",
	"content": "Load and Validate Mapped Data Overview Now that you\u0026rsquo;ve successfully mapped your customer data to Senzing format, it\u0026rsquo;s time to load it into Senzing and see entity resolution in action! This module shows you how to register your data source, load the mapped data, and explore the resolved entities.\nWhat You\u0026rsquo;ll Do Register DATA_SOURCE - Add \u0026ldquo;CUSTOMERS\u0026rdquo; to Senzing configuration Load Mapped Data - Import customers_senzing.jsonl into Senzing Take a Snapshot - Create a point-in-time view of resolved entities Explore Results - See which records resolved into entities Analyze Matches - Understand why entities matched Prerequisites Before starting this module, you should have:\n✅ Completed Module 4 (Customer mapping) ✅ File: workspace/customers/customers_senzing.jsonl (120 records) ✅ Validation: Passed linter and sz_json_analyzer Catch-Up Option: If you didn\u0026rsquo;t complete Module 4, copy the solution:\ncp solutions/customers/customers_senzing.jsonl workspace/customers/ The Senzing Tools You\u0026rsquo;ll use these Senzing SDK tools:\nTool Purpose What It Does sz_configtool Configuration Add/modify data sources and features sz_file_loader Data Loading Import JSONL files into Senzing sz_snapshot Export Create JSON snapshot of resolved entities sz_explorer Exploration Interactive UI to browse entities Expected Results After loading 120 customer records, you should see:\nEntity Count: Fewer than 120 (some duplicates should resolve) Key Resolutions: The 3 \u0026ldquo;Mooney, Susan\u0026rdquo; records → Should resolve to 1-2 entities Records with matching identifiers → Should resolve Records with similar names + addresses → May resolve Learning Objectives By the end of this module, you will:\n✅ Understand how to configure Senzing for new data sources ✅ Know how to load mapped data into Senzing ✅ Be able to explore resolved entities ✅ Understand entity resolution match rules ✅ Interpret why records matched or didn\u0026rsquo;t match Validation Checkpoints Checkpoint 1: Configuration\n✅ DATA_SOURCE \u0026ldquo;CUSTOMERS\u0026rdquo; registered ✅ No configuration errors Checkpoint 2: Loading\n✅ 120 records loaded successfully ✅ No load errors reported Checkpoint 3: Resolution\n✅ Entities created (\u0026lt; 120) ✅ Expected duplicates resolved ✅ Match keys visible in snapshot Using AI: You can ask Amazon Q Developer to help you run these commands and interpret the results. The AI can explain what the output means and help troubleshoot any issues.\nPLACEHOLDER: Detailed step-by-step instructions will be added based on demonstration video, including:\nExact commands to run Expected outputs at each step How to interpret results Troubleshooting common issues Ready to load your data and see entity resolution in action?\n"
},
{
	"uri": "//localhost:1313/99_unused/6_bonuswatchlist/index.html",
	"title": "Bonus: Map Watchlist Data",
	"tags": [],
	"description": "",
	"content": "Bonus: Map Watchlist Data Overview Ready for a challenge? In this optional bonus module, you\u0026rsquo;ll map watchlist data in FollowTheMoney (FTM) format to Senzing. This is more complex than the customer mapping because it includes:\nNested structures - Properties stored as arrays Multiple entity types - Person, Company, Sanction, Ownership, Directorship Relationships - How entities connect to each other International names - Names in various scripts (Arabic, Cyrillic, Chinese) Why This Matters Real-world entity resolution often involves:\nWatchlist screening (sanctions, PEPs, adverse media) Corporate structures (ownership, directorships) Cross-border entities (multiple languages, naming conventions) Learning to map FTM format prepares you for these scenarios.\nThe Challenge Source Data:\nworkspace/watchlist/ftm.jsonl - 70 entities in FollowTheMoney format Pre-analyzed schema: solutions/watchlist/ftm_schema.md Entity Breakdown:\n33 Person entities 6 Company entities 17 Sanction relationships 8 Ownership relationships 6 Directorship relationships Complexity:\nProperties are nested arrays: {\u0026quot;properties\u0026quot;: {\u0026quot;name\u0026quot;: [\u0026quot;John Smith\u0026quot;]}} Multiple record types in one file Relationships reference other entity IDs International character encoding What You\u0026rsquo;ll Do Analyze FTM Structure - Understand the nested format Map Entities - Person and Company entities first Map Relationships - Sanction, Ownership, Directorship Flatten Arrays - Extract values from FTM property arrays Handle References - Deal with entity relationships Validate - Ensure all 70 entities mapped correctly Using the Mapping Assistant The same Senzing Mapping Assistant workflow applies:\nSTAGE 1: INIT - Load reference files (already done) STAGE 2: INVENTORY - Analyze ftm_schema.md STAGE 3: PLANNING - Multiple DATA_SOURCEs? How to handle relationships? STAGE 4: MAPPING - Map each FTM property to Senzing features STAGE 5: OUTPUTS - Generate watchlist mapper Expected Outcomes Input:\nftm.jsonl (70 entities) Output:\nSenzing JSONL with all entities and relationships mapped May need separate handling for relationship records International names preserved and searchable Validation After mapping, you should see:\n✅ 70 records mapped (or more if relationships become separate records) ✅ All person/company features recognized ✅ Sanction data preserved as payload ✅ Relationships properly encoded Start Simple: Map just the Person and Company entities first. Add relationships as a second step.\nComplete Solution Available: Check solutions/watchlist/ for reference if you get stuck.\nPLACEHOLDER: Detailed step-by-step instructions will be added based on demonstration video. This is an advanced exercise - attempt only after completing Module 4 successfully.\nReady for the challenge?\n"
},
{
	"uri": "//localhost:1313/7_cleanup/index.html",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources Overview If you ran this workshop in your own AWS account, it\u0026rsquo;s important to clean up the resources you created to avoid ongoing charges.\nResources to Clean Up Depending on which parts of the workshop you completed, you may have created the following resources:\nAmazon Q Developer configurations (no charges) Any AWS services used during the workshop exercises Cleanup Steps 1. Amazon Q Developer Amazon Q Developer authentication and IDE configurations don\u0026rsquo;t incur charges, but you can disconnect if desired:\nIn VS Code, you can sign out of Amazon Q through the extension settings CLI authentication can be cleared with q logout (if available) 2. AWS Resources If you created any AWS resources during the workshop:\nReview your AWS Console for any resources created during the workshop Delete resources in the reverse order of creation to avoid dependency issues Check for any CloudFormation stacks that may have been created Verify S3 buckets are empty before deletion Verification To ensure all resources are cleaned up:\nCheck your AWS billing dashboard for any unexpected charges Review the AWS Cost Explorer for resource usage during the workshop period Set up billing alerts if you haven\u0026rsquo;t already Most of this workshop focuses on Amazon Q Developer setup and configuration, which doesn\u0026rsquo;t create billable AWS resources. However, always verify your account to ensure no unexpected resources remain.\nNeed Help? If you\u0026rsquo;re unsure about any resources or cleanup steps:\nReview the AWS documentation for the specific services you used Contact AWS Support if you have questions about billing or resource cleanup Check the AWS forums for community assistance "
},
{
	"uri": "//localhost:1313/getting-started.html",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Workshop architecture The following architecture diagram illustrates the various components of the workshop.\nPreparing for the workshop Follow the installation instructions in this section to prepare your environment for the workshop.\nIf you are attending an AWS guided event, setup your environment here. If you are not participating in an AWS guided event, setup your environment here. ::alert[If you are running this workshop on your own AWS account, remember to delete all resources by following the Clean Up Resources section to avoid unnecessary charges.]{header=Note}\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/21_aws_guided_event/index.html",
	"title": "Running the Workshop at an AWS Guided Event",
	"tags": [],
	"description": "",
	"content": "Running the Workshop at an AWS Guided Event AWS-Hosted Workshop Setup Follow these steps to complete your environment setup:\nStep Action Description 1 Join the Event Access AWS Workshop Studio with your event code 2 Access Cloud IDE Log into your pre-configured code-server environment with Senzing and workshop materials 3 Setup Amazon Q Developer Authenticate with Amazon Q Developer using AWS Builder ID 4 Configure Senzing MCP Connect Amazon Q to Senzing entity resolution tools 5 Resources \u0026amp; Support Important documentation links and workshop support "
},
{
	"uri": "//localhost:1313/2_setup_configuration/23_amazonqsetup.html",
	"title": "Amazon Q Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will authenticate the Amazon Q IDE extension using AWS Builder ID.\nThis workshop uses AWS Builder ID for authentication. Builder ID is free and doesn\u0026rsquo;t require an AWS account.\nDon\u0026#39;t have an AWS Builder ID? AWS Builder ID is a personal profile that provides access to select tools and services including Amazon CodeCatalyst, Amazon Q Developer, and AWS Training and Certification. AWS Builder ID is free and you don\u0026rsquo;t need to enter any credit card details upon creation of a profile. For more information, refer to the documentation.\nTo create a profile:\nEnter your email address -\u0026gt; Next. Enter your name -\u0026gt; Next. Check your email for a verification code (subject: \u0026ldquo;Verify your AWS Builder ID email address\u0026rdquo;) Enter the verification code -\u0026gt; Verify Create a password -\u0026gt; Create AWS Builder ID Authentication Setup In the Amazon Q extension, select Use for free → Continue. When prompted, select Proceed to Browser When asked \u0026ldquo;Do you want Code (or code-sever) to open the external website?\u0026rdquo;, choose Open. In the browser: You will be redirected to the AWS Builder ID Login flow. Login with your credentials.\nConfirm the pre-populated code by clicking Confirm and continue.\nSelect Allow access when prompted\nYou\u0026rsquo;ll see a \u0026ldquo;Request approved\u0026rdquo; confirmation\nReturn to your IDE\nWrap up Checkpoint: Verify that Amazon Q shows you\u0026rsquo;re authenticated before proceeding.\nYou\u0026rsquo;ve successfully authenticated Amazon Q Developer in your IDE. You\u0026rsquo;re now ready to configure the Senzing MCP Server!\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/25_senzingmcpsetup/index.html",
	"title": "Senzing MCP Server Setup",
	"tags": [],
	"description": "",
	"content": "Overview The Senzing MCP (Model Context Protocol) Server provides Amazon Q Developer with direct access to Senzing entity resolution capabilities. This integration allows you to interact with your Senzing database using natural language queries through Amazon Q.\nIn this section, you will:\nConfigure the pre-installed Senzing MCP server in Amazon Q Developer Authorize MCP tools for Amazon Q to use Verify the integration is working The MCP server provides 8+ tools for entity search, relationship analysis, and resolution explanation. This makes it easy to explore and understand entity resolution results using conversational AI.\nPrerequisites Amazon Q Developer authenticated (configured in previous section) Senzing SDK installed and initialized in your environment (pre-configured in workshop) Senzing MCP server add on pre-installed (pre-configured in workshop) What You\u0026rsquo;ll Learn By the end of this section, you\u0026rsquo;ll have:\nConnected Amazon Q Developer to the Senzing MCP server Authorized Senzing tools for use in your AI chat interface Verified the integration works with test queries Gained the ability to query entity resolution results conversationally Benefits of MCP Integration Throughout this workshop, you\u0026rsquo;ll use this integration to:\nQuery entity resolution results conversationally Understand how Senzing identified duplicate entities Analyze relationships between resolved entities Get explanations of resolution decisions Explore your mapped data interactively Getting Started Let\u0026rsquo;s begin by configuring the MCP server connection!\nThe setup process takes approximately 5-10 minutes. Follow each step carefully for a smooth experience.\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/27_resourcesandsupport.html",
	"title": "Resources and Support",
	"tags": [],
	"description": "",
	"content": "Resources and Support Now that your environment is configured, here are important resources you can reference throughout the workshop.\nWorkshop Support During Instructor-Led Events Ask your instructor or workshop facilitators Use the event chat or Q\u0026amp;A features For Self-Paced Learners Review the complete solutions provided in each module Check the troubleshooting sections within each module Consult the Senzing and AWS documentation links below All workshop modules include validation checkpoints. If you encounter issues, check that you\u0026rsquo;ve completed each validation step before proceeding.\nSenzing Resources Senzing Documentation: https://senzing.com/docs/ Senzing GitHub: https://github.com/senzing Senzing Community Support: https://senzing.com/support/ The Senzing documentation includes:\nComplete entity specification reference API documentation for the Senzing SDK Integration guides for various platforms Best practices for entity resolution AWS Resources Amazon Q Developer Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/ AWS Builder ID: https://profile.aws.amazon.com/ AWS Workshop Studio: https://workshops.aws/ Amazon Q Developer provides:\nCode generation and analysis Chat-based assistance for development tasks Integration with your IDE Access to AWS documentation and best practices You\u0026rsquo;ll use Amazon Q Developer extensively in Module 4 when mapping customer data. Familiarize yourself with its chat interface in your IDE.\nNext Steps With your environment configured and resources bookmarked, you\u0026rsquo;re ready to learn about Senzing\u0026rsquo;s entity resolution model and the mapping workflow.\nContinue to Module 3: Understanding Senzing Mapping to build the conceptual foundation before hands-on work.\n"
},
{
	"uri": "//localhost:1313/3_workshopoverview/30_mappingchallenge.html",
	"title": "What is Data Mapping",
	"tags": [],
	"description": "",
	"content": "What is Data Mapping? Data mapping is the process of defining how data fields from a source system correspond to fields in a target system.\nWhy Data Mapping is Hard Data mapping projects face several common challenges:\nComplex schemas on both sides\nSource systems may have dozens or hundreds of fields with unclear naming conventions Target systems (like Senzing) have their own complex specifications with specific requirements Understanding how fields relate across systems requires deep knowledge of both Subject matter experts are scarce\nPeople who understand the source data are often too busy with operational work Original developers or data architects may have moved on to other projects Institutional knowledge about field meanings and business rules is hard to capture Takes a long time to understand\nReading through data dictionaries and specifications is tedious Testing mappings to verify correctness requires multiple iterations Edge cases and data quality issues only emerge during implementation Documentation is often incomplete or outdated "
},
{
	"uri": "//localhost:1313/99_unused/31_aiassistedsolution.html",
	"title": "AI-Assisted Solution",
	"tags": [],
	"description": "",
	"content": "Overcoming Mapping Challenges with AI The challenges of data mapping - complex schemas, scarce expertise, and time-consuming processes - can be dramatically reduced by combining AI assistance with a proven workflow.\nHow AI Helps AI reads the complex specifications for you\nInstead of you spending hours reading through Senzing\u0026rsquo;s 2,100+ line entity specification, AI can load and understand the entire document instantly AI has knowledge of all Senzing features, attributes, and best practices in its context You can ask questions and get instant answers instead of searching through documentation AI serves as the subject matter expert\nAI can analyze your source data schema and suggest appropriate mappings It understands both common data patterns and Senzing\u0026rsquo;s requirements You provide business context, AI provides technical guidance on Senzing specifications AI dramatically reduces time to implement\nWhat traditionally took 3-6 days can now be completed in under an hour AI generates mapper code automatically based on your decisions Documentation is created as a byproduct, not an afterthought The Proven Workflow AI alone isn\u0026rsquo;t enough - you need a structured approach. This workshop teaches the Senzing Mapping Assistant workflow, which combines AI capabilities with human decision-making through 5 stages:\nSTAGE 1: INIT - Load reference materials AI loads the Senzing specification, examples, and crosswalks into context\nSTAGE 2: INVENTORY - Analyze source data AI reviews your data schema to understand what fields you\u0026rsquo;re working with\nSTAGE 3: PLANNING - Determine strategy You and AI decide on DATA_SOURCE codes and entity types together\nSTAGE 4: MAPPING - Field-by-field decisions AI suggests mappings, you confirm or correct based on your business knowledge\nSTAGE 5: OUTPUTS - Generate deliverables AI creates mapper code, documentation, and examples automatically\nThe Critical Caveat AI is your assistant, not a replacement for your judgment.\nWhile AI dramatically speeds up the mapping process, YOU still must:\nUnderstand your source data - Know what your fields mean and how they should be used Make business decisions - Decide what should influence entity matching vs what\u0026rsquo;s operational data Provide context AI doesn\u0026rsquo;t have - Explain business rules, data quirks, special requirements Validate AI suggestions - Check that recommendations align with your requirements Correct mistakes - Guide AI when it misunderstands or makes wrong assumptions Example: AI might suggest mapping registration_date to Senzing\u0026rsquo;s REGISTRATION_DATE feature. But YOU need to know whether this is a legal entity registration date (use the feature) or a customer signup date (use payload). AI knows Senzing specs, YOU know your data.\nWhat Makes This Workshop Different You\u0026rsquo;re not just learning to prompt AI. You\u0026rsquo;re learning:\nA repeatable structured workflow - Use the same 5-stage process for any dataset Human-AI collaboration - How to work WITH AI effectively, not just use it Professional validation tools - Tools used in production Senzing implementations Entity resolution concepts - Understanding through hands-on practice The Result By combining AI assistance with a proven workflow, you get:\n✅ Speed - Complete mappings in hours instead of days ✅ Quality - AI knows all Senzing specifications and best practices ✅ Documentation - Complete mapping specifications generated automatically ✅ Validation - Professional tools verify correctness at each step ✅ Knowledge transfer - You learn entity resolution concepts through the process ✅ Production-ready code - Mappers that are maintainable and well-documented\nThink of it this way: AI provides speed and knowledge of Senzing specifications. You provide business context and decision-making. Together, you create production-quality mappers quickly and correctly.\nReady to learn about Senzing\u0026rsquo;s entity model?\n"
},
{
	"uri": "//localhost:1313/3_workshopoverview/31_mappingworkflow.html",
	"title": "Senzing&#39;s AI Assisted Approach",
	"tags": [],
	"description": "",
	"content": "Traditional vs AI-Assisted Mapping Traditional Approach Manually analyzing fields, writing custom code, debugging issues, and updating documentation took days or weeks for each data source. The process involved:\nReading through data dictionaries and sample files Consulting Senzing documentation repeatedly Writing mapper code from scratch Debugging field mappings and data transformations Maintaining separate documentation Senzing\u0026rsquo;s AI-Assisted Approach With AI assistance and the Senzing Mapping Assistant workflow, you can:\nAutomate schema analysis - AI reads and understands your data structure Get guided decisions on field mappings based on Senzing best practices Generate production-ready code automatically from your mapping decisions Validate output automatically using professional tools Complete mappings in hours instead of days What Makes This Different? This isn\u0026rsquo;t just a coding exercise or simple code generation. You\u0026rsquo;ll learn a repeatable workflow that combines:\nAI assistance for speed and accuracy Human decision-making for business context Proven validation tools for quality assurance Complete documentation for maintainability This isn\u0026rsquo;t just code generation - it\u0026rsquo;s AI-assisted decision-making with you in control. The AI guides you through the specifications and best practices, but YOU make the final decisions based on your understanding of the data.\n"
},
{
	"uri": "//localhost:1313/99_unused/33_senzingentityspec.html",
	"title": "The Senzing Entity Model",
	"tags": [],
	"description": "",
	"content": "Understanding Senzing\u0026rsquo;s Data Format Senzing uses a standardized JSON format for entity resolution. Each record represents either a PERSON or ORGANIZATION entity and contains structured information that Senzing uses to find matches and relationships across your data.\nThe key distinction to understand is Features vs Payload:\nFeatures - Attributes used for entity matching and resolution (NAME, ADDRESS, PHONE, EMAIL, DATE_OF_BIRTH, identifiers like SSN or PASSPORT). These go in a FEATURES array within your JSON record. Payload - Additional data you want to store but not use for matching (account balances, customer tiers, transaction dates). These are placed at the root level of the JSON record. Senzing recognizes dozens of standard features with specific attribute names. For example, a NAME feature includes NAME_FIRST, NAME_LAST, and optionally NAME_MIDDLE, NAME_PREFIX, NAME_SUFFIX. Each feature type has its own set of expected attributes.\nWhat You Need to Know Before mapping data, you should understand:\nEntity Types - When to use RECORD_TYPE: \u0026quot;PERSON\u0026quot; vs \u0026quot;ORGANIZATION\u0026quot; Common Features - NAME, ADDRESS, PHONE, EMAIL, DATE_OF_BIRTH, and standard identifiers Identifier Types - SSN, DRIVERS_LICENSE, PASSPORT, NATIONAL_ID and when to use each Feature Structure - How attributes nest within features The DATA_SOURCE Concept - Every record needs a DATA_SOURCE value identifying where it came from Don\u0026rsquo;t try to memorize everything! The AI mapping assistant will guide you through the specifications. This section is just to give you the mental model before you start.\nReference Materials For complete details on Senzing\u0026rsquo;s entity format, see:\nFull Entity Specification: workshop/senzing/reference/senzing_entity_specification.md (2100+ lines covering every feature type) Mapping Examples: workshop/senzing/reference/senzing_mapping_examples.md (real-world examples of common mapping patterns) Identifier Crosswalk: workshop/senzing/reference/identifier_crosswalk.json (canonical identifier types and aliases) You\u0026rsquo;ll work with these files when using the AI mapping assistant in Module 4.\nExample Senzing Record Here\u0026rsquo;s what a complete Senzing record looks like:\n{ \u0026#34;DATA_SOURCE\u0026#34;: \u0026#34;CUSTOMERS\u0026#34;, \u0026#34;RECORD_ID\u0026#34;: \u0026#34;1001\u0026#34;, \u0026#34;RECORD_TYPE\u0026#34;: \u0026#34;PERSON\u0026#34;, \u0026#34;PRIMARY_NAME_LAST\u0026#34;: \u0026#34;Smith\u0026#34;, \u0026#34;PRIMARY_NAME_FIRST\u0026#34;: \u0026#34;John\u0026#34;, \u0026#34;DATE_OF_BIRTH\u0026#34;: \u0026#34;1985-03-15\u0026#34;, \u0026#34;ADDR_LINE1\u0026#34;: \u0026#34;123 Main Street\u0026#34;, \u0026#34;ADDR_CITY\u0026#34;: \u0026#34;Springfield\u0026#34;, \u0026#34;ADDR_STATE\u0026#34;: \u0026#34;IL\u0026#34;, \u0026#34;ADDR_POSTAL_CODE\u0026#34;: \u0026#34;62701\u0026#34;, \u0026#34;PHONE_NUMBER\u0026#34;: \u0026#34;555-1234\u0026#34;, \u0026#34;EMAIL_ADDRESS\u0026#34;: \u0026#34;john.smith@email.com\u0026#34;, \u0026#34;SSN_NUMBER\u0026#34;: \u0026#34;123-45-6789\u0026#34;, \u0026#34;CUSTOMER_SINCE_DATE\u0026#34;: \u0026#34;2020-01-15\u0026#34;, \u0026#34;ACCOUNT_STATUS\u0026#34;: \u0026#34;ACTIVE\u0026#34; } Notice:\nFeatures are at the root level using PRIMARY_ prefix (this is the \u0026ldquo;flattened\u0026rdquo; format we\u0026rsquo;ll use) Payload attributes (CUSTOMER_SINCE_DATE, ACCOUNT_STATUS) are also at the root Each record has DATA_SOURCE, RECORD_ID, and RECORD_TYPE Ready to see the data you\u0026rsquo;ll be mapping?\n"
},
{
	"uri": "//localhost:1313/3_workshopoverview/32_workshopfolder.html",
	"title": "Workshop Folder Tour",
	"tags": [],
	"description": "",
	"content": "Opening Your Workshop Environment All workshop materials are located in the /home/ubuntu/workshop/ directory.\nIn this section, you\u0026rsquo;ll explore the workshop folder structure and learn how AI can help you quickly understand unfamiliar tools and documentation - a key skill you\u0026rsquo;ll use throughout the workshop.\nOpen this folder now in your IDE:\nIn your code-server IDE, use the file explorer (left sidebar) Navigate to /home/ubuntu/workshop/ Expand the folders to see the structure The Workshop Folder Structure Here is a brief explanation of the main folders and their contents: workshop/ ├── senzing/ # Senzing toolkit - prompts, references, tools │ ├── prompts/ │ │ └── senzing_mapping_assistant.md # 5-stage AI workflow prompt │ ├── reference/ │ │ ├── senzing_entity_specification.md # Complete Senzing feature documentation │ │ ├── senzing_mapping_examples.md # Real-world mapping examples │ │ ├── identifier_crosswalk.json # Identifier type lookup table │ │ └── usage_type_crosswalk.json # Valid usage types for features │ ├── tools/ │ │ ├── sz_schema_generator.py # Analyzes source data structure │ │ ├── lint_senzing_json.py # JSON validator for Senzing format │ │ ├── sz_json_analyzer.py # Analyzes mapped Senzing output │ │ └── sz_default_config.json # Reference Senzing configuration │ └── SENZING_TOOLS_REFERENCE.md # Complete tool documentation ├── workspace/ # Your source data for exercises └── solutions/ # Complete reference implementations The solutions folder is provided so you can catch up if you fall behind or check your work. Try the exercise yourself first - that\u0026rsquo;s where the learning happens!\nReady to explore the workshop materials with AI assistance? Let\u0026rsquo;s move to the next page where you\u0026rsquo;ll practice asking AI questions about the folder structure!\n"
},
{
	"uri": "//localhost:1313/99_unused/34_workshopdata.html",
	"title": "Your Workshop Datasets",
	"tags": [],
	"description": "",
	"content": "Two Datasets for Learning This workshop includes two datasets that you\u0026rsquo;ll map to Senzing format:\n1. Customer Data (Core Exercise - Module 4) File: workshop/workspace/customers/customers.csv\nThe Customer Data Challenge This is a realistic customer database export with 120 records containing a mix of individual persons (114) and organizations (6).\nYour goal: Transform this into validated Senzing JSON format that can identify:\nDuplicate customers with similar names and addresses Same person with multiple identifiers Organizations vs individuals Relationships between entities Dataset characteristics:\nCore Fields:\nCustomer ID and name Customer type indicator (Individual or Company) Demographics (for persons):\nGender and date of birth Contact Information:\nAddress (street, city, state, zip, country) Phone numbers Email addresses Identifiers:\nDynamic identifier fields (SSN, Driver\u0026rsquo;s License, Passport, National ID) Stored as: id_type, id_number, id_country Payload Attributes (not used for matching):\nCustomer registration date Account status Account balance Customer tier Data Quality: Like real-world data, this dataset has varying completeness - not every field is populated for every record.\n2. Watchlist Data (Bonus Exercise - Module 6) File: workshop/workspace/watchlist/ftm.jsonl\nThis is 70 entities in FollowTheMoney (FTM) format representing international watchlist records. This dataset is more complex:\nEntity Types:\n33 Person entities 6 Company entities 17 Sanction relationships 8 Ownership relationships 6 Directorship relationships Challenges:\nNested JSON structure (properties as arrays) Multiple entity types in one file Relationships between entities International names (various scripts and languages) This advanced dataset teaches you to map complex formats and handle entity relationships.\nPre-Generated Schema Analysis To help you understand the data structure, pre-generated schema files are available:\nCustomer Schema: workshop/solutions/customers/customers_schema.md\nField-by-field analysis Population percentages Sample values Data type detection Watchlist Schema: workshop/solutions/watchlist/ftm_schema.md\nFTM structure breakdown Entity type distribution Property analysis These schema files were generated using workshop/senzing/tools/sz_schema_generator.py - a tool you\u0026rsquo;ll use when mapping your own data.\nViewing the Data You can explore the datasets directly in your Cloud9 environment:\nView customer data:\ncd workshop/workspace/customers head -20 customers.csv View watchlist data:\ncd workshop/workspace/watchlist head -5 ftm.jsonl | python3 -m json.tool Review schemas:\ncat workshop/solutions/customers/customers_schema.md | head -50 Take a few minutes to familiarize yourself with the data structure before proceeding to learn the mapping workflow.\nReady to learn how the AI-assisted mapping process works?\n"
},
{
	"uri": "//localhost:1313/3_workshopoverview/33_explorewithai.html",
	"title": "Explore with AI",
	"tags": [],
	"description": "",
	"content": "Using AI to Understand the Workshop Files Rather than reading lengthy documentation, let\u0026rsquo;s use AI to explore the workshop folder structure. This demonstrates a key modern development skill: asking targeted questions to quickly understand unfamiliar codebases and tools.\nWhy ask AI? Instead of reading lengthy documentation, you can ask targeted questions and get immediate, contextual answers. This is a key skill for modern developers!\nOpening Amazon Q Developer First, let\u0026rsquo;s open Amazon Q Developer and give it context about the workshop folder.\nClick the Amazon Q icon in the left side panel of your IDE\nAsk Amazon Q: Use the senzing folder as context for this session\nThis tells Q to reference all the files in the senzing folder when answering your questions.\nWhat you should see:\nQuestion 1: What does the senzing folder contain? Ask Amazon Q: What does the senzing folder contain?\nWhat you should see:\nExpected Response: AI should explain that the senzing folder contains prompts, reference documentation, and tools for working with Senzing entity resolution.\nQuestion 2: What does the mapping assistant prompt do? Ask Amazon Q: What does the mapping assistant prompt do?\nWhat you should see:\nExpected Response: AI should explain the 5-stage workflow (INIT → INVENTORY → PLANNING → MAPPING → OUTPUTS) and how the prompt guides you through structured data mapping.\nQuestion 3: What are those senzing tools and when do I use them? Ask Amazon Q: What are those senzing tools and when do I use them?\nWhat you should see:\nExpected Response: AI should describe the schema generator (for analyzing source data), JSON linter (for validation), and JSON analyzer (for understanding mapped output), along with when to use each.\nWrap Up You\u0026rsquo;ve now practiced using AI to understand unfamiliar tools and documentation. This skill - asking AI targeted questions about code and tools - will be essential throughout the workshop.\nKey Takeaways:\nThe workshop/ folder contains everything you need: tools, data, and solutions AI can help you quickly understand complex folder structures and tools Asking targeted questions is faster than reading documentation cover-to-cover "
},
{
	"uri": "//localhost:1313/99_unused/36_validationtools.html",
	"title": "Validation Tools",
	"tags": [],
	"description": "",
	"content": "Verify Your Mappings Are Correct Data mapping isn\u0026rsquo;t complete until it\u0026rsquo;s validated. This workshop includes three essential tools that verify your mappings at different stages of the process.\nThe Three Validation Tools 1. Schema Generator - sz_schema_generator.py Purpose: Analyze source data structure before mapping\nWhat it does:\nScans your source data file (CSV or JSON) Generates field-by-field analysis showing: Field names and data types Population percentage (% of records with values) Uniqueness (% of distinct values) Sample values Min/max lengths When to use: Before mapping (Stage 2 - INVENTORY) to understand your source data\nExample:\ncd workshop/senzing/tools python3 sz_schema_generator.py ../../workspace/customers/customers.csv Output: Markdown file with complete schema documentation\n2. JSON Linter - lint_senzing_json.py Purpose: Validate Senzing JSON structure and syntax\nWhat it does:\nVerifies JSON is well-formed Checks required fields exist (DATA_SOURCE, RECORD_ID) Validates feature structure Catches common formatting errors Reports warnings and errors When to use: After mapping (Stage 5) to verify output format is correct\nExample:\ncd workshop/senzing/reference python3 lint_senzing_json.py ../../solutions/customers/customers_senzing.jsonl Output: Either \u0026ldquo;OK: All files passed\u0026rdquo; or detailed error messages\n3. JSON Analyzer - sz_json_analyzer.py Purpose: Show what Senzing will actually recognize in your data\nWhat it does:\nUses cached Senzing configuration to analyze mapped data Shows which attributes Senzing recognizes (GREEN) Shows which attributes are payload/unmapped (YELLOW) Flags errors like unregistered DATA_SOURCE (RED) Warns about sparse optional fields (ORANGE) Provides statistics on record counts and coverage When to use: After linting passes, to see what Senzing sees\nExample:\ncd workshop/senzing/tools python3 sz_json_analyzer.py ../../solutions/customers/customers_senzing.jsonl Output: Color-coded report showing:\nGREEN: Mapped Senzing features (will be used for entity resolution) YELLOW: Unmapped attributes (payload data, stored but not matched) RED: Errors (missing DATA_SOURCE registration, invalid values) ORANGE: Warnings (sparse data, unusual patterns) Validation Workflow The typical validation sequence:\n1. Generate schema → Understand source data 2. Map with AI assistance → Create mapper 3. Run mapper → Generate Senzing JSON 4. Run linter → Verify JSON structure 5. Run analyzer → Verify Senzing will recognize everything 6. Fix any issues → Re-run steps 3-5 7. Load into Senzing → See entity resolution results Always validate in this order: Linter first (structure), then analyzer (semantics). If the linter fails, the analyzer results won\u0026rsquo;t be meaningful.\nTool Locations All three tools are in the workshop/senzing/ directory:\nworkshop/senzing/ ├── tools/ │ ├── sz_schema_generator.py # Analyze source data │ ├── sz_json_analyzer.py # Analyze mapped data │ └── g2config.json # Cached Senzing config └── reference/ └── lint_senzing_json.py # Validate JSON structure Example Validation Output Linter Success:\nLinting file: customers_senzing.jsonl OK: All files passed Analyzer Success Indicators:\nAll expected features shown in GREEN Payload attributes in YELLOW (expected) No RED errors (or only DATA_SOURCE not registered, which is normal before loading) Statistics match your record count What \u0026ldquo;Good\u0026rdquo; Looks Like:\n✅ Linter passes with no errors ✅ All features recognized (GREEN) ✅ Payload correctly identified (YELLOW) ✅ No missing required fields ✅ Record count matches source data Hands-On Practice Coming Next In Module 4, you\u0026rsquo;ll use all three tools as you map the customer dataset:\nReview the pre-generated schema Map fields with AI assistance Validate with linter and analyzer Iterate until everything passes Important: Don\u0026rsquo;t skip validation! It\u0026rsquo;s the only way to know your mapping is correct before loading thousands of records into Senzing.\nReady to start mapping? Let\u0026rsquo;s move to Module 4!\n"
},
{
	"uri": "//localhost:1313/99_unused/37_learningobjectives.html",
	"title": "What You&#39;ll Learn",
	"tags": [],
	"description": "",
	"content": "What You\u0026rsquo;ll Learn in This Workshop By completing this workshop, you\u0026rsquo;ll gain comprehensive knowledge across three areas: technical skills, practical knowledge, and process expertise.\nTechnical Skills You\u0026rsquo;ll understand:\nAI-Assisted Workflows:\nHow to use AI for structured, validated workflows (not just code generation) How to guide AI with proper context and reference materials When to accept AI suggestions vs when to override them Senzing Entity Resolution:\nThe Senzing entity resolution data model Difference between Features (used for matching) and Payload (operational data) How Senzing uses features to identify duplicate entities Validation Techniques:\nProfessional validation tools (lint_senzing_json.py, sz_json_analyzer.py) How to interpret validation output Iterative refinement based on validation feedback Practical Knowledge You\u0026rsquo;ll learn:\nMapping Decisions:\nWhen to use which Senzing attributes (NAME, ADDRESS, identifiers, etc.) How to handle mixed entity types (persons and organizations) Common mapping pitfalls and how to avoid them Data Transformations:\nHow to parse complex identifier fields Date format standardization Handling missing or incomplete data Real-World Challenges:\nWorking with varying data completeness Mapping dynamic identifier fields Distinguishing operational data (payload) from matching data (features) Process Expertise You\u0026rsquo;ll master:\nThe 5-Stage Senzing Mapping Assistant Workflow:\nINIT - Loading reference materials into AI context INVENTORY - Analyzing source data schema PLANNING - Determining data sources and entity types MAPPING - Making field-by-field mapping decisions OUTPUTS - Generating code, documentation, and validated output Validation Practices:\nHow to validate mappings before production use Using linting to catch syntax errors Using analysis tools to verify attribute recognition Iterating on mappings based on validation results Documentation:\nCreating mapping specifications Documenting business decisions Maintaining mapper code with clear comments Repeatability:\nHow to apply this workflow to your own data sources Adapting the process for different data formats (CSV, JSON, XML) Building a library of reusable mapping patterns What You\u0026rsquo;ll Have at the End By completing this workshop, you will have:\nPractical Deliverables:\n✅ Mapped real customer data to Senzing format using AI assistance ✅ Created production-ready mapper code ✅ Validated your mappings with professional tools ✅ Complete mapping documentation Deep Understanding:\n✅ Know the Senzing entity resolution data model ✅ Understand Features vs Payload attributes ✅ Recognize common mapping patterns and best practices Repeatable Process:\n✅ Learned the 5-stage Senzing Mapping Assistant workflow ✅ Can apply this workflow to any data source ✅ Know how to validate and iterate on your mappings This workshop focuses on mapping - preparing data for entity resolution. Loading the mapped data into Senzing and viewing resolution results is covered in Module 5.\nModule-by-Module Learning Path Here\u0026rsquo;s how the workshop builds your knowledge:\nModule 1: Workshop overview and orientation Module 2: Environment setup with Amazon Q Developer and Senzing Module 3: (Current) Understanding concepts and workflow Module 4: Hands-on customer data mapping with AI Module 5: Loading mapped data and viewing entity resolution results Module 6: (Bonus) Advanced mapping with watchlist data and relationships Module 7: Cleanup and next steps\nReady to dive into hands-on mapping? Continue to Module 4!\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/41_examinesourcedata.html",
	"title": "Step 1: Examine Source Data",
	"tags": [],
	"description": "",
	"content": "Understanding Your Source Data Before mapping data to Senzing format, you need to understand what you\u0026rsquo;re working with. In this step, you\u0026rsquo;ll examine the customer CSV file to identify its structure, fields, and business purpose.\nOpen the Customer Data File Navigate to the source data:\nIn your IDE file explorer (left sidebar), expand the workshop folder Navigate to workshop/workspace/customers/ Double-click customers.csv to open it in the editor Explore the Data Structure Take a few minutes to review the CSV file. Ask yourself these questions:\nAbout the data structure:\nWhat columns are present? What types of data do you see? (names, addresses, identifiers, dates, etc.) Are there any empty or inconsistent values? About the business purpose:\nWhy are you loading this data into Senzing? Are you looking for duplicate customer records? Will you compare this against a watchlist later? Do you need to consolidate customer identities? Understanding both the structure and purpose of your data will help you guide the AI to create an effective mapping.\nKey Insight: You don\u0026rsquo;t need to memorize every field - just get familiar with what\u0026rsquo;s there. The AI will help with the detailed mapping, but you need to provide the business context and mapping strategy.\nCheckpoint: You should understand the basic structure of the customer data and why you\u0026rsquo;re loading it into Senzing before proceeding to the next step.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/42_createschema.html",
	"title": "Step 2: Generate Schema",
	"tags": [],
	"description": "",
	"content": "Generate a Data Schema Now that you\u0026rsquo;ve examined the customer data, you need to document its structure. A schema document provides essential information about field types, population rates, and sample values - critical details for accurate mapping.\nWhy Generate a Schema? When mapping data, AI needs to understand both the source and target schemas. However, you shouldn\u0026rsquo;t upload entire data files to AI because:\nPrivacy concerns - Files may contain sensitive customer information Context limits - AI models have limited context windows and cannot process large files Accuracy - AI can infer schema from sample rows, but this is error-prone Instead, you\u0026rsquo;ll use the Senzing schema generator tool to create a comprehensive schema document that includes:\nField names and data types Population percentages (how often fields contain values) Sample values and ranges Field statistics Pro Tip: For complex schemas the tool cannot handle, locate the official schema documentation or ask AI to help extract schema information from documentation.\nGenerate the Schema with Amazon Q Ask Amazon Q to generate the schema:\nOpen Amazon Q Developer (click the Q icon in the left sidebar) Type: Generate a schema for the customer CSV Security Note: Notice that Q asks for your approval before running the schema generator. This is an important security feature - never give tools blanket access to run commands without review. Always verify what actions AI wants to take before approving.\nReview Q\u0026rsquo;s response:\nQ Developer will run the schema generator tool and create a customer_schema.md file in your workspace.\nReview the Generated Schema Open the schema file:\nIn your IDE file explorer, navigate to workshop/workspace/customers/ Open customer_schema.md Review the schema to understand:\nWhat fields are available Which fields have high population rates (good for matching) What the data values look like This schema document will help guide the AI during the mapping process.\nFallback: Use the Pre-Generated Schema If you encounter issues generating the schema, a pre-generated version is available in the solutions folder:\nworkshop/solutions/customers/customer_schema.md\nCheckpoint: You should have a customer_schema.md file that documents all fields, data types, and sample values from the customer CSV.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/43_mapwithassistant.html",
	"title": "Step 3: Map the Schema",
	"tags": [],
	"description": "",
	"content": "Use the Senzing Mapping Assistant Overview Now you\u0026rsquo;ll use the Senzing Mapping Assistant prompt with Amazon Q to guide you through the mapping process. This AI-assisted workflow takes you through 5 stages to create a complete mapper.\nThe 5-Stage Workflow The Mapping Assistant guides you through:\nINIT - Load reference files and verify tools INVENTORY - Analyze source data schema PLANNING - Determine data source and entity types MAPPING - Map fields to Senzing features OUTPUTS - Generate mapper script and documentation Start the Mapping Assistant Prompt to use:\nTODO: Placeholder prompt for starting the mapping assistant with customer data What you should see:\nWork Through Each Stage TODO: Detailed instructions for each stage will go here.\nPlaceholder for:\nINIT stage instructions and screenshot INVENTORY stage instructions and screenshot PLANNING stage instructions and screenshot MAPPING stage instructions and screenshot OUTPUTS stage instructions and screenshot The AI will guide you through each stage. Answer its questions thoughtfully and validate its suggestions.\nCheckpoint: You should have a complete mapper Python script and mapping documentation.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/44_validatemapping.html",
	"title": "Step 4: Validate Mapping",
	"tags": [],
	"description": "",
	"content": "Validate the Mapping with JSON Analyzer Overview Before loading data into Senzing, validate that your mapping produces correct Senzing JSON format. Use the JSON analyzer tool to verify the mapped output.\nRun the JSON Analyzer Command to use:\nTODO: Placeholder command for analyzing the mapped customer JSON What you should see:\nInterpret the Results TODO: Instructions for interpreting analyzer output will go here.\nPlaceholder for:\nUnderstanding color-coded output (GREEN=recognized features, YELLOW=payload, RED=errors) Verifying all required fields are present Checking for unregistered data sources Identifying sparse fields Confirming feature mappings are correct Expected results:\nAll Senzing features should be GREEN DATA_SOURCE should be recognized No RED errors Payload fields in YELLOW (expected) If you see RED errors, fix them before proceeding to data loading.\nCheckpoint: Your mapped data should pass validation with no errors.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/45_loaddata.html",
	"title": "Step 5: Load Data",
	"tags": [],
	"description": "",
	"content": "Load the Data in Senzing Overview Now that your mapping is validated, load the customer data into Senzing for entity resolution. Senzing will analyze the records and identify which ones represent the same real-world entities.\nLoad the Mapped Data Command to use:\nTODO: Placeholder command for loading customer data into Senzing What you should see:\nMonitor the Load Process TODO: Instructions for monitoring data loading will go here.\nPlaceholder for:\nRunning the load command Watching progress indicators Verifying successful completion Checking for any load errors Understanding what Senzing is doing during load Expected results:\nAll 120 records loaded successfully No errors or warnings Data ready for entity resolution analysis Senzing processes records in real-time, performing entity resolution as data loads.\nCheckpoint: All customer records should be loaded into Senzing without errors.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/46_takesnapshot.html",
	"title": "Step 6: Take Snapshot",
	"tags": [],
	"description": "",
	"content": "Take a Snapshot Overview After loading data, take a snapshot to capture the current state of entity resolution results. This snapshot will be used for analysis in the next steps.\nCreate the Snapshot Command to use:\nTODO: Placeholder command for taking a Senzing snapshot What you should see:\nUnderstand Snapshots TODO: Explanation of what snapshots are will go here.\nPlaceholder for:\nWhat a snapshot captures Why snapshots are useful Where snapshot data is stored How to use snapshots for analysis Expected results:\nSnapshot file created successfully Contains current entity resolution state Ready for statistical analysis Snapshots let you analyze resolution results without querying the live database repeatedly.\nCheckpoint: You should have a snapshot file capturing the current entity resolution state.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/47_analyzesnapshot.html",
	"title": "Step 7: Analyze Snapshot",
	"tags": [],
	"description": "",
	"content": "Analyze the Snapshot for Stats and Examples Overview Analyze the snapshot to understand entity resolution results. You\u0026rsquo;ll see statistics about how many entities were resolved and review specific examples of matches.\nGenerate Analysis Report Command to use:\nTODO: Placeholder command for analyzing the snapshot What you should see:\nReview Resolution Statistics TODO: Instructions for reviewing statistics will go here.\nPlaceholder for:\nTotal records loaded Number of resolved entities Match types (exact, fuzzy, relationship) Duplicate detection rate Data quality indicators Expected results:\n120 records resolved into fewer entities Statistics showing duplicate identification Examples of resolved entities Quality metrics Examine Example Resolutions TODO: Instructions for reviewing examples will go here.\nPlaceholder for:\nViewing resolved entity examples Understanding why records matched Identifying resolution patterns Noting interesting cases for MCP analysis Look for interesting resolution cases to explore with the MCP server in the next step.\nCheckpoint: You should understand the resolution statistics and have identified interesting examples to explore.\n"
},
{
	"uri": "//localhost:1313/4_exercise1_customers/48_reviewwithmcp.html",
	"title": "Step 8: Review Examples",
	"tags": [],
	"description": "",
	"content": "Review with MCP Server Overview Use the Senzing MCP Server through Amazon Q to explore entity resolution results interactively. Ask \u0026ldquo;how\u0026rdquo; and \u0026ldquo;why\u0026rdquo; questions to understand resolution decisions.\nUsing the MCP Server Now that data is loaded, you can use conversational AI to query and understand the results.\nExample Question 1: Get an Entity Ask Amazon Q: Get entity 1\nWhat you should see:\nExample Question 2: Why Did Records Match? Ask Amazon Q: TODO: Placeholder question about why specific records matched\nWhat you should see:\nExample Question 3: How Did Resolution Work? Ask Amazon Q: TODO: Placeholder question about resolution methodology\nWhat you should see:\nExplore Further TODO: Additional exploration ideas will go here.\nPlaceholder for:\nAsking about specific entities Understanding match scores Exploring relationship graphs Querying data source distribution Investigating resolution rules The MCP server lets you explore entity resolution results conversationally. Experiment with different questions!\nCheckpoint: You should understand how to use the MCP server to explore and understand entity resolution results.\nExercise 1 Complete! Congratulations! You\u0026rsquo;ve successfully:\n✅ Examined customer source data ✅ Generated a data schema ✅ Mapped data with AI assistance ✅ Validated the mapping ✅ Loaded data into Senzing ✅ Captured a snapshot ✅ Analyzed resolution statistics ✅ Explored results with MCP You\u0026rsquo;re now ready for Exercise 2: Map and Load Watchlist Data!\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/51_examinesourcedata.html",
	"title": "Step 1: Examine Source Data",
	"tags": [],
	"description": "",
	"content": "Examine the Source Data Overview Examine the watchlist JSON data to understand its structure. Unlike the CSV format in Exercise 1, this data uses the FollowTheMoney (FTM) schema with nested structures.\nLocate the Source Data The watchlist data is located at:\n/home/ubuntu/workshop/workspace/watchlist/watchlist.jsonl Examine the Data TODO: Instructions for examining the JSON file will go here.\nPlaceholder for:\nOpening the file in the IDE Understanding JSONL format (one JSON object per line) Reviewing the FTM schema structure Looking at sample entities Identifying nested fields and relationships Expected observations:\nNumber of entities: 70 Format: FollowTheMoney (FTM) JSON schema International names and identifiers Sanctions and watchlist information Entity relationships What you should see:\nCheckpoint: You should understand the FTM JSON structure and how it differs from the CSV in Exercise 1.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/52_createschema.html",
	"title": "Step 2: Create Schema",
	"tags": [],
	"description": "",
	"content": "Provide or Create the Schema Overview Document the schema for the watchlist JSON data. The schema generator can help analyze the FTM format and create a schema document.\nUsing the Schema Generator TODO: Instructions for running the schema generator will go here.\nPlaceholder for:\nRunning sz_schema_generator.py on the watchlist JSONL Reviewing the generated schema output Understanding nested field structures Noting FTM-specific attributes Saving the schema for reference Command:\npython3 ~/workshop/senzing/tools/sz_schema_generator.py \\ ~/workshop/workspace/watchlist/watchlist.jsonl Expected output:\nMarkdown-formatted schema document Field names, data types, and sample values Nested structure documentation Statistics on field population What you should see:\nJSON schemas are more complex than CSV. Pay attention to nested structures and arrays.\nCheckpoint: You should have a schema document describing the watchlist data structure.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/53_mapschema.html",
	"title": "Step 3: Map the Schema",
	"tags": [],
	"description": "",
	"content": "Use the Senzing Mapping Assistant Overview Use the Senzing Mapping Assistant prompt with Amazon Q to map the watchlist data. This follows the same 5-stage workflow but with JSON input instead of CSV.\nThe 5-Stage Workflow The Mapping Assistant guides you through:\nINIT - Load reference files and verify tools INVENTORY - Analyze source data schema PLANNING - Determine data source and entity types MAPPING - Map fields to Senzing features OUTPUTS - Generate mapper script and documentation Start the Mapping Assistant Prompt to use:\nTODO: Placeholder prompt for starting the mapping assistant with watchlist data What you should see:\nWork Through Each Stage TODO: Detailed instructions for each stage will go here.\nPlaceholder for:\nINIT stage instructions and screenshot INVENTORY stage instructions and screenshot PLANNING stage instructions and screenshot MAPPING stage instructions and screenshot (handling nested JSON) OUTPUTS stage instructions and screenshot Watchlist data includes relationships and nested structures. The AI will help you handle these complexities.\nCheckpoint: You should have a complete mapper Python script and mapping documentation for watchlist data.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/54_validatemapping.html",
	"title": "Step 4: Validate Mapping",
	"tags": [],
	"description": "",
	"content": "Validate the Mapping with JSON Analyzer Overview Validate that your watchlist mapping produces correct Senzing JSON format. Use the JSON analyzer tool to verify the mapped output.\nRun the JSON Analyzer Command to use:\nTODO: Placeholder command for analyzing the mapped watchlist JSON What you should see:\nInterpret the Results TODO: Instructions for interpreting analyzer output will go here.\nPlaceholder for:\nUnderstanding color-coded output (GREEN=recognized features, YELLOW=payload, RED=errors) Verifying all required fields are present Checking for unregistered data sources Validating international name handling Confirming relationship mappings are correct Expected results:\nAll Senzing features should be GREEN DATA_SOURCE should be recognized No RED errors Payload fields in YELLOW (expected) International identifiers properly mapped If you see RED errors, fix them before proceeding to data loading.\nCheckpoint: Your mapped watchlist data should pass validation with no errors.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/55_loaddata.html",
	"title": "Step 5: Load Data",
	"tags": [],
	"description": "",
	"content": "Load the Data in Senzing Overview Load the watchlist data into Senzing. This data will be combined with the customer data from Exercise 1, allowing cross-dataset entity resolution.\nLoad the Mapped Data Command to use:\nTODO: Placeholder command for loading watchlist data into Senzing What you should see:\nMonitor the Load Process TODO: Instructions for monitoring data loading will go here.\nPlaceholder for:\nRunning the load command Watching progress indicators Verifying successful completion Checking for any load errors Understanding cross-dataset resolution Expected results:\nAll 70 watchlist entities loaded successfully No errors or warnings Data combined with existing customer records Cross-dataset resolution occurring Senzing will automatically compare watchlist entities against customer data, identifying any matches.\nCheckpoint: All watchlist entities should be loaded into Senzing without errors.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/56_takesnapshot.html",
	"title": "Step 6: Take Snapshot",
	"tags": [],
	"description": "",
	"content": "Take a Snapshot Overview Take a new snapshot to capture the combined results of customer and watchlist data. This snapshot will show how entities resolve across both datasets.\nCreate the Snapshot Command to use:\nTODO: Placeholder command for taking a Senzing snapshot after watchlist load What you should see:\nUnderstand Combined Snapshots TODO: Explanation of combined dataset snapshots will go here.\nPlaceholder for:\nWhat the combined snapshot captures How to analyze cross-dataset resolution Where snapshot data is stored Comparing to the customer-only snapshot Expected results:\nSnapshot file created successfully Contains customer + watchlist entity resolution state Ready for cross-dataset analysis This snapshot shows how watchlist entities relate to customer records, a key use case for entity resolution.\nCheckpoint: You should have a snapshot file capturing the combined customer and watchlist entity resolution state.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/57_analyzesnapshot.html",
	"title": "Step 7: Analyze Snapshot",
	"tags": [],
	"description": "",
	"content": "Analyze the Snapshot for Stats and Examples Overview Analyze the combined snapshot to understand how customer and watchlist entities resolved together. Look for cross-dataset matches and interesting resolution patterns.\nGenerate Analysis Report Command to use:\nTODO: Placeholder command for analyzing the combined snapshot What you should see:\nReview Resolution Statistics TODO: Instructions for reviewing combined statistics will go here.\nPlaceholder for:\nTotal entities across both datasets Cross-dataset matches identified Watchlist hit rate Data source distribution Quality metrics Expected results:\nCombined entity count (customers + watchlist) Statistics showing cross-dataset resolution Examples of watchlist matches Quality and coverage metrics Examine Cross-Dataset Resolutions TODO: Instructions for reviewing cross-dataset examples will go here.\nPlaceholder for:\nViewing entities that span both datasets Understanding why cross-dataset matches occurred Identifying interesting watchlist hits Noting cases for MCP exploration Look for customer records that match watchlist entities - these are the most interesting cases to explore.\nCheckpoint: You should understand the combined resolution statistics and have identified interesting cross-dataset examples.\n"
},
{
	"uri": "//localhost:1313/5_exercise2_watchlist/58_reviewexamples.html",
	"title": "Step 8: Review Examples",
	"tags": [],
	"description": "",
	"content": "Review with MCP Server Overview Use the Senzing MCP Server to explore cross-dataset resolution results. Focus on understanding how customer records matched against watchlist entities.\nUsing the MCP Server Query the combined dataset to understand cross-dataset resolution decisions.\nExample Question 1: Find Watchlist Matches Ask Amazon Q: TODO: Placeholder question about finding customers that matched watchlist entities\nWhat you should see:\nExample Question 2: Why Did Cross-Dataset Match Occur? Ask Amazon Q: TODO: Placeholder question about why a customer matched a watchlist entity\nWhat you should see:\nExample Question 3: Explore Entity Relationships Ask Amazon Q: TODO: Placeholder question about exploring watchlist relationships\nWhat you should see:\nExplore Further TODO: Additional exploration ideas will go here.\nPlaceholder for:\nQuerying specific watchlist entities Understanding sanctions data Exploring international name matching Investigating relationship graphs Analyzing data source combinations Cross-dataset matches are the most valuable insights from watchlist integration. Explore these thoroughly!\nCheckpoint: You should understand how to explore cross-dataset resolution results using the MCP server.\nExercise 2 Complete! Congratulations! You\u0026rsquo;ve successfully:\n✅ Examined watchlist source data (JSON/FTM format) ✅ Generated a data schema for JSON ✅ Mapped complex nested data with AI assistance ✅ Validated the watchlist mapping ✅ Loaded watchlist data into Senzing ✅ Captured a combined snapshot ✅ Analyzed cross-dataset resolution statistics ✅ Explored watchlist matches with MCP You\u0026rsquo;ve completed both core exercises and learned how to map, load, and analyze entity resolution across multiple datasets!\n"
},
{
	"uri": "//localhost:1313/99_unused/index.html",
	"title": "Unused Content",
	"tags": [],
	"description": "",
	"content": "Unused Content This module contains content that is not currently used in the workshop but is preserved for future reference.\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/21_aws_guided_event/211_joinworkshopevent.html",
	"title": "Join the workshop event",
	"tags": [],
	"description": "",
	"content": "If you are joining an AWS-hosted event, you will use a platform named AWS Workshop Studio. This platform allows AWS field teams to run Workshops, Game Days, Boot camps, Immersion Days, and other events that require hands-on access to AWS accounts or resources.\nFollow the steps below to gain access to your Workshop Studio AWS account:\nStep 1: Go to Workshop Studio Access your workshop here: Workshop Studio\nStep 2:. Choose Sign-In method. Select \u0026ldquo;Email one-time-password (OTP)\u0026rdquo; for AWS guided events. Step 3: Enter your Email Enter your email and click \u0026ldquo;Send Passcode\u0026rdquo;. Step 4: Enter OTP Check your email for a message titled \u0026ldquo;Verify your AWS Training and Certification email address\u0026rdquo;. Copy the passcode and enter it here. Click Sign in. Step 5: Enter Event Access Code (If prompted) Paste the 12-character code from your organizer and click \u0026ldquo;Next.\u0026rdquo; Step 6: Agree to Terms and Join Event Review the Terms and Conditions, check the box, and click \u0026ldquo;Join event. "
},
{
	"uri": "//localhost:1313/2_setup_configuration/21_aws_guided_event/212_accessingide.html",
	"title": "Accessing your IDE",
	"tags": [],
	"description": "",
	"content": "Access your cloud-based code-server IDE For this workshop, you\u0026rsquo;ll use Visual Studio Code running on an Amazon EC2 instance via code-server. This provides a consistent development environment with all tools pre-configured.\nOpen the Workshop Studio event dashboard\nNavigate to the Event Outputs panel at the bottom of the page.\nCopy the Password 🔑 - you\u0026rsquo;ll need this to authenticate.\nClick on the URL to open your cloud IDE in a new tab.\nIn the Welcome to code-server dialog, paste the password you copied and choose Submit.\nTip: If the login fails, verify you copied the complete password without extra spaces.\nYou should now see the code-server IDE interface with the file explorer on the left.\nPre-packaged components Your cloud IDE comes with all necessary tools pre-installed:\nTool Purpose Senzing SDK (version 4.0) Entity resolution engine - installed and configured with an empty database Amazon Q extension for Visual Studio Code IDE extension that provides AI-powered code suggestions, chat assistance, and MCP server support senzing-mcp-server Model Context Protocol server that connects Amazon Q to Senzing capabilities 📁 Workshop Materials Location: Workshop files and Senzing resources are located in /home/ubuntu/\nWrap up You have successfully accessed your VS code-server IDE, which has been pre-configured for the workshop. Please move to the next section where you will authenticate with the Amazon Q Developer extension and CLI.\nNext Step: Proceed to Amazon Q Authentication Setup to connect to Amazon Q Developer.\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/25_senzingmcpsetup/251_configuremcpserver.html",
	"title": "Step 1: Configure the MCP Server",
	"tags": [],
	"description": "",
	"content": "Overview The Senzing MCP server and all required environment variables are pre-configured in your workshop environment. You just need to connect it to Amazon Q Developer.\nAccess MCP Configuration Open the Amazon Q chat panel in your IDE\nClick the tools icon in the chat interface to access MCP configuration\nSelect the plus (+) symbol to add a new MCP server Choose Configuration Scope Choose your configuration scope: Global (recommended): Saves to ~/.aws/amazonq/default.json - available for all projects Local: Saves to .amazonq/default.json in current workspace only We recommend Global scope so the Senzing MCP server is available in all your projects.\nConfigure Server Settings Fill in the MCP server configuration:\nField Value Server Name Senzing Transport stdio Command /home/ubuntu/senzing-mcp-server/launch_senzing_mcp.sh Arguments (leave empty) Timeout 60000 (60 seconds) Use the full absolute path for Command as shown above. This is the pre-installed location in your workshop environment.\nAdd Environment Variables Add Environment Variables:\nClick \u0026ldquo;Add Environment Variable\u0026rdquo; three times and enter these exact values:\nVariable Name Value SENZING_ENGINE_CONFIGURATION_JSON {\u0026quot;PIPELINE\u0026quot;:{\u0026quot;CONFIGPATH\u0026quot;:\u0026quot;/etc/opt/senzing\u0026quot;,\u0026quot;RESOURCEPATH\u0026quot;:\u0026quot;/opt/senzing/er/resources\u0026quot;,\u0026quot;SUPPORTPATH\u0026quot;:\u0026quot;/opt/senzing/data\u0026quot;},\u0026quot;SQL\u0026quot;:{\u0026quot;CONNECTION\u0026quot;:\u0026quot;sqlite3://na:na@/home/ubuntu/sz_sqlite/G2C.db\u0026quot;}} LD_LIBRARY_PATH /opt/senzing/er/lib PYTHONPATH /home/ubuntu/.local/bin:/opt/senzing/er/sdk/python Copy and paste each value exactly as shown. The SENZING_ENGINE_CONFIGURATION_JSON value should be on a single line.\nSave Configuration Save the configuration Press the Save button to store your MCP server settings.\nIf you don\u0026rsquo;t see the save button, you may see an error instead.\nMake sure you\u0026rsquo;re adding each environment variable as a separate key-value pair. Each variable name and its value should be entered individually, not combined as JSON.\nTroubleshooting If you encounter issues:\nCommand path error: Verify the path /home/ubuntu/senzing-mcp-server/launch_senzing_mcp.sh exists Environment variables error: Ensure each variable is entered separately, not as a JSON object Timeout issues: The 60-second timeout should be sufficient; if not, check Senzing installation Once you see a successful save (no error message), proceed to authorize the MCP tools.\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/25_senzingmcpsetup/252_authorizetools.html",
	"title": "Step 2: Authorize MCP Tools",
	"tags": [],
	"description": "",
	"content": "Overview After saving your MCP server configuration, Amazon Q will automatically detect the Senzing MCP tools and prompt you to authorize them.\nAuthorization Prompt You\u0026rsquo;ll see a prompt showing all available Senzing tools:\nChoose Authorization Level Choose your authorization preference:\nAlways allow: Recommended for workshop - allows Q to use these tools automatically Ask: You\u0026rsquo;ll be prompted before each tool use (more manual but gives you control) Deny: Blocks the tool from being used Recommendation Select \u0026ldquo;Always allow\u0026rdquo; for a smoother workshop experience.\nThis setting:\nEnables Amazon Q to automatically use Senzing tools when needed Eliminates repetitive authorization prompts Creates a more natural conversational AI experience Can be changed later in Amazon Q settings if needed \u0026ldquo;Always allow\u0026rdquo; is safe in this workshop environment since you control all the data and queries. In production environments with sensitive data, you might prefer \u0026ldquo;Ask\u0026rdquo; for additional oversight.\nAuthorization Scope The authorization applies to:\nAll tools provided by the Senzing MCP server Future tool additions (if the MCP server is updated) Only this MCP server (doesn\u0026rsquo;t affect other integrations) Next Steps Once you\u0026rsquo;ve authorized the tools, you\u0026rsquo;re ready to verify the integration is working correctly!\nEnsure you\u0026rsquo;ve authorized at least some Senzing tools before proceeding to verification.\n"
},
{
	"uri": "//localhost:1313/2_setup_configuration/25_senzingmcpsetup/253_verifyintegration.html",
	"title": "Step 3: Verify the Integration",
	"tags": [],
	"description": "",
	"content": "Overview Now that you\u0026rsquo;ve configured and authorized the Senzing MCP server, let\u0026rsquo;s verify that Amazon Q can successfully access and use the Senzing tools.\nTest 1: List Available Tools In the Amazon Q chat, ask:\nWhat MCP tools are available? These tools will become more useful once you\u0026rsquo;ve loaded entity data in later modules. For now, we\u0026rsquo;re just confirming the connection works.\nTest 2: Query Test Entity Try a test query:\nGet entity1 Expected Result You should see a response indicating that entity 1 does not exist. This is expected because you haven\u0026rsquo;t loaded any data yet.\nThe important part is that:\nAmazon Q successfully called the Senzing MCP tool The tool executed without errors You received a valid response (even though the entity doesn\u0026rsquo;t exist) A response like \u0026ldquo;entity 1 does not exist\u0026rdquo; or \u0026ldquo;no entity found\u0026rdquo; confirms the MCP server is working correctly!\nWhat This Means A successful response (even if the entity doesn\u0026rsquo;t exist) indicates:\n✅ Amazon Q is connected to the Senzing MCP server ✅ The MCP server can communicate with Senzing ✅ Environment variables are configured correctly ✅ Tools are authorized and functional\nTroubleshooting If you encounter errors:\nError Possible Cause Solution \u0026ldquo;MCP server not found\u0026rdquo; Configuration not saved Revisit Step 1 \u0026ldquo;Tool not authorized\u0026rdquo; Authorization declined Revisit Step 2 \u0026ldquo;Connection timeout\u0026rdquo; Senzing not installed Contact workshop facilitator \u0026ldquo;Database error\u0026rdquo; Environment variables incorrect Verify configuration in Step 1 Wrap Up Ensure the MCP server is configured and you can see Senzing tools in Amazon Q before proceeding.\nCongratulations! You\u0026rsquo;ve successfully connected Amazon Q Developer to the Senzing MCP server!\nWhat You Can Do Now Throughout this workshop, you\u0026rsquo;ll use this integration to:\nQuery entity resolution results conversationally Understand how Senzing identified duplicate entities Analyze relationships between resolved entities Get explanations of resolution decisions Explore your mapped data interactively Ready to Continue? You\u0026rsquo;re now ready to learn about Senzing mapping concepts!\nNext Step: Proceed to Module 3: Understanding Senzing Mapping\nAdditional Resources For more details on the Senzing MCP server:\nReview /home/ubuntu/senzing-mcp-server/AMAZON_Q_SETUP.md in your environment GitHub repository: https://github.com/jbutcher21/senzing-mcp-server Amazon Q MCP Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/mcp-ide.html "
},
{
	"uri": "//localhost:1313/categories/index.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/index.html",
	"title": "One-click Entity Resolution",
	"tags": [],
	"description": "",
	"content": "One-click Entity Resolution on AWS using Amazon Q Developer and Senzing (you can\u0026rsquo;t unsee this!) What is Entity Resolution? Entity resolution identifies when different records refer to the same real-world entity across your data sources. Without it, your organization faces:\nDuplicate customers costing 10-25% of revenue through poor experiences Compliance risks from incomplete customer views Missed opportunities from fragmented data insights Operational inefficiency from manual data cleanup Learn more about entity resolution\nTransform Your Data Operations in Hours, Not Months In this hands-on workshop, you\u0026rsquo;ll learn how to leverage AI tools like Amazon Q Developer to map, load, and explore your data with Senzing entity resolution using a structured, repeatable workflow that delivers production-ready results.\nLearning Outcomes By completing this workshop, you will:\nUnderstand how to map source data to Senzing format using the 5-stage AI-assisted workflow Know how to generate production-ready mapper code and documentation automatically Learn to validate mappings using proven Senzing tools both before and after mapping Be able to load mapped data and analyze entity resolution results Master the Senzing Mapping Assistant workflow for use with your own data sources Intended audience This workshop is designed for software developers, data engineers, and anyone interested in entity resolution who understands ETL (Extract, Transform, Load) workflows.\nYou should have:\nBasic familiarity with IDEs (Visual Studio Code or code-server) Understanding of data formats (CSV, JSON) Experience with data transformation concepts No prior Senzing experience required Workshop Details Cost: Free for AWS-hosted events\nDuration: Estimated 1-2 hours\n"
},
{
	"uri": "//localhost:1313/tags/index.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]