[
{
	"uri": "/1_introduction/index.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Workshop Overview Welcome to the One-click Entity Resolution workshop! This workshop teaches you to use AI prompts and agentic workflows to streamline the entire entity resolution process - from mapping source data, to running Senzing utilities, to analyzing resolution results through conversational AI.\nYou\u0026rsquo;ll work with realistic data examples and use tools like Amazon Q Developer and the Senzing MCP Server to experience how AI can transform hours of manual work into minutes of guided interaction.\nLearning Outcomes By completing this workshop, you will know how to:\nMap your real source data to Senzing using the 5-stage Senzing Mapping Assistant workflow Get answers to your mapping questions and learn best practices through AI Load data into Senzing and take snapshots for analysis Use conversational AI to explore entity resolved data Validate that everything is working correctly at each step Intended Audience This workshop is designed for:\nSoftware developers and data engineers Technical professionals working with data integration and ETL workflows Anyone interested in AI-assisted data transformation Anyone interested in entity resolution Prerequisites:\nBasic familiarity with IDEs (Visual Studio Code or code-server) Understanding of data formats (CSV, JSON) Experience with data transformation concepts Prior Senzing experience is not required Workshop Conventions Throughout this workshop, you\u0026rsquo;ll encounter various visual elements:\nTips provide helpful suggestions and best practices.\nInfo boxes highlight important information or context.\nWarnings alert you to common pitfalls or important considerations.\nCode Blocks:\n# Command-line instructions look like this python3 mapper.py input.csv -o output.jsonl Validation Checkpoints: ‚úÖ Green checkmarks indicate validation steps to confirm your progress\n"
},
{
	"uri": "/scratch.html",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Introduction Overview What is entity resolutiuon?\nshows the entity resolution image here.\nThe data mapping challenge: Data mapping is the process of translating data from a source schema to a target schema.\nSource Schema: Your organization\u0026rsquo;s data (customer records, watchlist data, etc.) Target Schema: Senzing\u0026rsquo;s entity resolution format The Challenge: Understanding how your source attributes map to Senzing\u0026rsquo;s standardized format Overview Specifically, you will be able to:\nUnderstand the Senzing entity resolution data model and requirements Use AI tools (Amazon Q Developer) to assist with data mapping workflows Analyze your source data and what it contains Apply best practices for production data mapping projects Create mapping logic that transforms source data to Senzing format Load mapped data into Senzing and validate the results Prerequisites Basic understanding of data formats (CSV, JSON) Basic Python knowledge (helpful but not required) Basic understanding of AWS core services AWS account with appropriate permissions (provided in AWS-led event) Familiarity with using an IDE (preferably code-server) Familiarity with using an AI assistant Familiarity with command-line interfaces What You\u0026rsquo;ll Build Throughout this workshop, you will create a complete, production-ready data mapping pipeline:\nSchema analysis documents profiling your source data Mapping specifications documenting field-by-field decisions Python mapper code that transforms CSV to Senzing JSON format Validated entity data ready to load into Senzing Entity resolution results showing how Senzing identifies duplicates Workshop Structure This workshop is divided into the following modules:\nModule Duration Description Module 1: Introduction 10 minutes Workshop overview and learning objectives Module 2: Setup \u0026amp; Configuration 15 minutes Access your development environment and configure Amazon Q Module 3: Understanding Senzing Mapping 30 minutes Learn the Senzing data model and mapping workflow Module 4: Hands-On - Map Customer Data 60 minutes Use AI to map customer records to Senzing format Module 5: Load and Validate 30 minutes Load your mapped data and see entity resolution in action Module 6: Bonus - Watchlist Mapping 45 minutes Optional: Map complex international watchlist data Module 7: Cleanup and Next Steps 10 minutes Clean up resources and explore what\u0026rsquo;s next Total Time: Approximately 2.5-3 hours (core modules 1-5)\nWorkshop Modules Module 1: Introduction Get oriented with the workshop goals, structure, and the data mapping challenge you\u0026rsquo;ll solve.\nModule 2: Setup and Configuration Access your development environment and authenticate with Amazon Q Developer for both the IDE and CLI.\nModule 3: Understanding Senzing Mapping Learn the Senzing entity resolution data model, the difference between Features and Payload attributes, and the 5-stage mapping workflow.\nModule 4: Hands-On Customer Mapping ‚≠ê Core Module Apply what you learned: analyze customer data, make mapping decisions with AI guidance, generate mapper code, and validate your output.\nModule 5: Load and Validate See your mapping in action by loading data into Senzing and exploring entity resolution results.\nModule 6: Bonus - Watchlist Mapping (Optional) Challenge yourself with more complex data including nested structures, international names, and entity relationships.\nModule 7: Cleanup and Next Steps Clean up your AWS resources and learn how to apply these skills to your own data.\nGetting Started Ready to begin? Here\u0026rsquo;s what happens next:\nRead Module 1 to understand the context and learning objectives Complete Module 2 to set up your development environment Learn the concepts in Module 3 before diving into hands-on work Map customer data in Module 4 using AI assistance Validate your work in Module 5 by loading into Senzing Self-Paced Approach: This workshop is designed for self-paced learning. Take your time, use the validation checkpoints, and reference the complete solutions if you get stuck.\nWorkshop Conventions Throughout this workshop, you\u0026rsquo;ll encounter various visual elements to help guide your learning:\nTips provide helpful suggestions and best practices.\nInfo boxes highlight important information or context.\nWarnings alert you to common pitfalls or important considerations.\nCode Blocks:\n# Command-line instructions look like this python3 mapper.py input.csv -o output.jsonl Validation Checkpoints: ‚úÖ Green checkmarks indicate validation steps to confirm your progress\nSupport and Resources Workshop Support During Instructor-Led Events:\nAsk your instructor or workshop facilitators Use the event chat or Q\u0026amp;A features For Self-Paced Learners:\nReview the complete solutions provided in each module Check the troubleshooting sections Consult the Senzing documentation links below Senzing Resources Senzing Documentation: https://senzing.com/docs/ Senzing GitHub: https://github.com/senzing Senzing Community Support: https://senzing.com/support/ AWS Resources Amazon Q Developer Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/ AWS Builder ID: https://profile.aws.amazon.com/ AWS Workshop Studio: https://workshops.aws/ Goals By the end of this workshop, you will have:\nPractical Skills:\nMapped real data to Senzing format using AI assistance Created production-ready mapper code Validated your mappings with professional tools Deep Understanding:\nKnow the Senzing entity resolution data model Understand Features vs Payload attributes Recognize common mapping patterns and best practices Repeatable Process:\nLearned the 5-stage Senzing Mapping Assistant workflow Can apply this workflow to any data source Know how to validate and iterate on your mappings The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\nReady to get started? Let\u0026rsquo;s begin with Module 1: Introduction!\nafter setup there should be Modules for\nSome sort of the problem with mapping and why this was built? Exercise Workflow Exercise 1 - mapping the customer file Exercise 2 - mapping the watchlist file some sort of wrap up In each exercise we will have the same tasks (the overview will briefly describe the steps and tools)\nExamine the source data Provide or create the schema Use the senzing mapping assistant to map it Validate the mapping with the json analyzer Load the data in Senzing Take a snaphot Analyze the snapshot for stats and examples Review the examples with the mcp server by asking how and why questions Source Data (Your Format):\nCSV with columns like: customer_id, customer_name, dob, address, email\u0026hellip; May have organization-specific field names Mixed data quality and completeness Target Data (Senzing Format):\nJSON with standardized features: NAME, DATE_OF_BIRTH, ADDRESS, EMAIL\u0026hellip; Specific attribute names (NAME_FIRST, NAME_LAST, ADDR_LINE1\u0026hellip;) Clear distinction between matching features and payload data The Challenge Knowing which source fields map to which Senzing features, and how to transform the data correctly.\nFor each source field, you must decide:\nShould it be a Feature (used for entity matching)? Should it be Payload (stored but not matched)? Which specific Senzing attribute name is correct? Does the data need transformation (date formats, name parsing, etc.)? Example decisions:\nYour CSV has customer_name - should this map to NAME_FULL? Or split into NAME_FIRST and NAME_LAST? You have id_number - is this an SSN? Driver\u0026rsquo;s License? Passport? Your registration_date - is this legal entity registration or customer signup date? (Very different meanings!) Traditional Manual Approach Without AI assistance, data mapping typically requires:\nReading specifications - Understanding hundreds of Senzing features and attributes Making decisions - Determining appropriate mappings for each field Writing code - Building transformation logic Manual validation - Checking outputs meet requirements Iterative debugging - Finding and fixing errors Time cost: 3-6 days for a typical dataset with 15-20 fields\nThe AI-Assisted Solution This workshop teaches you to use the Senzing Mapping Assistant - a structured AI workflow that:\n‚úÖ Reads specifications instantly - AI loads the complete Senzing spec into context ‚úÖ Suggests mappings - AI recommends appropriate features based on your data ‚úÖ Generates code - AI writes the mapper automatically ‚úÖ Creates documentation - Mapping decisions documented as byproduct\nTime savings: Same work in under an hour\nImportant Caveat: AI is your assistant, not a replacement for your judgment. YOU still need to:\nUnderstand what your source fields mean Make business decisions about what should influence matching Validate that AI suggestions align with your requirements Correct AI when it misunderstands your data What You\u0026rsquo;ll Learn In this workshop, you\u0026rsquo;ll learn:\nThe Senzing entity model (Features vs Payload, entity types) How to work with AI to make mapping decisions A repeatable 5-stage workflow for any dataset Professional validation tools to verify correctness How to load and analyze entity resolution results Ready to understand Senzing\u0026rsquo;s data model?\nThe 5 Stages STAGE 1: INIT - Load Reference Materials You load the Senzing specifications and reference materials into the AI context. This gives the AI the knowledge it needs to guide you through mapping.\nWhat happens:\nLoad entity specification Load mapping examples Load validation tools Load identifier and usage type crosswalks Your role: Copy prompts to load files into AI context\nSTAGE 2: INVENTORY - Analyze Source Data The AI analyzes your source data schema to understand what fields you\u0026rsquo;re working with.\nWhat happens:\nAI reviews pre-generated schema file Identifies field names, data types, population rates Notes patterns and potential issues Your role: Provide schema file (generated by sz_schema_generator.py)\nSTAGE 3: PLANNING - Identify Entities and Sources You work with the AI to determine:\nWhat DATA_SOURCE value to use Are there multiple entity types in your data? Any special handling needed? What happens:\nAI asks clarifying questions You make business decisions Establish mapping strategy Your role: Answer questions about your data and business context\nSTAGE 4: MAPPING - Field-by-Field Decisions This is the core mapping work. For each source field, you and the AI determine:\nShould it be a Senzing Feature (used for matching)? Should it be Payload (stored but not matched)? Should it be ignored? What is the correct Senzing attribute name? What happens:\nAI suggests mappings based on field names and content You confirm or override decisions Handle edge cases (e.g., dynamic identifier types) Decide on data transformations (date formats, name parsing) Your role: Make informed decisions with AI guidance\nSTAGE 5: OUTPUTS - Generate Code and Documentation The AI generates production-ready deliverables:\nOutputs:\nMapper specification (markdown documentation of all decisions) Python mapper (executable code) Sample output (example mapped JSON records) README (usage instructions) What happens:\nAI writes the mapper based on Stage 4 decisions You run the mapper Validate output with linting and analysis tools Your role: Review generated code, run mapper, validate output\nWhy This Workflow Works Structured Decision-Making: Each stage has a clear purpose and deliverable\nValidation Gates: You verify correctness at each stage before proceeding\nIterative Refinement: Easy to go back and adjust decisions\nDocumentation Trail: Complete record of why each mapping decision was made\nProduction Ready: Output includes code, documentation, and validation\nThis isn\u0026rsquo;t just code generation - it\u0026rsquo;s AI-assisted decision-making with you in control. The AI guides you through the specifications and best practices, but YOU make the final decisions based on your understanding of the data.\nThe Full Prompt The complete Senzing Mapping Assistant prompt with detailed instructions for each stage is available at:\nworkshop/senzing/prompts/senzing_mapping_assistant.md\nYou\u0026rsquo;ll load this prompt in Module 4 when you start the hands-on customer mapping exercise.\nWhat to Expect in Module 4 In the next module, you\u0026rsquo;ll:\nLoad the Mapping Assistant into Amazon Q Developer Work through all 5 stages for the customer dataset Make real mapping decisions with AI guidance Generate a working mapper Validate your output with linting and analysis tools Estimated time: 60 minutes for complete customer mapping\nReady to learn about the validation tools you\u0026rsquo;ll use?\n"
},
{
	"uri": "/2_setup_configuration/index.html",
	"title": "Setup and Configuration",
	"tags": [],
	"description": "",
	"content": "Setup and Configuration Overview Welcome to the Amazon Q Workshop! üöÄ Before diving into the hands-on activities, you need to complete the setup phase to ensure your development environment is properly configured.\nThis module will guide you through accessing your development environment and connecting to Amazon Q Developer, whether you\u0026rsquo;re participating in an AWS-hosted event or running the workshop independently.\nGoals In this module you will:\nAccess your development environment (cloud-based IDE or local setup)\nConnect to Amazon Q Developer using appropriate authentication\nVerify your setup is ready for the workshop activities\nSetup Options You can run the workshop either as part of an AWS event or on your own. Based on your scenario, choose the relevant tab:\nüè¢ AWS-hosted event üíª Self-guided If you are attending an AWS-hosted event, you will:\nAccess a cloud-based Integrated Development Environment (IDE) that has been provisioned on your behalf with all needed software and sample code repository that will be used throughout the workshop.\nConnect to Amazon Q Developer in your IDE and the CLI using an AWS Builder ID.\nTo learn how to access your cloud-hosted IDE, please head to AWS-hosted event section\nIf you are running this workshop on your own AWS account, you\u0026rsquo;ll need to:\nSetup Visual Studio Code in your local machine with all needed plugins and dependencies\nDownload the workshop code\nSet up authentication for Amazon Q Developer in your IDE and the CLI using either your organization\u0026rsquo;s IDC Professional license or using an AWS Builder ID.\nIf you are running this workshop on your own AWS account, remember to delete all resources by following the Clean Up Resources section to avoid unnecessary charges.\nTo learn how to configure your local development environment, please head to Self-guided section\nThe examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various AWS services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n"
},
{
	"uri": "/3_workshopoverview/index.html",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Workshop Overview Overview Before diving into hands-on mapping, let\u0026rsquo;s understand the mapping challenge, explore Senzing\u0026rsquo;s AI-assisted solution, and tour the workshop materials.\nThis module prepares you with the context needed for successful data mapping in the hands-on exercises.\nWhat You\u0026rsquo;ll Learn In this brief module, you will:\nUnderstand the Data Mapping Challenge - What mapping is and why it\u0026rsquo;s traditionally difficult Learn Senzing\u0026rsquo;s AI Assisted Solution - The 5-stage Senzing Mapping Assistant workflow Tour the Workshop Folder - Understanding where everything is located Explore with AI - Practice using AI to understand workshop materials Estimated Reading Time: 10-15 minutes\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "/4_exercise1_customers/index.html",
	"title": "Exercise 1: Customers",
	"tags": [],
	"description": "",
	"content": "Exercise 1: Customers Map and Load Customer Data Overview In this exercise, you\u0026rsquo;ll map customer data from CSV format to Senzing\u0026rsquo;s entity resolution format using AI assistance. You\u0026rsquo;ll then load the data into Senzing and analyze the results to see how entity resolution identifies duplicate records.\nWhat You\u0026rsquo;ll Learn By completing this exercise, you will:\nExamine source data to understand its structure Use AI to generate data schemas automatically Apply the Senzing Mapping Assistant workflow to create a mapper Validate your mapping with analysis tools Load data into Senzing for entity resolution Take a snapshot and analyze resolution statistics Use the MCP server to understand resolution decisions Exercise Steps This exercise follows a structured 7-step workflow:\nExamine the source data - Understand what data you\u0026rsquo;re working with Generate the schema - Document the data structure Use the Senzing Mapping Assistant - AI-guided mapping workflow (5 stages) Validate the mapping - Verify output with JSON analyzer Load the data - Import mapped records into Senzing Analyze resolved data - Capture snapshot, review statistics, and explore entities Key takeaways - Reflect on AI-assisted development Estimated Time: 30-45 minutes\nComplete each step before moving to the next. Each step builds on the previous one.\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "/6_exercise2_watchlist/index.html",
	"title": "Exercise 2: Watchlist",
	"tags": [],
	"description": "",
	"content": "Exercise 2: Watchlist Map and Load Watchlist Data Overview In this exercise, you\u0026rsquo;ll map watchlist data from JSON format (FollowTheMoney schema) to Senzing\u0026rsquo;s entity resolution format. Watchlist data is more complex than customer data, featuring international names, relationships, and sanctions information.\nWhat You\u0026rsquo;ll Learn By completing this exercise, you will:\nWork with JSON source data (vs CSV from Exercise 1) Handle complex nested data structures Map international names and identifiers Process entity relationships Combine watchlist data with existing customer data Analyze cross-dataset entity resolution Exercise Steps This exercise follows the same 8-step workflow as Exercise 1:\nExamine the source data - Understand watchlist JSON structure Provide or create the schema - Document the data structure Use the Senzing Mapping Assistant - AI-guided mapping workflow Validate the mapping - Verify output with JSON analyzer Load the data in Senzing - Import mapped records Take a snapshot - Capture resolution results Analyze the snapshot - Review statistics and examples Review with MCP server - Ask how and why questions Estimated Time: 30-45 minutes\nWatchlist data presents new challenges: nested JSON structures, international names, and relationships. Apply what you learned in Exercise 1!\nThis exercise builds on Exercise 1. You\u0026rsquo;ll see how watchlist entities resolve against customer data.\nLet\u0026rsquo;s get started!\n"
},
{
	"uri": "/7_cleanup/index.html",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources Overview If you ran this workshop in your own AWS account, it\u0026rsquo;s important to clean up the resources you created to avoid ongoing charges.\nResources to Clean Up Depending on which parts of the workshop you completed, you may have created the following resources:\nAmazon Q Developer configurations (no charges) Any AWS services used during the workshop exercises Cleanup Steps 1. Amazon Q Developer Amazon Q Developer authentication and IDE configurations don\u0026rsquo;t incur charges, but you can disconnect if desired:\nIn VS Code, you can sign out of Amazon Q through the extension settings CLI authentication can be cleared with q logout (if available) 2. AWS Resources If you created any AWS resources during the workshop:\nReview your AWS Console for any resources created during the workshop Delete resources in the reverse order of creation to avoid dependency issues Check for any CloudFormation stacks that may have been created Verify S3 buckets are empty before deletion Verification To ensure all resources are cleaned up:\nCheck your AWS billing dashboard for any unexpected charges Review the AWS Cost Explorer for resource usage during the workshop period Set up billing alerts if you haven\u0026rsquo;t already Most of this workshop focuses on Amazon Q Developer setup and configuration, which doesn\u0026rsquo;t create billable AWS resources. However, always verify your account to ensure no unexpected resources remain.\nNeed Help? If you\u0026rsquo;re unsure about any resources or cleanup steps:\nReview the AWS documentation for the specific services you used Contact AWS Support if you have questions about billing or resource cleanup Check the AWS forums for community assistance "
},
{
	"uri": "/getting-started.html",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "Workshop architecture The following architecture diagram illustrates the various components of the workshop.\nPreparing for the workshop Follow the installation instructions in this section to prepare your environment for the workshop.\nIf you are attending an AWS guided event, setup your environment here. If you are not participating in an AWS guided event, setup your environment here. ::alert[If you are running this workshop on your own AWS account, remember to delete all resources by following the Clean Up Resources section to avoid unnecessary charges.]{header=Note}\n"
},
{
	"uri": "/2_setup_configuration/21_aws_guided_event/index.html",
	"title": "Running the Workshop at an AWS Guided Event",
	"tags": [],
	"description": "",
	"content": "Running the Workshop at an AWS Guided Event AWS-Hosted Workshop Setup Follow these steps to complete your environment setup:\nStep Action Description 1 Join the Event Access AWS Workshop Studio with your event code 2 Access Cloud IDE Log into your pre-configured code-server environment with Senzing and workshop materials 3 Setup Amazon Q Developer Authenticate with Amazon Q Developer using AWS Builder ID 4 Configure Senzing MCP Connect Amazon Q to Senzing entity resolution tools 5 Resources \u0026amp; Support Important documentation links and workshop support "
},
{
	"uri": "/2_setup_configuration/22_own_aws_account/index.html",
	"title": "Running the Workshop in your own AWS Account",
	"tags": [],
	"description": "",
	"content": "Running the Workshop in your own AWS Account Only follow the instructions in this section if you are using your own account. Click here for instructions on running the workshop in an AWS-hosted event.\nIf you are running this workshop on your own AWS account, you will:\nStep Action Explanation 1 Configure IDE Setup Visual Studio Code with all needed plugins, dependencies, and download the workshop code. 2 Setup Amazon Q Developer Set up authentication for Amazon Q Developer in your IDE and the CLI using either your organization\u0026rsquo;s IDC Professional license or using an AWS Builder ID. If you are running this workshop on your own AWS account, remember to delete all resources by following the Clean Up Resources section to avoid unnecessary charges.\n"
},
{
	"uri": "/2_setup_configuration/23_amazonqsetup.html",
	"title": "Amazon Q Setup",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will:\nSet up authentication for both the Amazon Q IDE extension and CLI using AWS Builder ID Using AWS IAM Identity Center? See the Amazon Q Developer documentation for instructions on connecting Amazon Q with your IDC configuration.\nDon\u0026#39;t have an AWS Builder ID? AWS Builder ID is a personal profile that provides access to select tools and services including Amazon CodeCatalyst, Amazon Q Developer, and AWS Training and Certification. AWS Builder ID is free and you don\u0026rsquo;t need to enter any credit card details upon creation of a profile. For more information, refer to the documentation.\nTo create a profile:\nEnter your email address -\u0026gt; Next. Enter your name -\u0026gt; Next. Check your email for a verification code (subject: \u0026ldquo;Verify your AWS Builder ID email address\u0026rdquo;) Enter the verification code -\u0026gt; Verify Create a password -\u0026gt; Create AWS Builder ID Enable workspace indexing for enhanced AI assistance Authentication Setup Important: Complete authentication for both the IDE Plugin and CLI tabs below to access Amazon Q\u0026rsquo;s full capabilities.\nIDE Plugin CLI In the Amazon Q extension, select Use for free ‚Üí Continue. When prompted, select Proceed to Browser When asked \u0026ldquo;Do you want Code (or code-sever) to open the external website?\u0026rdquo;, choose Open. In the browser: You will be redirected to the AWS Builder ID Login flow. Login with your credentials.\nConfirm the pre-populated code by clicking Confirm and continue.\nSelect Allow access when prompted\nYou\u0026rsquo;ll see a \u0026ldquo;Request approved\u0026rdquo; confirmation\nReturn to your IDE\nIn your terminal, run:\nq login Select Use for Free with Builder ID and press Enter\nOpen the provided URL in your browser: Select Confirm and continue Select Allow access After seeing \u0026ldquo;Request approved\u0026rdquo;, return to your terminal Test your setup with:\nq chat \u0026#34;Hello, Amazon Q!\u0026#34; --no-interactive Enable Additional Features IDE Plugin CLI Enable Workspace Context To make Amazon Q aware of your entire codebase:\nClick on \u0026lsquo;Amazon Q\u0026rsquo; in the status bar Select \u0026lsquo;Open Settings\u0026rsquo; Select the check-box under Amazon Q: Workspace Index to enable workspace indexing Initial workspace indexing takes 1-20 minutes and may increase CPU usage. Subsequent changes update the index incrementally.\nExample queries using workspace context üí¨ @workspace Where is the business logic to handle users? üí¨ @workspace Explain the data flow between the front-end and back-end. üí¨ Add new API tests using the existing test utilities found in the @workspace. Experimental Features The CLI provides access to experimental features that you can optionally enable:\nLaunch the Q chat interface: q chat Access the experiments section: /experiment Browse and toggle available experimental features: Use Spacebar to enable/disable features Note: No experimental features are required for this workshop.\nWrap up Checkpoint: Ensure you have completed authentication and feature setup for both the IDE Plugin and CLI before proceeding.\nYou\u0026rsquo;ve now configured Amazon Q in both your IDE and CLI using your AWS Builder ID, giving you AI assistance across different interfaces.\nYou\u0026rsquo;re now ready to start the workshop!\n"
},
{
	"uri": "/2_setup_configuration/25_senzingmcpsetup/index.html",
	"title": "Senzing MCP Server Setup",
	"tags": [],
	"description": "",
	"content": "Overview The Senzing MCP (Model Context Protocol) Server provides Amazon Q Developer with direct access to Senzing entity resolution capabilities. This integration allows you to interact with your Senzing database using natural language queries through Amazon Q.\nIn this section, you will:\nConfigure the pre-installed Senzing MCP server in Amazon Q Developer Authorize MCP tools for Amazon Q to use Verify the integration is working The MCP server provides 8+ tools for entity search, relationship analysis, and resolution explanation. This makes it easy to explore and understand entity resolution results using conversational AI.\nPrerequisites Amazon Q Developer authenticated (configured in previous section) Senzing SDK installed and initialized in your environment (pre-configured in workshop) Senzing MCP server add on pre-installed (pre-configured in workshop) What You\u0026rsquo;ll Learn By the end of this section, you\u0026rsquo;ll have:\nConnected Amazon Q Developer to the Senzing MCP server Authorized Senzing tools for use in your AI chat interface Verified the integration works with test queries Gained the ability to query entity resolution results conversationally Benefits of MCP Integration Throughout this workshop, you\u0026rsquo;ll use this integration to:\nQuery entity resolution results conversationally Understand how Senzing identified duplicate entities Analyze relationships between resolved entities Get explanations of resolution decisions Explore your mapped data interactively Getting Started Let\u0026rsquo;s begin by configuring the MCP server connection!\nThe setup process takes approximately 5-10 minutes. Follow each step carefully for a smooth experience.\n"
},
{
	"uri": "/2_setup_configuration/27_resourcesandsupport.html",
	"title": "Resources and Support",
	"tags": [],
	"description": "",
	"content": "Resources and Support Now that your environment is configured, here are important resources you can reference throughout the workshop.\nWorkshop Support During Instructor-Led Events Ask your instructor or workshop facilitators Use the event chat or Q\u0026amp;A features For Self-Paced Learners Review the complete solutions provided in each module Check the troubleshooting sections within each module Consult the Senzing and AWS documentation links below All workshop modules include validation checkpoints. If you encounter issues, check that you\u0026rsquo;ve completed each validation step before proceeding.\nSenzing Resources Senzing Documentation: https://senzing.com/docs/ Senzing GitHub: https://github.com/senzing Senzing Community Support: https://senzing.com/support/ The Senzing documentation includes:\nComplete entity specification reference API documentation for the Senzing SDK Integration guides for various platforms Best practices for entity resolution AWS Resources Amazon Q Developer Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/ AWS Builder ID: https://profile.aws.amazon.com/ AWS Workshop Studio: https://workshops.aws/ Amazon Q Developer provides:\nCode generation and analysis Chat-based assistance for development tasks Integration with your IDE Access to AWS documentation and best practices You\u0026rsquo;ll use Amazon Q Developer extensively in Module 4 when mapping customer data. Familiarize yourself with its chat interface in your IDE.\nNext Steps With your environment configured and resources bookmarked, you\u0026rsquo;re ready to learn about Senzing\u0026rsquo;s entity resolution model and the mapping workflow.\nContinue to Module 3: Understanding Senzing Mapping to build the conceptual foundation before hands-on work.\n"
},
{
	"uri": "/3_workshopoverview/30_mappingchallenge.html",
	"title": "What is Data Mapping",
	"tags": [],
	"description": "",
	"content": "What is Data Mapping? Data mapping is the process of defining how data fields from a source system correspond to fields in a target system.\nWhy Data Mapping is Hard Data mapping projects face several common challenges:\nComplex schemas on both sides\nSource systems may have dozens or hundreds of fields with unclear naming conventions Target systems (like Senzing) have their own complex specifications with specific requirements Understanding how fields relate across systems requires deep knowledge of both Subject matter experts are scarce\nPeople who understand the source data are often too busy with operational work Original developers or data architects may have moved on to other projects Institutional knowledge about field meanings and business rules is hard to capture Takes a long time to understand\nReading through data dictionaries and specifications is tedious Testing mappings to verify correctness requires multiple iterations Edge cases and data quality issues only emerge during implementation Documentation is often incomplete or outdated "
},
{
	"uri": "/3_workshopoverview/31_mappingworkflow.html",
	"title": "Senzing&#39;s AI Assisted Approach",
	"tags": [],
	"description": "",
	"content": "Traditional vs AI-Assisted Mapping Traditional Approach Manually analyzing fields, writing custom code, debugging issues, and updating documentation took days or weeks for each data source. The process involved:\nReading through data dictionaries and sample files Consulting Senzing documentation repeatedly Writing mapper code from scratch Debugging field mappings and data transformations Maintaining separate documentation Senzing\u0026rsquo;s AI-Assisted Approach With AI assistance and the Senzing Mapping Assistant workflow, you can:\nAutomate schema analysis - AI reads and understands your data structure Get guided decisions on field mappings based on Senzing best practices Generate production-ready code automatically from your mapping decisions Validate output automatically using professional tools Complete mappings in hours instead of days What Makes This Different? This isn\u0026rsquo;t just a coding exercise or simple code generation. You\u0026rsquo;ll learn a repeatable workflow that combines:\nAI assistance for speed and accuracy Human decision-making for business context Proven validation tools for quality assurance Complete documentation for maintainability This isn\u0026rsquo;t just code generation - it\u0026rsquo;s AI-assisted decision-making with you in control. The AI guides you through the specifications and best practices, but YOU make the final decisions based on your understanding of the data.\n"
},
{
	"uri": "/3_workshopoverview/32_workshopfolder.html",
	"title": "Workshop Folder Tour",
	"tags": [],
	"description": "",
	"content": "Opening Your Workshop Environment All workshop materials are located in the /home/ubuntu/workshop/ directory.\nIn this section, you\u0026rsquo;ll explore the workshop folder structure and learn how AI can help you quickly understand unfamiliar tools and documentation - a key skill you\u0026rsquo;ll use throughout the workshop.\nOpen this folder now in your IDE:\nIn your code-server IDE, use the file explorer (left sidebar) Navigate to /home/ubuntu/workshop/ Expand the folders to see the structure The Workshop Folder Structure Here is a brief explanation of the main folders and their contents: workshop/ ‚îú‚îÄ‚îÄ senzing/ # Senzing toolkit - prompts, references, tools ‚îÇ ‚îú‚îÄ‚îÄ prompts/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ senzing_mapping_assistant.md # 5-stage AI workflow prompt ‚îÇ ‚îú‚îÄ‚îÄ reference/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ senzing_entity_specification.md # Complete Senzing feature documentation ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ senzing_mapping_examples.md # Real-world mapping examples ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ identifier_crosswalk.json # Identifier type lookup table ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ usage_type_crosswalk.json # Valid usage types for features ‚îÇ ‚îú‚îÄ‚îÄ tools/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ sz_schema_generator.py # Analyzes source data structure ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ lint_senzing_json.py # JSON validator for Senzing format ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ sz_json_analyzer.py # Analyzes mapped Senzing output ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ sz_default_config.json # Reference Senzing configuration ‚îÇ ‚îî‚îÄ‚îÄ SENZING_TOOLS_REFERENCE.md # Complete tool documentation ‚îú‚îÄ‚îÄ workspace/ # Your source data for exercises ‚îî‚îÄ‚îÄ solutions/ # Complete reference implementations The solutions folder is provided so you can catch up if you fall behind or check your work. Try the exercise yourself first - that\u0026rsquo;s where the learning happens!\nReady to explore the workshop materials with AI assistance? Let\u0026rsquo;s move to the next page where you\u0026rsquo;ll practice asking AI questions about the folder structure!\n"
},
{
	"uri": "/3_workshopoverview/33_explorewithai.html",
	"title": "Explore with AI",
	"tags": [],
	"description": "",
	"content": "Using AI to Understand the Workshop Files Rather than reading lengthy documentation, let\u0026rsquo;s use AI to explore the workshop folder structure. This demonstrates a key modern development skill: asking targeted questions to quickly understand unfamiliar codebases and tools.\nWhy ask AI? Instead of reading lengthy documentation, you can ask targeted questions and get immediate, contextual answers. This is a key skill for modern developers!\nOpening Amazon Q Developer First, let\u0026rsquo;s open Amazon Q Developer and give it context about the workshop folder.\nOpen Amazon Q Developer (click the Q icon in the left side panel of your IDE)\nTell Amazon Q: Use the senzing folder as context for this session\nThis tells Q to reference all the files in the senzing folder when answering your questions.\nQuestion 1: What does the senzing folder contain? Ask Amazon Q: What does the senzing folder contain?\nExpected Response: AI should explain that the senzing folder contains prompts, reference documentation, and tools for working with Senzing entity resolution.\nQuestion 2: What does the mapping assistant prompt do? Ask Amazon Q: What does the mapping assistant prompt do?\nExpected Response: AI should explain the 5-stage workflow (INIT ‚Üí INVENTORY ‚Üí PLANNING ‚Üí MAPPING ‚Üí OUTPUTS) and how the prompt guides you through structured data mapping.\nQuestion 3: What are those senzing tools and when do I use them? Ask Amazon Q: What are those senzing tools and when do I use them?\nExpected Response: AI should describe the schema generator (for analyzing source data), JSON linter (for validation), and JSON analyzer (for understanding mapped output), along with when to use each.\nWrap Up You\u0026rsquo;ve now practiced using AI to understand unfamiliar tools and documentation. This skill - asking AI targeted questions about code and tools - will be essential throughout the workshop.\nKey Takeaways:\nThe workshop/ folder contains everything you need: tools, data, and solutions AI can help you quickly understand complex folder structures and tools Asking targeted questions is faster than reading documentation cover-to-cover "
},
{
	"uri": "/4_exercise1_customers/41_examinesourcedata.html",
	"title": "Step 1: Explore Senzing",
	"tags": [],
	"description": "",
	"content": "Understanding Senzing Workshop Resources Before diving into mapping, it\u0026rsquo;s valuable to understand what resources Senzing provides to help you succeed. In this step, you\u0026rsquo;ll explore the senzing/ folder structure and learn about the tools and references available.\nOpen the Workshop Folder First, open the workshop folder in your IDE to see the project structure.\nIn your IDE file explorer:\nNavigate to the workshop/ directory Expand the folder to see its contents Add the Senzing folder as context Add context to Amazon Q:\nOpen Amazon Q Developer (if not already open) In the Q chat input field, type @ to open the context menu Select the senzing folder from the list Press Enter to add it as context Important: When AI\u0026rsquo;s fail it is because they do not have enough context. So let\u0026rsquo;s add the senzing folder as context for this exercise. Later on, if you find the AI not responding properly, try using the @ sign again to remind it!\nNow ask Amazon Q to explain what the senzing folder contains.\nTell Amazon Q: @senzing What does the senzing folder contain?\nWhat You\u0026rsquo;ll Discover The senzing/ folder contains workshop materials organized into 4 key sections:\n1. Prompts (prompts/)\nSenzing Mapping Assistant - The AI prompt that guides you through the 5-stage mapping workflow This is what orchestrates the entire mapping process you\u0026rsquo;ll use in Step 3 2. Reference Materials (reference/)\nsenzing_entity_specification.md - The complete Senzing entity model (2,100+ lines) Mapping examples - Real-world mapping patterns Crosswalk tables - Identifier and usage type mappings Entity diagrams - Visual guides to the data model 3. Tools (tools/)\nsz_schema_generator.py - Analyzes CSV files and generates schema documents lint_senzing_json.py - Validates JSON format during development sz_json_analyzer.py - Checks data quality before loading Config and loader utilities - For Senzing environment setup 4. Documentation\nSENZING_TOOLS_REFERENCE.md - Complete tool usage guide Understanding the Mapping Assistant The Mapping Assistant is the key to this exercise. Ask Q to explain what it does.\nAsk Amazon Q: What does the mapping assistant prompt do?\nThe 5-Stage Mapping Workflow The assistant creates a structured process:\nINIT - Loads references and validates tools INVENTORY - Extracts all source fields PLANNING - Identifies entities and DATA_SOURCE codes MAPPING - Dispositions fields as Feature/Payload/Ignore OUTPUTS - Generates README, specification, and mapper code Key features:\nInteractive guidance at each stage Built-in validation with linter Guardrails against AI hallucination Citations for all mapping decisions Learn About the Tools Ask Amazon Q: What are those senzing tools and when do I use them?\nTool Workflow Standard workflow order:\nSchema Generator ‚Üí Analyze source data structure AI Mapping ‚Üí Use Mapping Assistant to create mapper Analyzer ‚Üí Check quality of mapped output Config Tool ‚Üí Configure data sources in Senzing Loader ‚Üí Load data into Senzing Snapshot ‚Üí Analyze resolution results Workshop Development Tools (Python scripts) work anywhere. Senzing Core Tools require the Senzing environment to be configured. (Already done in this workshop)\nCheckpoint: You should understand the senzing folder structure, the 5-stage Mapping Assistant workflow, and when to use each tool.\n"
},
{
	"uri": "/4_exercise1_customers/42_createschema.html",
	"title": "Step 2: Generate Schema",
	"tags": [],
	"description": "",
	"content": "First: Examine Your Source Data Before generating the schema, you need to understand your data structure and business goals.\nOpen the customer CSV file:\nIn your IDE file explorer (left sidebar), navigate to:\nNavigate to the workshop/workspace/customers directory Expand the folder to see its contents Double-click customers.csv to open it in your IDE Review the data structure:\nScroll through the first 10-20 rows Note the column headers Observe data patterns and completeness Look for different customer types (Individual vs Company) Define your business objective: Are you trying to find duplicate customers? Will you match this against other datasets? What constitutes a \u0026ldquo;match\u0026rdquo; for your use case? Remember: You guide the AI - understanding your data and goals ensures better mapping results.\nGenerate a Data Schema Now that you\u0026rsquo;ve examined the customer data, you need to document its structure. A schema document provides essential information about field types, population rates, and sample values - critical details for accurate mapping.\nWhy Generate a Schema? When mapping data, AI needs to understand both the source and target schemas. However, you shouldn\u0026rsquo;t upload entire data files to AI because:\nPrivacy concerns - Files may contain sensitive customer information Context limits - AI models have limited context windows and cannot process large files Accuracy - AI can infer schema from sample rows, but this is error-prone Instead, you\u0026rsquo;ll use the Senzing schema generator tool to create a comprehensive schema document that includes:\nField names and data types Population percentages (how often fields contain values) Sample values and ranges Field statistics Pro Tip: For complex schemas the tool cannot handle, locate the official schema documentation or ask AI to help extract schema information from documentation.\nGenerate the Schema with Amazon Q Ask Amazon Q to generate the schema:\nOpen Amazon Q Developer (click the Q icon in the left sidebar)\nTell Amazon Q: Generate a schema for the customer CSV\nSecurity Note: Notice that Q asks for your approval before running the schema generator. This is an important security feature - never give tools blanket access to run commands without review. Always verify what actions AI wants to take before approving.\nQ Developer will run the schema generator tool and create a customer_schema.md file in your workspace.\nReview Q\u0026rsquo;s response:\nQ Developer will run the schema generator tool and create a customer_schema.md file in your workspace.\nReview the Generated Schema Compare the actual schema with the AI summary:\nIn your IDE file explorer, navigate to workshop/workspace/customers/ Open customer_schema.md Compare the actual schema content with the AI summary below Does the summary match what you see in the file?\nSchema Analysis: 120 Records, 19 Fields The schema generator will show you detailed statistics for your customer data:\nCore Identity (100% population):\ncustomer_id - Unique record identifier customer_name - Full customer names customer_type - \u0026ldquo;I\u0026rdquo; (Individual/Person) or \u0026ldquo;C\u0026rdquo; (Company/Organization) Demographics:\ngender (17%) - Gender information (sparse) dob (53%) - Date of birth (moderate coverage) Location Information (varying coverage):\naddress (61%) - Street addresses city, state, zip_code, country - Address components Contact Information:\nphone (18%) - Phone numbers (sparse) email (38%) - Email addresses (moderate) Identifiers (30% population): The schema shows a dynamic identifier pattern with three related fields:\nid_type - Type code (PASSPORT, DRIVERS_LICENSE, SSN, NATIONAL_ID) id_number - The identifier value id_country - Issuing country This pattern means you\u0026rsquo;ll need special handling to map these to the correct Senzing features.\nBusiness/Operational Data:\nregistration_date (83%) - When customer joined account_status (83%) - Active/Inactive status account_balance (75%) - Current balance customer_tier - Customer classification Dataset Composition Entity Mix:\n114 Person records (customer_type = \u0026ldquo;I\u0026rdquo;) 6 Organization records (customer_type = \u0026ldquo;C\u0026rdquo;) Total: 120 records This mix means your mapper needs to handle both PERSON and ORGANIZATION entity types, mapping fields conditionally based on customer_type.\nData Quality Observations High-value matching fields: customer_name (100%), dob (53%), address (61%) Sparse fields: gender (17%), phone (18%) - less reliable for matching Good identifier coverage: 30% have government IDs Strong operational data: 75-83% coverage on business fields This schema document will guide the AI during mapping to make informed decisions about which fields to use for entity resolution.\nFallback: Use the Pre-Generated Schema If you encounter issues generating the schema, a pre-generated version is available in the solutions folder:\nworkshop/solutions/customers/customer_schema.md\nCheckpoint: You should have a customer_schema.md file that documents all fields, data types, and sample values from the customer CSV.\n"
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/index.html",
	"title": "Step 3: Map with Assistant",
	"tags": [],
	"description": "",
	"content": "Map Data with the Senzing Mapping Assistant Now you\u0026rsquo;ll use the Senzing Mapping Assistant prompt with Amazon Q to guide you through the mapping process. This AI-assisted workflow takes you through 5 structured stages to create a complete, production-ready mapper.\nThe 5-Stage Workflow The Mapping Assistant guides you through:\nINIT - Load reference files and verify tools are available INVENTORY - Analyze your source data schema PLANNING - Determine data source name and entity types MAPPING - Map each field to Senzing features or payload OUTPUTS - Generate mapper script, documentation, and sample output Key Principle: The AI guides you through each stage, but you make the final decisions. Answer questions thoughtfully, validate suggestions, and don\u0026rsquo;t hesitate to ask \u0026ldquo;why?\u0026rdquo; about any mapping choice.\nAI Variability: AI responses may differ from what\u0026rsquo;s shown in this workshop. This is normal - AI models generate contextual responses based on your specific data and environment. If your AI\u0026rsquo;s response differs:\nFollow the AI\u0026rsquo;s guidance - It\u0026rsquo;s analyzing your actual data Ask clarifying questions if something seems unclear Use the workshop examples as reference but adapt to your AI\u0026rsquo;s approach Focus on the end goal - a working mapper that handles your data correctly Ready to Start? Click through each stage in order. Each stage builds on the previous one and includes checkpoints to verify you\u0026rsquo;re on track.\nEstimated Time: 20-30 minutes\n"
},
{
	"uri": "/4_exercise1_customers/44_validatemapping.html",
	"title": "Step 4: Validate Mapping",
	"tags": [],
	"description": "",
	"content": "Run the Mapper on Full Dataset Now that you have a mapper, test it on the complete customer dataset to generate the Senzing JSON output.\nExecute the Mapper Tell Amazon Q: Run the mapper on the actual data\nQ will execute the mapper, run the linter, and run the JSON analyzer - all steps documented in the Senzing tools reference.\nManaging AI Context If the analyzer output doesn\u0026rsquo;t show errors or warnings, the AI may have lost context during the long conversation. When this happens, re-add the context using the @ symbol.\nContext Management: Long conversations can cause AI to forget earlier instructions. Use the @ symbol to re-add important reference files when needed.\nRe-add context to Amazon Q:\nIn the Q chat input field, type @ to open the context menu Select SENZING_TOOLS_REFERENCE.md from the list Press Enter to add it as context Tell Amazon Q: @SENZING_TOOLS_REFERENCE.md Rerun the json analyzer on the mapped customer JSON\nAbout Warnings: Warnings highlight data quality considerations, but they\u0026rsquo;re informational. The data is what it is - it\u0026rsquo;s just good to know these patterns in case something can be improved at the source!\nCheckpoint: Your mapped data should pass validation with no errors. All 120 records should be valid Senzing JSON with features correctly recognized.\n"
},
{
	"uri": "/4_exercise1_customers/45_loaddata.html",
	"title": "Step 5: Load Data",
	"tags": [],
	"description": "",
	"content": "Configure and Load Data into Senzing Now that your mapping is validated, configure the DATA_SOURCE and load the customer data into Senzing for entity resolution.\nStep 1: Add the DATA_SOURCE Ask Amazon Q: Add the data source for me\nStep 2: Load the Mapped Records Ask Amazon Q: Yes to the prior question, or load the mapped customer records into Senzing\nReal-Time Resolution: Unlike batch systems, Senzing resolves entities as data loads. By the time loading completes, all 120 records have been fully processed and resolved.\nCheckpoint: All 120 customer records should be loaded with 8 redo records processed. No errors should occur.\n"
},
{
	"uri": "/4_exercise1_customers/46_analyzeresolveddata.html",
	"title": "Step 6: Analyze Resolved Data",
	"tags": [],
	"description": "",
	"content": "Capture and Analyze Entity Resolution Results After loading data, take a snapshot and analyze how Senzing resolved your customer records. You\u0026rsquo;ll explore statistics, understand match patterns, and investigate specific entity examples.\nPart 1: Take Snapshot Generate the Snapshot Ask Amazon Q: Yes to the prior question, or take a snapshot\nUnderstanding the Results Key Result: 120 records ‚Üí 78 entities\nThis means Senzing identified 42 duplicate records (35% compression), consolidating them into their true entities.\nAI Analysis - Without You Even Asking Look at what Amazon Q has already told you, without you even asking:\nMost of your matches were made on name, address, and date of birth There are 31 simple duplications and 4 multi-record entities The compression rate was high - did you expect that? An interesting entity to check out is Entity 1, which has 5 records The Power of Proactive AI: The AI didn\u0026rsquo;t wait for you to ask questions - it analyzed the snapshot and surfaced the most important insights automatically. This is AI working for you.\nTraditional Analysis with sz_explorer Traditionally, you would use Senzing\u0026rsquo;s sz_explorer utility to review snapshots. You can still do this here!\nAsk Amazon Q: give me the command to open the snapshot with sz_explorer in a terminal using full paths to both\nOpen a new terminal:\nIn your IDE, open the terminal (View ‚Üí Terminal, or press Ctrl+`) Paste and run the command Q provides This is the traditional exploratory data analysis (EDA) interface. It provides a command-line menu system to explore entities, relationships, and statistics.\nTraditional Workflow: The sz_explorer tool is documented in the Senzing EDA guide. While powerful, it requires learning menu navigation and command syntax.\nIntroducing Senzing\u0026rsquo;s MCP Server! Here\u0026rsquo;s where it gets really cool. A snapshot only contains stats and entity IDs - no PII. That\u0026rsquo;s where the MCP server comes in. Let\u0026rsquo;s try a couple of things.\nWe already know that Entity 1 is the most interesting entity in the snapshot. So let\u0026rsquo;s start there.\nAsk Amazon Q: who is entity 1\nJust like that, Q queries the Senzing MCP server and shows you the complete entity profile - all the records that merged, the features that matched, even the payload data. No command syntax. No menu navigation. Just ask.\nNeed Technical Proof? Ask Amazon Q: how did senzing resolve entity 1\nNow you see the step-by-step resolution logic - which records merged when, what features matched at each step, even the similarity scores. This is the kind of detail that used to require digging through logs or complex API calls.\nThe Questions Are Only Limited By Your Imagination Try some of these:\nAsk Amazon Q: what possible matches should I review and why?\nAsk Amazon Q: show me that drivers license and dob match\nAsk Amazon Q: what relationships might qualify for a household and why? Can't just be same address, can it?\nNatural Language Exploration: That last question about households? I had to refine it a bit to get exactly what I wanted. That\u0026rsquo;s the beauty of conversational AI - you can iterate on your questions until you get the insight you\u0026rsquo;re looking for. Try it yourself!\nExercise 3 Complete! Checkpoint: You\u0026rsquo;ve captured snapshots, analyzed resolution statistics, explored the traditional sz_explorer tool, and used the MCP server to conversationally investigate entity resolution decisions.\nYou now have hands-on experience with the complete Senzing workflow - from mapping to loading to analysis. More importantly, you\u0026rsquo;ve seen how AI assistance transforms every step: from generating schemas, to validating mappings, to exploring resolved entities.\nThis is entity resolution, modernized.\n"
},
{
	"uri": "/4_exercise1_customers/47_keytakeaways.html",
	"title": "Step 7: Key Takeaways",
	"tags": [],
	"description": "",
	"content": "The Power of Agentic AI Throughout this exercise, you experienced a fundamentally different way of working with development tools. Instead of memorizing commands, navigating file systems, and context-switching between documentation and implementation, you simply described what you wanted to accomplish.\nWhat Changed Traditional Workflow Friction:\n\u0026ldquo;What directory is that file in again?\u0026rdquo; \u0026ldquo;What was the command syntax for this tool?\u0026rdquo; \u0026ldquo;Where did I save that output?\u0026rdquo; \u0026ldquo;What step comes next in the process?\u0026rdquo; \u0026ldquo;How do I format this configuration file?\u0026rdquo; AI-Assisted Workflow:\nYou told the AI: \u0026ldquo;Generate a schema, map the customer data, validate it, configure the data source, and load it into Senzing\u0026rdquo;\nThe AI handled:\nLocating the correct tools and directories Constructing proper command syntax Managing workflow context across the conversation Handling environment-specific configuration Remembering what was done previously Determining the next logical step Time Savings Compound The efficiency gains aren\u0026rsquo;t just about individual commands - they multiply across the workflow:\nNo context switching: Stay focused on your data and business goals No documentation searches: Tools are invoked with proper syntax automatically No mental overhead: The AI tracks what\u0026rsquo;s been done and what comes next Fewer errors: Commands are constructed correctly the first time Better learning: Ask \u0026ldquo;why\u0026rdquo; questions instead of \u0026ldquo;how\u0026rdquo; questions This is the Promise of AI-Assisted Development Less time fighting with tools. More time understanding your data and achieving results.\nThe technical skills - command syntax, file paths, tool options - become less critical. The conceptual skills - understanding your data, asking good questions, validating results - become more important.\nThis shift lets you focus on what matters: your data quality, your business logic, your entity resolution strategy. The AI handles the mechanics so you can focus on the meaning.\nReflection Questions As you continue to the next exercises, consider:\nWhat surprised you about working with the AI assistant during this workflow? Where did you intervene to correct or guide the AI, and why was that important? What questions did you ask that helped you understand entity resolution concepts better? How might you apply this AI-assisted approach to other data projects? The AI is a Tool, Not a Replacement: Notice that throughout this exercise, YOU made the important decisions - what fields to map, how to handle edge cases, whether mappings made sense. The AI accelerated execution, but your judgment drove the outcome.\nWhat\u0026rsquo;s Next: Ready to apply these skills to more complex data? The bonus watchlist exercise introduces international names, relationships, and nested data structures - all mappable with the same workflow you just mastered.\n"
},
{
	"uri": "/6_exercise2_watchlist/61_workflowsummary.html",
	"title": "Step 1: Workflow Summary",
	"tags": [],
	"description": "",
	"content": "Complete Mapping Workflow at a Glance This table provides a complete overview of the watchlist mapping workflow. You can complete the entire exercise by following this summary, or dive into the detailed pages for deeper exploration.\nQuick Start: This summary shows every prompt and action needed to map FTM watchlist data to Senzing format. Use it as a standalone guide or reference as you work through the detailed pages.\nImportant Notes Before You Start 1. Your Experience May Be Different This workflow shows what happened in a real mapping session, including:\nThe specific questions asked Corrections made when AI assumptions were wrong The context loss that occurred and recovery Your session will likely differ:\nAI might suggest different field mappings You may catch different errors Your data structure might require different decisions Context loss may happen at different points (or not at all) Use this as a guide, not a script. The principles are what matter, not matching it exactly.\n2. You Are In Charge Notice throughout this workflow:\nI directed the AI to things I knew about the data I questioned assumptions when I was uncertain I corrected errors when I spotted them I made final decisions on field dispositions Key examples:\n\u0026ldquo;Are you sure those are all the identifier types, and how do you know?\u0026rdquo; ‚Üí Found missing identifiers \u0026ldquo;previous name should be name_org, not name_full\u0026rdquo; ‚Üí Corrected convention error \u0026ldquo;You say 3 master entity types but list 2\u0026hellip;\u0026rdquo; ‚Üí Caught arithmetic mistake The AI is your assistant, not your replacement. You have the domain knowledge. The AI has Senzing knowledge. Together you make good decisions.\n3. What To Do If Things Go Wrong Context Loss (80% Warning)\nIf AI warns \u0026ldquo;Context ~80% full\u0026rdquo; and offers to compact: Say YES immediately If you decline: You will likely hit 100% and lose all context If context is lost: Use the recovery strategies in Step 6 Re-explore the repository Find artifacts (snapshot files, generated code) Reference documentation Continue from checkpoints AI Makes Wrong Assumptions\nQuestion it: \u0026ldquo;How do you know?\u0026rdquo; \u0026ldquo;Are you sure?\u0026rdquo; Check actual data: Don\u0026rsquo;t accept answers based on schema alone Correct errors immediately: The earlier you catch mistakes, the easier to fix Validation Errors\nLinter fails: Fix JSON syntax errors in generated output Analyzer shows critical errors: Usually data source configuration - follow the fix steps Load fails: Check error messages for specific record issues Can\u0026rsquo;t Continue (Session Ending/Context Full)\nIdeally: Use /resume when you return (if available in your AI tool)\nIf no /resume available: Just give context manually and tell it what to do next:\nExample recovery (what actually happened):\nYou: \u0026#34;Its already loaded. can you find the snapshot on the watchlist directory and show it to me\u0026#34; AI: [finds snapshot file and shows it] You: \u0026#34;in the senzing tools reference it tells you how to summarize the snapshot, can you do that\u0026#34; AI: [proceeds with analysis] Simple approach:\nTell AI where you are: \u0026ldquo;Data is loaded, snapshot is taken\u0026rdquo; Tell AI what to find: \u0026ldquo;Find the snapshot file in workspace/watchlist\u0026rdquo; Tell AI what to do next: \u0026ldquo;Analyze it according to the tools reference\u0026rdquo; See Step 6 for the complete recovery example.\nStep Action / Prompt Used Reason / Outcome 1. Generate Schema Prompt: \u0026ldquo;Generate a schema for the ftm.jsonl file I have open\u0026rdquo;\nAI runs: python3 senzing/tools/sz_schema_generator.py workspace/watchlist/ftm.jsonl -o workspace/watchlist/ftm_schema.md Understand the FTM data structure: 73 records, 5 schema types (Person, Company, Sanction, Ownership, Directorship), 31 unique fields 2. Start AI Mapping Prompt: \u0026ldquo;@senzing\u0026rdquo;\nThen: \u0026ldquo;Yes\u0026rdquo; [to start AI-assisted mapping] Load AI mapping assistant to guide through 5-stage workflow Stage 1: INIT AI loads 5 reference files: entity spec, examples, linter, identifier crosswalk, usage type crosswalk Provides AI with complete Senzing knowledge base and validation tools Stage 2: INVENTORY Prompt: \u0026ldquo;yes\u0026rdquo; [to proceed]\nAI extracts all 31 fields from schema Creates complete field inventory: root fields, Person attributes, relationship fields, identifier fields, sanction metadata. Integrity check: 31 extracted = 31 displayed Stage 3: PLANNING AI identifies master entities (Person, Company) and DATA_SOURCE codes (SANCTIONS, CORP_FILINGS) Your correction: \u0026ldquo;You say 3 master entity types but list 2: person and company. What is the third?\u0026quot;\nResult: Corrected to 2 master types, 3 relationship types Stage 4: MAPPING Prompt: \u0026ldquo;Show me the full mapping table first\u0026rdquo;\nAI maps each field to Senzing features, payload, or ignored Key decisions: Sanction records merge as payload, Ownership/Directorship become REL_POINTER relationships, identifiers merge onto Person entities Question: Identifier Types Prompt: \u0026ldquo;Are you sure those are all the identifier types, and how do you know?\u0026rdquo; Correction: AI checked actual data and found DRIVERS_LICENSE and SSN (not just assumed from schema) Question: Company Relationships Prompt: \u0026ldquo;I didn\u0026rsquo;t see any rel_pointers in the json examples, don\u0026rsquo;t companies have relationships as well?\u0026rdquo; Discovery: Found company-to-company ownership (Universal Exports owns 3 subsidiaries) Question: Relationship Roles Prompt: \u0026ldquo;What are my options for assigning roles to relationship pointers\u0026rdquo;\nYour decision: \u0026ldquo;The principal and president\u0026rdquo; Result: Use OWNER_OF for ownership, PRINCIPAL_OF/PRESIDENT_OF for directorships based on role values Question: Previous Names Your correction: \u0026ldquo;previous name should be name_org, not name_full\u0026rdquo; Fix: For organizations, use NAME_ORG for previous names Question: Company Identifiers Prompt: \u0026ldquo;Aren\u0026rsquo;t there any identifiers for companies?\u0026rdquo; Result: None in this dataset Stage 5: OUTPUTS Prompt: \u0026ldquo;yes\u0026rdquo; [to proceed to JSON generation]\nThen: \u0026ldquo;ok show me company json record examples and lint\u0026rdquo; Complete mapping implementation with multi-pass processing for relationships. AI generates README.md, ftm_mapper.md (spec), ftm_mapper.py (code) 3. Run Mapper Prompt: \u0026ldquo;ok lets run it on the actual data\u0026rdquo;\nAI runs: python3 ftm_mapper.py ftm.jsonl ftm_senzing.jsonl Result: 39 Senzing entities generated from 73 FTM records 4. Lint Output AI automatically runs: python3 senzing/tools/lint_senzing_json.py ftm_senzing.jsonl Result: ‚úÖ PASSED - No JSON syntax errors 5. Analyze Quality Prompt: \u0026ldquo;I didn\u0026rsquo;t see any errors or warnings. please run the json analyzer again according to tools reference documentation\u0026rdquo;\nAI runs: python3 senzing/tools/sz_json_analyzer.py ftm_senzing.jsonl -o analysis.md Result: ‚ùå Critical errors - DATA_SOURCE not found: CORP_FILINGS, SANCTIONS 6. Configure Data Sources Prompt: \u0026ldquo;I updated the senzing config. Run the json analyzer again\u0026rdquo;\nThen: \u0026ldquo;yes\u0026rdquo; [to configure data sources]\nAI creates config and runs: sz_configtool -f ftm_config.g2c Result: ‚úÖ Both data sources registered 7. Re-analyze AI runs: python3 senzing/tools/sz_json_analyzer.py ftm_senzing.jsonl -o analysis.md Result: ‚úÖ Critical Errors: 0 (resolved!), 13 Senzing features with good coverage 8. Load Data Prompt: \u0026ldquo;ok load it\u0026rdquo;\nAI runs: sz_file_loader -f ftm_senzing.jsonl Result: ‚úÖ 39 records loaded, 0 errors, 0.0 minutes, 259 candidate matches evaluated 9. Take Snapshot Prompt: \u0026ldquo;ok take a snapshot\u0026rdquo;\nAI runs: sz_snapshot -o ftm-watchlist-snapshot-$(date +%Y-%m-%d) -Q Captures entity resolution statistics for analysis \u0026mdash; CONTEXT LOST \u0026mdash; Session ended ‚Üí Context hit 100%, session reset See Step 6 for what happened and recovery 10. Recover Context New session - Prompt: \u0026ldquo;did you compact\u0026rdquo;\nAI: ‚ùå No memory of previous work Had to recover by finding snapshot file 11. Find Snapshot Prompt: \u0026ldquo;@senzing\u0026rdquo; [re-explore]\nThen: \u0026ldquo;do you still have access to the senzing mcp server?\u0026quot;\nThen: \u0026ldquo;Its already loaded. can you find the snapshot on the watchlist directory and show it to me\u0026rdquo; AI locates: ftm-watchlist-snapshot-2025-11-14.json 12. Analyze Snapshot Prompt: \u0026ldquo;in the senzing tools reference it tells you how to summarize the snapshot, can you do that\u0026rdquo; Result: 92 entities from 159 total records (42% compression), 17 cross-source matches 13. Find Multi-Source Entity Prompt: \u0026ldquo;show that multi source entity\u0026rdquo;\nAI uses: get_entity(91) via MCP server Discovery: Alexander Vasiliev spans CUSTOMERS, SANCTIONS, CORP_FILINGS with Cyrillic name variants 14. Resolution Timeline Prompt: \u0026ldquo;how did this entity come together\u0026rdquo;\nAI uses: how_entity_resolved(91) Insight: Cross-language name matching (Cyrillic‚ÜíLatin) and international phone format variations 15. Map Relationships Prompt: \u0026ldquo;show his relationships in a simple graph view\u0026rdquo;\nAI uses: get_entity() calls for related entities Network: Alexander Vasiliev owns Mullenkrants Autoworks, Faisal Siddiqui is President Key Learning Points from This Workflow Interactive Decision-Making Notice the questions asked during Stage 4 mapping. These weren\u0026rsquo;t errors - they were quality checkpoints:\nVerifying assumptions against actual data Discovering edge cases (company relationships) Clarifying Senzing conventions (NAME_ORG vs NAME_FULL) Important: Asking questions improves quality BUT consumes context. Be specific with questions. If AI warns \u0026ldquo;Context ~80% full\u0026rdquo; and offers to compact, say YES - this compresses the conversation so you can continue. See Step 6 for what happens when you decline and context recovery strategies.\nMulti-Pass Processing FTM data required special handling:\nPass 1: Process master entities (Person, Company) Pass 2: Merge relationship metadata (Sanction, Ownership, Directorship) Pass 3: Merge identifiers onto Person entities Validation Pipeline Multiple validation layers caught different issues:\nLinter: JSON syntax and structure Analyzer: Data source configuration and feature coverage Loading: Actual entity resolution matching Entity Resolution Results The final numbers tell the story:\n42% compression - Significant duplicate detection 17 cross-source matches - Hidden connections revealed Multi-jurisdictional networks - Corporate structures mapped Next Steps: Continue to the detailed pages to see the full prompts, responses, and teaching moments, or jump straight to mapping your own FTM data using this workflow as a guide.\n"
},
{
	"uri": "/6_exercise2_watchlist/62_examineandschema.html",
	"title": "Step 2: Examine Source Data and Generate Schema",
	"tags": [],
	"description": "",
	"content": "Understanding the FTM Watchlist Data Before mapping any data to Senzing format, you need to understand its structure. Unlike the customer CSV file in Exercise 1, this watchlist data is in FollowTheMoney (FTM) JSON format - a more complex format with nested structures and relationships.\nFTM Format: FollowTheMoney is an open-source data model designed for investigative journalism and anti-corruption work. It\u0026rsquo;s used by OpenSanctions, OCCRP, and other transparency initiatives to model entities, relationships, and sanctions data.\nExamine the Source File The watchlist data is located at:\nworkspace/watchlist/ftm.jsonl Format Characteristics:\nJSONL (JSON Lines) - One JSON object per line Nested structures - Complex objects with properties arrays Multiple schema types - Person, Company, Sanction, Ownership, Directorship International data - Arabic and Cyrillic character sets Relationship pointers - Entities reference other entities by ID Sample FTM Record:\n{ \u0026#34;id\u0026#34;: \u0026#34;sanctions-person-1006\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;Person\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;name\u0026#34;: [\u0026#34;Robert Smith Sr\u0026#34;], \u0026#34;firstName\u0026#34;: [\u0026#34;Robert\u0026#34;], \u0026#34;middleName\u0026#34;: [\u0026#34;E\u0026#34;], \u0026#34;lastName\u0026#34;: [\u0026#34;Smith Sr\u0026#34;], \u0026#34;birthDate\u0026#34;: [\u0026#34;1954-03-31\u0026#34;], \u0026#34;address\u0026#34;: [\u0026#34;123 Main St, Las Vegas\u0026#34;], \u0026#34;phone\u0026#34;: [\u0026#34;702-555-0123\u0026#34;], \u0026#34;email\u0026#34;: [\u0026#34;rsmith@example.com\u0026#34;] } } Generate the Schema Your Prompt:\nGenerate a schema for the ftm.jsonl file I have open AI Response:\nThe AI will run the schema generator tool:\npython3 senzing/tools/sz_schema_generator.py workspace/watchlist/ftm.jsonl -o workspace/watchlist/ftm_schema.md Why Generate a Schema? The schema generator extracts all field names, data types, sample values, and coverage statistics. This gives you and the AI a complete understanding of what data you\u0026rsquo;re working with.\nSchema Analysis Results Overview Statistics Metric Value Total Records 73 Schema Types 5 (Person, Sanction, Company, Ownership, Directorship) Unique Fields 31 Character Sets Latin, Cyrillic, Arabic Record Type Breakdown Schema Type Count Purpose Person 33 Individual entities (people) Sanction 17 Sanctions metadata for persons Company 6 Organization entities Ownership 8 Company ownership relationships Directorship 6 Company management relationships Complete Field Inventory Root Fields:\nid - Unique identifier for each record schema - Record type (Person, Company, Sanction, etc.) properties - Container for all attributes Person Attributes:\nname, firstName, middleName, lastName - Name fields birthDate - Date of birth address - Full address strings email, phone - Contact information nationality, country - Geographic data passportNumber, idNumber, taxNumber - Identifiers gender - Male/Female alias, previousName, weakAlias - Name variations Company Attributes:\nname - Organization name jurisdiction - Country of incorporation incorporationDate - Legal registration date address - Business address previousName - Former names Relationship Fields:\nentity - The subject of a sanction or relationship owner - The owning entity in an ownership relationship asset - The owned entity in an ownership relationship director - The person in a directorship organization - The organization in a directorship role - Management role (e.g., \u0026ldquo;President\u0026rdquo;, \u0026ldquo;Principal\u0026rdquo;) percentage - Ownership percentage Sanction Metadata:\nprogram - Sanctions program name authority - Sanctioning authority reason - Reason for sanction listingDate - Date added to sanctions list startDate, endDate - Sanction period status - Active/Inactive Identifier Fields (nested in Person records):\nholder - Person who holds the identifier type - Identifier type (passport, SSN, driver\u0026rsquo;s license) number - Identifier number country - Issuing country Key Differences from Customer CSV Aspect Customer CSV (Exercise 1) FTM Watchlist (Exercise 2) Format Flat CSV (one row = one record) Nested JSON (arrays, objects) Complexity Simple - 19 fields per record Complex - 31 fields across 5 schemas Entity Types All in one file Separated by schema type Relationships None Explicit (Ownership, Directorship) Character Sets English only Multilingual (Cyrillic, Arabic) Processing Single-pass Multi-pass (merge relationships) What This Means for Mapping The schema analysis reveals several mapping challenges:\nMultiple Schema Types: Need to decide which are \u0026ldquo;master entities\u0026rdquo; vs \u0026ldquo;metadata\u0026rdquo; vs \u0026ldquo;relationships\u0026rdquo; Nested Structures: Properties are arrays, not simple strings Relationship Pointers: Some records reference other records by ID Mixed Entity Types: Both Person and Organization entities in same dataset International Data: Name transliteration and format variations Important: This is significantly more complex than the customer CSV! The FTM format requires a multi-pass processing strategy where you:\nProcess master entities (Person, Company) Merge metadata records (Sanction) onto masters Convert relationship records (Ownership, Directorship) to REL_POINTER format Next Steps Now that you understand the data structure, you\u0026rsquo;re ready to map it to Senzing format using the AI-assisted mapping workflow. The next section walks through the 5-stage Senzing Mapping Assistant process.\nPro Tip: Keep the generated schema file (ftm_schema.md) open in a separate tab. You\u0026rsquo;ll reference it frequently during the mapping process to verify field names, check sample values, and understand data coverage.\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/index.html",
	"title": "Step 3: Map with Senzing AI Assistant",
	"tags": [],
	"description": "",
	"content": "The 5-Stage Mapping Workflow Now that you understand the FTM data structure, it\u0026rsquo;s time to map it to Senzing format using the Senzing Mapping Assistant - an AI-guided workflow with 5 structured stages.\nAI-Assisted Mapping: The Senzing Mapping Assistant is a prompt-based system that guides an AI through the complete mapping process. It has access to all Senzing documentation, examples, and validation tools, making it an expert mapping partner.\nThe Five Stages Each stage has a specific purpose and validation checkpoints:\nStage Name Purpose Output 1 INIT Load reference files and test tools AI has complete Senzing knowledge base 2 INVENTORY Extract all fields from schema Complete field list with integrity check 3 PLANNING Determine DATA_SOURCE and entity types Processing strategy for complex data 4 MAPPING Map each field to Senzing format Field disposition: Feature, Payload, or Ignored 5 OUTPUTS Generate code and documentation Mapper script, spec, and sample output How This Differs from Exercise 1 Exercise 1 (Customer CSV):\nWalked through every prompt and response with screenshots Detailed explanation of each decision Every field discussed individually Exercise 2 (FTM Watchlist):\nShows key prompts and results Highlights decision points and corrections Demonstrates how to handle complex data You can complete this faster by learning from Exercise 1 patterns Efficiency Note: If you completed Exercise 1, you already understand the workflow. Use this exercise to see how the same process handles more complex data structures and relationships.\nStarting the Mapping Process To begin AI-assisted mapping, you need to load the Senzing Mapping Assistant prompt. There are two ways to do this:\nOption 1: Using the @ reference (if available)\n@senzing Option 2: Explicitly load the prompt file\nRead and follow the instructions in: /home/ubuntu/workshop/senzing/prompts/senzing_mapping_assistant.md Once loaded, the AI will guide you through each stage, asking questions and validating decisions along the way.\nWhat to Expect Interactive Decision-Making The AI will ask questions when it:\nNeeds clarification on field meanings Encounters low-confidence mapping decisions Finds conflicting or unusual data patterns Discovers edge cases not covered in examples Quality Checkpoints You\u0026rsquo;ll be prompted to verify:\nAll fields were extracted (no hallucinations) DATA_SOURCE codes are appropriate Field dispositions make business sense Sample JSON passes validation Iterative Corrections Don\u0026rsquo;t expect perfection on the first try! This exercise shows real corrections made during mapping:\nVerifying identifier types against actual data Discovering missed relationship patterns Clarifying Senzing conventions (NAME_ORG vs NAME_FULL) Important: The AI is knowledgeable but not infallible. Your domain expertise is crucial. Question assumptions, verify against actual data, and correct mistakes when you spot them.\nFollowing Along The next five pages show:\nPrompts used - What was asked AI responses - What the AI provided Your corrections - What was adjusted and why Final decisions - What was ultimately implemented Each page is standalone, so you can jump to specific stages to see how particular challenges were handled.\nPro Tip: Keep three tabs open:\nYour AI assistant (Claude, ChatGPT, etc.) The schema file (ftm_schema.md) The source data file (ftm.jsonl) This lets you quickly verify AI suggestions against actual data.\nReady? Let\u0026rsquo;s start with Stage 1: INIT ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/64_validatemapping.html",
	"title": "Step 4: Validate Mapping",
	"tags": [],
	"description": "",
	"content": "Run the Mapper and Validate Output Now it\u0026rsquo;s time to run the mapper on the actual FTM data and validate the results using Senzing\u0026rsquo;s validation tools.\nStep 1: Run the Mapper Execute the mapper script on the actual FTM watchlist data:\npython3 ftm_mapper.py ftm.jsonl ftm_senzing.jsonl Expected Output:\nProcessing 73 FTM records... Pass 1: Processing master entities (Person, Company)... Created 33 Person entities Created 6 Company entities Pass 2: Merging Sanction metadata... Merged 17 sanctions onto Person entities Pass 3: Adding Ownership relationships... Added 8 ownership relationships Pass 4: Adding Directorship relationships... Added 6 directorship relationships ‚úÖ Generated 39 Senzing entities from 73 FTM records ‚úÖ Output written to: ftm_senzing.jsonl 39 entities from 73 records: This is expected! Sanction, Ownership, and Directorship records aren\u0026rsquo;t separate entities - they\u0026rsquo;re metadata and relationships merged onto the 39 master entities (33 Person + 6 Company).\nStep 2: Validate JSON Structure (Linter) First validation: Check that the JSON is structurally correct:\npython3 senzing/tools/lint_senzing_json.py ftm_senzing.jsonl Expected Result:\n‚úÖ PASSED - No JSON syntax errors All records have required fields (DATA_SOURCE, RECORD_ID) What the linter checks:\nValid JSON syntax (no missing commas, brackets, etc.) Required fields present (DATA_SOURCE, RECORD_ID) FEATURES array structure correct No obvious structural issues Step 3: Analyze Data Quality Second validation: Check that Senzing will recognize and use the mapped data:\npython3 senzing/tools/sz_json_analyzer.py ftm_senzing.jsonl -o analysis.md Important: According to the tools reference documentation, the analyzer outputs to a .md file. You must then read the markdown file to see the results, not just look at console output!\nRead the analysis results:\ncat analysis.md Initial Result:\n‚ùå Critical Errors: 2 ERROR: DATA_SOURCE not found: CORP_FILINGS ERROR: DATA_SOURCE not found: SANCTIONS ‚úÖ Feature Attributes: 13 Senzing features recognized - NAME_FIRST (25 records) - NAME_LAST (25 records) - NAME_ORG (6 records) - DATE_OF_BIRTH (18 records) - ADDR_FULL (22 records) - PHONE_NUMBER (31 records) - EMAIL_ADDRESS (15 records) ... ‚ö†Ô∏è Warnings: 3 - Some optional features have low population (expected) Step 4: Fix Critical Errors The analyzer found critical errors: DATA_SOURCE codes not registered in Senzing.\nCritical vs Non-Critical:\nCritical errors (red) - Must fix before loading. Senzing will reject the data. Warnings (orange) - Should review but don\u0026rsquo;t block loading. Info (yellow) - Payload attributes, expected and fine. Configure Data Sources Create a configuration file to register both DATA_SOURCE codes:\ncat \u0026gt; ftm_config.g2c \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; addDataSource CORP_FILINGS addDataSource SANCTIONS save EOF Run the configuration tool:\nsource ~/.bashrc \u0026amp;\u0026amp; sz_configtool -f ftm_config.g2c Expected Output:\nLoading configuration commands from ftm_config.g2c... Adding data source: CORP_FILINGS ‚úÖ Data source CORP_FILINGS added (DSRC_ID: 1001) Adding data source: SANCTIONS ‚úÖ Data source SANCTIONS added (DSRC_ID: 1002) Saving configuration... ‚úÖ Configuration saved successfully Step 5: Re-analyze After Fix Run the analyzer again to confirm errors are resolved:\npython3 senzing/tools/sz_json_analyzer.py ftm_senzing.jsonl -o analysis.md cat analysis.md Final Result:\n‚úÖ Critical Errors: 0 (resolved!) ‚úÖ Feature Attributes: 13 Senzing features with good coverage GREEN - Recognized and will be used for matching: - NAME_FIRST (25 records, 64%) - NAME_LAST (25 records, 64%) - NAME_ORG (6 records, 100%) - DATE_OF_BIRTH (18 records, 46%) - ADDR_FULL (22 records, 56%) - PHONE_NUMBER (31 records, 79%) - EMAIL_ADDRESS (15 records, 38%) - SSN_NUMBER (2 records, 5%) - DRIVERS_LICENSE_NUMBER (1 record, 3%) - PASSPORT_NUMBER (8 records, 21%) - NATIONAL_ID_NUMBER (3 records, 8%) - REL_ANCHOR_DOMAIN (39 records, 100%) - REL_POINTER_DOMAIN (12 records, 31%) ‚úÖ Payload Attributes: 6 custom attributes YELLOW - Will be stored but not used for matching: - SANCTION_PROGRAM (17 records) - SANCTION_AUTHORITY (17 records) - SANCTION_REASON (17 records) - SANCTION_LISTING_DATE (17 records) - SANCTION_STATUS (17 records) - INCORPORATION_DATE (4 records) ‚ö†Ô∏è Warnings: Expected low population on optional features ORANGE - Not a problem, just informational: - SSN_NUMBER only in 5% of records (expected - international data) - PASSPORT_NUMBER only in 21% of records (expected - incomplete data) Ready to load into Senzing! Understanding the Color Codes:\nGREEN - Feature attributes recognized by Senzing, will be used for matching YELLOW - Payload attributes, stored with entity but don\u0026rsquo;t affect matching ORANGE - Warnings about data coverage (usually expected) RED - Critical errors that must be fixed Validation Summary After completing all validation steps, you should have:\nValidation Tool Result ‚úÖ JSON Structure lint_senzing_json.py PASSED ‚úÖ Data Sources sz_json_analyzer.py 0 critical errors ‚úÖ Feature Coverage sz_json_analyzer.py 13 features recognized ‚úÖ Payload Attributes sz_json_analyzer.py 6 attributes will be stored ‚úÖ Relationships sz_json_analyzer.py REL_ANCHOR (39), REL_POINTER (12) Status: ‚úÖ Ready to load into Senzing\nCommon Validation Issues Issue Cause Solution DATA_SOURCE not found Not registered in Senzing config Use sz_configtool to add data sources Unrecognized features Typo in feature name Check spelling against entity spec All attributes showing as yellow Forgot FEATURES array Wrap features in FEATURES array No REL_ANCHOR found Missing relationship setup Add REL_ANCHOR to master entities REL_POINTER pointing to nothing Target entity doesn\u0026rsquo;t exist Check multi-pass processing order Key Learnings Two-Layer Validation Senzing provides two distinct validation tools for different purposes:\nLinter - Checks JSON structure and required fields Analyzer - Checks Senzing-specific quality (data sources, features, coverage) Both are necessary! Linter catches syntax errors, analyzer catches semantic issues.\nCritical vs Warning Not all analyzer output is equally important:\nCritical errors (red) - Must fix, data will be rejected Warnings (orange) - Review but often expected (e.g., sparse optional fields) Info (yellow) - Payload attributes, completely fine Don\u0026rsquo;t panic at yellow/orange output - read what it says!\nConfiguration is Part of Validation Data sources must be registered before loading. This is a configuration step, not a data issue. The analyzer catches this before you try to load data and get cryptic errors.\nNext Step With validation complete and all critical errors resolved, you\u0026rsquo;re ready to load the data into Senzing and see entity resolution in action!\nContinue to Step 5: Load Data ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/65_loaddata.html",
	"title": "Step 5: Load Data into Senzing",
	"tags": [],
	"description": "",
	"content": "Load Mapped Data and Take Snapshot With validation complete and all critical errors resolved, load the watchlist data into Senzing to perform entity resolution.\nStep 1: Load the Data Use the Senzing file loader to import the mapped JSONL:\nsource ~/.bashrc \u0026amp;\u0026amp; sz_file_loader -f ftm_senzing.jsonl Expected Output:\nLoading records from ftm_senzing.jsonl... Processing: |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100% Load Summary: Records loaded: 39 Records failed: 0 Processing time: 0.0 minutes Entity Resolution Statistics: Candidate matches evaluated: 259 Ambiguous matches detected: 5 ‚úÖ Load completed successfully What Just Happened:\nAll 39 records loaded successfully (no data errors) Senzing evaluated 259 potential matches across all data sources 5 matches were \u0026ldquo;ambiguous\u0026rdquo; (could match multiple ways - Senzing will flag these) Processing took under 1 minute (this is a small dataset) Understanding the Load Statistics Records Loaded: 39 This is the watchlist data (33 Person + 6 Company), but remember:\nYou already had 120 customer records loaded (from Exercise 1) Total records now in Senzing: 159 (120 + 39) This enables cross-source matching between customers and watchlist! Candidate Matches: 259 Senzing compared these new 39 records against:\nEach other (watchlist-to-watchlist matches) The existing 120 customer records (watchlist-to-customer matches) Generated 259 pairwise comparisons that met matching thresholds Ambiguous Matches: 5 Some records could match in multiple ways. Senzing flags these for human review:\nSame person with slightly different data in multiple sources? Actually different people who happen to have similar attributes? Legitimate uncertainty that requires business judgment Step 2: Take a Snapshot Capture the current entity resolution state for analysis:\nsource ~/.bashrc \u0026amp;\u0026amp; sz_snapshot -o ftm-watchlist-snapshot-$(date +%Y-%m-%d) -Q Expected Output:\nGenerating snapshot: ftm-watchlist-snapshot-2025-11-16.json Analyzing entity resolution statistics... Snapshot complete: Total data sources: 3 (CUSTOMERS, SANCTIONS, CORP_FILINGS) Total records: 159 Total entities: 92 Overall compression: 42.1% Cross-source matches: 17 ‚úÖ Snapshot written to: ftm-watchlist-snapshot-2025-11-16.json What is a Snapshot? A snapshot is a JSON file containing comprehensive entity resolution statistics:\nRecord and entity counts by data source Compression rates (how many duplicates were found) Cross-source matches (same entity in multiple systems) Relationship counts Match quality breakdown (definitive, possible, ambiguous) Interpreting the Snapshot Statistics 159 Records ‚Üí 92 Entities (42.1% Compression) What this means:\n159 total source records were loaded Senzing determined they represent only 92 unique real-world entities 42.1% of records were duplicates of entities already known 67 records merged into existing entities This is significant compression! Almost half the records were recognized as duplicates.\nCross-Source Matches: 17 What this means:\n17 entities have records from multiple data sources Examples: A person appears in both CUSTOMERS and SANCTIONS (watchlist hit!) A person appears in both CUSTOMERS and CORP_FILINGS (customer is a company officer) A person appears in SANCTIONS and CORP_FILINGS (sanctioned individual owns a company) These cross-source matches are the most interesting findings from entity resolution!\nStep 3: Quick Verification Verify the data loaded correctly by checking record counts:\nsource ~/.bashrc \u0026amp;\u0026amp; sz_explorer -query \u0026#39;DATA_SOURCE=SANCTIONS\u0026#39; Expected:\nShould return entities that have SANCTIONS records Verify some have merged sanction payload attributes Check that relationships are present source ~/.bashrc \u0026amp;\u0026amp; sz_explorer -query \u0026#39;DATA_SOURCE=CORP_FILINGS\u0026#39; Expected:\nShould return company entities Verify organization names appear correctly Check for ownership relationships Common Loading Issues Issue Cause Solution DATA_SOURCE not found errors Data sources not configured Re-run sz_configtool with data source commands All records fail to load JSON structure errors Re-run linter to find syntax problems Some records fail Invalid data in specific records Check error log for record IDs and field issues Load seems stuck Large dataset processing Be patient, or check with top command Duplicate record errors RECORD_ID collision Ensure RECORD_IDs are unique within DATA_SOURCE Success Indicators You know the load was successful if:\n‚úÖ Records loaded = 39, Records failed = 0 ‚úÖ Snapshot generates without errors ‚úÖ Total entities \u0026lt; Total records (compression detected) ‚úÖ Cross-source matches \u0026gt; 0 (found connections between data sources) ‚úÖ Explorer queries return results for both SANCTIONS and CORP_FILINGS What Happens During Loading? 1. Record Ingestion Senzing reads each JSON record and extracts features and payload attributes.\n2. Entity Resolution For each record, Senzing:\nGenerates match candidates based on features (name, address, phone, identifiers, etc.) Scores each candidate match using ML algorithms Determines if record matches existing entity or creates new one Updates entity with new information from record 3. Relationship Processing For records with REL_POINTER:\nLinks entities together with typed relationships Builds network graph of connections Enables relationship path finding and network expansion 4. Indexing All features, payload attributes, and relationships are indexed for fast querying.\nKey Learnings Load Time Scales 39 records loaded in \u0026lt; 1 minute. Performance varies by:\nRecord complexity (features, relationships, payload) Dataset size (existing entities to compare against) Hardware (CPU, memory, disk) For reference:\nThousands of records: minutes Millions of records: hours (but can parallelize) Cross-Source Matching is Automatic You didn\u0026rsquo;t do anything special to enable watchlist-to-customer matching. Senzing automatically:\nCompared new SANCTIONS records against existing CUSTOMERS Compared new CORP_FILINGS against both CUSTOMERS and SANCTIONS Found matches based on feature similarity (names, phones, addresses, etc.) Compression Indicates Quality 42% compression suggests:\nGood data quality (clean enough to match) Realistic duplicates (not over-matching) Effective feature mapping (right attributes used for matching) Low compression (\u0026lt;10%) might indicate:\nVery clean, de-duplicated source data Under-matching (not enough features, or thresholds too strict) Very high compression (\u0026gt;80%) might indicate:\nVery dirty data with many duplicates Over-matching (false positives, thresholds too loose) Next Steps With data loaded and snapshot captured, you\u0026rsquo;re ready to analyze the entity resolution results in detail!\nThe next section demonstrates an important lesson about working with AI across sessions\u0026hellip;\nContinue to Step 6: Analyze Snapshot ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/66_analyzesnapshot.html",
	"title": "Step 6: Analyze Snapshot",
	"tags": [],
	"description": "",
	"content": "Analyzing Entity Resolution Results With data loaded and a snapshot captured, it\u0026rsquo;s time to analyze the entity resolution results to understand what Senzing found.\nIMPORTANT LESSON: Context Loss Across Sessions\nThis step documents a real scenario that happened during workshop development. The mapping and loading were completed in one AI session, then the session ended. When analysis resumed in a new session, the AI had lost all context about what had been done.\nThis section shows what happened, why it happened, and how to recover - a critical skill for working with AI on multi-day projects!\nWhat Happened: The Context Gap Original Prompt (New Session) did you compact AI Response I\u0026#39;ll explore the senzing directory to show you what\u0026#39;s available for the workshop. Problem: The AI didn\u0026rsquo;t know:\nWhat data had been mapped What files existed What work had been completed What \u0026ldquo;compact\u0026rdquo; meant in this context Why Context Was Lost In this case, context was lost because the context window filled up during the mapping conversation:\nLong Interactive Mapping Session\n5-stage mapping workflow with multiple questions and corrections Each question and answer consumed context tokens Asking questions improves quality but uses more context Data Loading Completed\nAll 39 records successfully loaded into Senzing ‚úÖ Loading was done, data was in Senzing Next step should have been: take snapshot (Senzing analysis) Context Warning Appeared\nAI warned: \u0026ldquo;Context ~80% full\u0026rdquo; AI likely offered to compact (compress conversation to save tokens) This was the signal: say YES to compact Took a Chance\nSaid no / declined the compact offer Continued without compacting the conversation Kept going, probably asked to take snapshot Each additional message used more tokens Context filled to 100% ‚Üí Session automatically reset All Context Lost\nWhen you asked \u0026ldquo;did you compact\u0026rdquo; - AI had no memory The conversation was never compacted (compressed) Session had reset, all context gone Had to recover from scratch Understanding \u0026ldquo;Compact\u0026rdquo; vs \u0026ldquo;Take Snapshot\u0026rdquo; THESE ARE COMPLETELY DIFFERENT:\nCOMPACT (AI Feature):\nAI action to compress/summarize conversation context Reduces token count by condensing the conversation Allows you to continue in the same session Prevents hitting the 100% limit Example: AI says \u0026ldquo;Would you like me to compact the conversation?\u0026rdquo; ‚Üí Say YES! TAKE SNAPSHOT (Senzing Tool):\nSenzing command: sz_snapshot Analyzes loaded entities and generates statistics Creates JSON file with entity counts, compression rates, cross-source matches Has nothing to do with AI context management This is a data analysis step, not a context management step Compacting the conversation:\n‚úÖ Prevents context loss by compressing conversation history ‚úÖ Lets you continue in the same session ‚úÖ Reduces token count so you have room to keep working ‚úÖ Preserves enough context to maintain continuity The Lesson: Accept the Compact Offer\nWhen AI warns \u0026ldquo;Context ~80% full\u0026rdquo; and offers to compact:\nSAY YES to the compact offer AI will compress the conversation history This frees up tokens to continue working You can then take snapshot, analyze results, etc. in the same session DON\u0026rsquo;T decline and hope for the best - at 100%, session WILL reset automatically.\nIn this exercise: Data was loaded, ready to take snapshot. Context hit 80%, AI offered to compact. Should have said \u0026ldquo;yes, compact the conversation.\u0026rdquo; Instead, took a chance and declined. Kept going without compacting ‚Üí hit 100% ‚Üí session reset ‚Üí all context lost.\nHow to Recover: Step-by-Step Recovery Step 1: Re-establish Context Your Prompt:\n@senzing AI Action:\nExplored the senzing directory structure Re-discovered reference materials, tools, prompts But still didn\u0026rsquo;t know what YOU had done Lesson: AI tools can rediscover the repository, but not your work history.\nRecovery Step 2: Verify Tool Access Your Prompt:\ndo you still have access to the senzing mcp server? AI Response: Lists all available Senzing MCP functions:\nsearch - Search for entities by attributes get_entity - Get full entity details by ENTITY_ID get_source_record - Get entity by source record ID find_relationship_path - Find connections expand_entity_network - Network analysis explain_why_entities_related - Explain matching logic how_entity_resolved - Resolution timeline Lesson: Always verify tool access after context loss. Different sessions may have different capabilities.\nRecovery Step 3: Locate Work Artifacts Your Prompt:\nIts already loaded. can you find the snapshot on the watchlist directory and show it to me AI Action:\nSearches workspace/watchlist/ directory Finds ftm-watchlist-snapshot-2025-11-14.json Reads the snapshot file Lesson: Reference actual files to restore context. The file system remembers what the AI forgot!\nRecovery Step 4: Understand the Snapshot Format Your Prompt:\nin the senzing tools reference it tells you how to summarize the snapshot, can you do that AI Action:\nConsults SENZING_TOOLS_REFERENCE.md Learns proper snapshot summary format Generates correct analysis table Lesson: Reference documentation helps AI understand formats even without prior context.\nThe Recovered Snapshot Analysis After recovery, the AI produced this proper summary:\nEntity Resolution Summary by Data Source Data Source Records Entities Compression Cross-Source Matches CUSTOMERS 120 75 37.5% 12 SANCTIONS 17 14 17.6% 5 CORP_FILINGS 22 17 22.7% 10 TOTAL 159 92 42.1% 17 Key Findings Overall Compression: 42.1%\n159 source records collapsed to 92 unique entities 67 records (42%) were duplicates of entities already known Cross-Source Matches: 17 entities\n5 matches between CUSTOMERS and SANCTIONS (üö® watchlist hits!) 8 matches between CUSTOMERS and CORP_FILINGS (customers who are company officers) 4 matches between SANCTIONS and CORP_FILINGS (sanctioned individuals with companies) Relationships: 12 disclosed relationships\nOwnership connections (person‚Üícompany, company‚Üícompany) Directorship connections (person‚Üícompany with roles) Multi-Source Entity: 1\nOne person appears in ALL THREE data sources (customer, sanctioned, company owner) How to Prevent Context Loss While you can\u0026rsquo;t always prevent sessions from ending, you can minimize recovery time:\n1. Document Progress Create markdown files tracking what you\u0026rsquo;ve done:\n# Create a progress log cat \u0026gt; mapping_progress.md \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # FTM Watchlist Mapping Progress ## Completed Steps - [x] Generated schema from ftm.jsonl - [x] Ran 5-stage AI mapping assistant - [x] Generated mapper code (ftm_mapper.py) - [x] Validated with linter and analyzer - [x] Configured SANCTIONS and CORP_FILINGS data sources - [x] Loaded 39 records successfully - [x] Took snapshot: ftm-watchlist-snapshot-2025-11-14.json ## Next Steps - [ ] Analyze snapshot results - [ ] Investigate multi-source entities - [ ] Review relationship networks EOF 2. Save Key Information Keep a file with important paths and facts:\ncat \u0026gt; session_context.md \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # Session Context ## Working Directory /home/ubuntu/workshop/ ## Key Files - Source data: workspace/watchlist/ftm.jsonl - Schema: workspace/watchlist/ftm_schema.md - Mapper: ftm_mapper.py - Output: ftm_senzing.jsonl - Snapshot: ftm-watchlist-snapshot-2025-11-14.json ## Data Sources Configured - SANCTIONS (DSRC_ID: 1002) - CORP_FILINGS (DSRC_ID: 1001) ## Load Results - 39 records loaded, 0 failed - 259 candidate matches evaluated - 5 ambiguous matches EOF 3. Reference Past Work When starting a new session:\nRead session_context.md and mapping_progress.md to understand what we\u0026#39;ve completed. Then help me analyze the snapshot at workspace/watchlist/ftm-watchlist-snapshot-2025-11-14.json 4. Use Persistent Artifacts Snapshots, logs, output files persist even when context is lost Generated code and documentation can be re-read File system is your \u0026ldquo;memory\u0026rdquo; across sessions Recovery Strategies Summary Strategy When to Use How It Helps Re-explore repository Always first step Reestablishes project structure Verify tool access When using special capabilities Confirms what\u0026rsquo;s available Reference file artifacts When work was saved to disk File system remembers your work Consult documentation When format is unclear Provides standards and examples Create context files Before session ends Saves time in next session Key Learnings Context Loss is Normal Don\u0026rsquo;t panic when AI doesn\u0026rsquo;t remember previous work. This happens to everyone. Having recovery strategies makes it manageable.\nDocumentation \u0026gt; Memory Files on disk are more reliable than AI memory:\nProgress logs Context files Generated outputs Snapshots and reports Incremental Recovery Works You don\u0026rsquo;t need to redo all the work:\nRe-establish basic context (explore repo) Verify tools still work Find artifacts from previous work Continue from where you left off Prevention is Easier than Recovery Spending 2 minutes creating a context file saves 20 minutes of recovery later.\nNow Back to the Analysis With context recovered and snapshot properly summarized, let\u0026rsquo;s dig deeper into the interesting findings using the Senzing MCP server!\nContinue to Step 7: Review with MCP Server ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/67_reviewwithmcp.html",
	"title": "Step 7: Review with MCP Server",
	"tags": [],
	"description": "",
	"content": "Deep Entity Investigation Using Senzing MCP Now that context is recovered and you understand the snapshot results, use the Senzing MCP server to investigate specific entities, relationships, and resolution logic.\nMCP Server Advantage: While snapshots give you statistics, the MCP server lets you ask \u0026ldquo;how\u0026rdquo; and \u0026ldquo;why\u0026rdquo; questions about specific entities and matches. This is where AI-assisted analysis really shines!\nInvestigation 1: The Multi-Source Entity The snapshot showed 1 entity spans all three data sources. Let\u0026rsquo;s find it and understand how it came together.\nPrompt Show me the entity that appears in all three data sources (CUSTOMERS, SANCTIONS, CORP_FILINGS) AI Response Entity ID: 91 Resolved Name: –í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á (Alexander Pavlovich Vasiliev)\nSource Records (3) 1. CUSTOMERS:2063 - Alexander Pavlovich Vasiliev\nStatus: Platinum customer since 2023 Contact: 481-285-6234, avasiliev@example.com Address: Smolensk, Russia 2. SANCTIONS:corp-filings-person-2062 - –í–ê–°–ò–õ–¨–ï–í –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–≤–ª–æ–≤–∏—á\nProgram: SANCTIONS Authority: Sanctions Authority Reason: Category: Arms Trafficking Listing Date: 2014-03-17 Status: Active Phone: (4812)85-62-34 3. CORP_FILINGS:sanctions-person-2061 - –í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á\nOwnership: 100% owner of Mullenkrants Autoworks Country: Belarus Phone: +7(4812)85-62-34 Resolved Features Names (3 variants):\nEnglish: \u0026ldquo;Alexander Pavlovich Vasiliev\u0026rdquo; Russian (Cyrillic): \u0026ldquo;–í–ê–°–ò–õ–¨–ï–í –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–≤–ª–æ–≤–∏—á\u0026rdquo; Belarusian (Cyrillic): \u0026ldquo;–í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á\u0026rdquo; Phone Numbers (3 formats):\nUS format: \u0026ldquo;481-285-6234\u0026rdquo; Russian format: \u0026ldquo;(4812)85-62-34\u0026rdquo; International format: \u0026ldquo;+7(4812)85-62-34\u0026rdquo; Relationships:\nOwns Mullenkrants Autoworks GmbH (100%) Critical Finding: A Platinum customer is on the sanctions list for arms trafficking! This is exactly the kind of hidden risk entity resolution reveals.\nInvestigation 2: Resolution Timeline Understanding how this entity came together reveals Senzing\u0026rsquo;s cross-language and international format handling.\nPrompt how did this entity come together AI Response: Step-by-Step Resolution The MCP server\u0026rsquo;s how_entity_resolved() function shows the complete timeline:\nStep 1: CORP_FILINGS + SANCTIONS Match Match Key: +NAME+PHONE (Rule: CNAME_CFF - Close Name, Close Features First)\nNAME Match (Score: 90)\nCORP_FILINGS: \u0026ldquo;–í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á\u0026rdquo; (Belarusian Cyrillic) SANCTIONS: \u0026ldquo;–í–ê–°–ò–õ–¨–ï–í –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–≤–ª–æ–≤–∏—á\u0026rdquo; (Russian Cyrillic) Why matched: Cyrillic name transliteration variance (Belarusian vs Russian) –í–ê–°–Ü–õ–¨–ï–é ‚Üî –í–ê–°–ò–õ–¨–ï–í (different Cyrillic spelling of same name) Same underlying person, different script conventions PHONE Match (Score: 90)\nCORP_FILINGS: \u0026ldquo;+7(4812)85-62-34\u0026rdquo; SANCTIONS: \u0026ldquo;(4812)85-62-34\u0026rdquo; Why matched: Same Russian phone number, different formatting +7 is Russia country code (4812) is Smolensk area code 85-62-34 is local number Resolution Decision: DEFINITIVE MATCH ‚Üí Merged into Entity 91\nStep 2: CUSTOMERS Added to Merged Group Match Key: +NAME+PHONE (Rule: CNAME_CFF)\nNAME Match (Score: 90)\nExisting entity: \u0026ldquo;–í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á\u0026rdquo; + \u0026ldquo;–í–ê–°–ò–õ–¨–ï–í –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–≤–ª–æ–≤–∏—á\u0026rdquo; CUSTOMERS: \u0026ldquo;Alexander Pavlovich Vasiliev\u0026rdquo; Why matched: Cyrillic to Latin transliteration –í–ê–°–Ü–õ–¨–ï–é / –í–ê–°–ò–õ–¨–ï–í ‚Üí Vasiliev (standard transliteration) –ê–ª—è–∫—Å–∞–Ω–¥—Ä / –ê–ª–µ–∫—Å–∞–Ω–¥—Ä ‚Üí Alexander (Belarusian/Russian to English) –ü–∞—û–ª–∞–≤—ñ—á / –ü–∞–≤–ª–æ–≤–∏—á ‚Üí Pavlovich (patronymic transliteration) PHONE Match (Score: 98)\nExisting entity: \u0026ldquo;+7(4812)85-62-34\u0026rdquo; and \u0026ldquo;(4812)85-62-34\u0026rdquo; CUSTOMERS: \u0026ldquo;481-285-6234\u0026rdquo; Why matched: Normalized phone equivalence +7 (Russia) dropped (4812) area code ‚Üí 4812 Dashes vs parentheses vs plus signs normalized Core digits: 4812-85-62-34 match across all formats! Resolution Decision: DEFINITIVE MATCH ‚Üí Added to Entity 91\nKey Insights from Timeline 1. Cross-Language Matching Works Senzing successfully matched:\nBelarusian Cyrillic ‚Üî Russian Cyrillic Cyrillic ‚Üî Latin transliteration Different name conventions across cultures 2. International Phone Format Normalization Three phone number formats recognized as identical:\nUS-style digits: 481-285-6234 Russian style: (4812)85-62-34 International: +7(4812)85-62-34 3. Incremental Resolution Entity built in steps:\nFirst match: CORP_FILINGS + SANCTIONS (both Cyrillic) Second match: Adding CUSTOMERS (Latin) to existing Cyrillic names 4. High-Confidence Matches All matches scored 90+ (out of 100), indicating strong evidence:\nMultiple features agreed (name + phone) Patterns consistent with known transliteration rules No conflicting information Investigation 3: Relationship Network Now let\u0026rsquo;s map out the corporate structure and relationships.\nPrompt show his relationships in a simple graph view AI Response: Network Visualization Entity 91 (Alexander Pavlovich Vasiliev) ‚îÇ ‚îÇ OWNS 100% ‚ñº Entity 90 (Mullenkrants Autoworks GmbH) ‚îÇ Location: Germany ‚îÇ Industry: Automotive Manufacturing ‚îÇ ‚îÇ PRESIDENT ‚ñº Entity 100014 (Faisal Siddiqui) ‚îÇ Location: Pakistan ‚îÇ Status: Customer + Sanctioned Network Analysis Multi-Jurisdictional Network:\nRussia/Belarus ‚Üí Germany ‚Üí Pakistan Person (Russia/Belarus) owns company (Germany) Company employs sanctioned person (Pakistan) All Parties Sanctioned:\nAlexander Vasiliev: Arms Trafficking (2014) Mullenkrants Autoworks: Company owned by sanctioned individual Faisal Siddiqui: Category unknown (requires investigation) Corporate Structure:\nClear ownership: 100% stake (not partial or joint) Management hierarchy: President role defined Cross-border operations: Multiple countries involved Risk Implications:\nSanctioned individual controls entire company Company may be shell corporation or front International network suggests sophisticated operation Second sanctioned individual in management raises red flags Compliance Impact: This network would trigger enhanced due diligence requirements:\nKnow Your Customer (KYC) violations Anti-Money Laundering (AML) concerns Sanctions compliance issues Potential need to freeze accounts and report to authorities Investigation 4: Understanding Match Logic For any two entities, you can ask Senzing to explain the matching logic.\nPrompt explain why Entity 91 and Entity 90 are related AI Response Relationship Type: REL_POINTER (disclosed relationship, not inferred match)\nSource: Ownership record from FTM data\nOwner: Alexander Vasiliev (Entity 91) Asset: Mullenkrants Autoworks (Entity 90) Percentage: 100% Role: OWNER_OF Not a Resolved Match: This is a declared relationship from source data, not a match Senzing discovered. The FTM Ownership record explicitly stated this ownership structure.\nWhy This Matters:\nREL_POINTER relationships come from your data Senzing preserves and indexes them for network analysis Different from entity resolution matches (which Senzing infers) Common Investigation Patterns Finding Watchlist Hits Show me all entities that have both CUSTOMERS and SANCTIONS records Returns entities that are both customers and on watchlists - high-priority review cases.\nExploring Networks expand network from Entity 91 showing all relationships up to 2 degrees Shows complete network including indirect connections.\nUnderstanding Specific Matches explain why entity 91 matched with CUSTOMERS:2063 Shows exact features and scores that led to the match.\nFinding Ambiguous Cases show me entities with ambiguous matches that need review Lists cases where Senzing found multiple possible matches requiring human judgment.\nKey Learnings MCP Server Enables Deep Analysis Snapshots give you statistics MCP lets you investigate specific cases You can ask \u0026ldquo;how\u0026rdquo; and \u0026ldquo;why\u0026rdquo; questions about resolution logic Cross-Language Resolution is Powerful Senzing handled:\nCyrillic script variations (Belarusian vs Russian) Cyrillic ‚Üî Latin transliteration International phone format differences This works because Senzing:\nNormalizes data to standard forms Uses phonetic encoding for names Applies international format standards Relationships Tell Stories The network graph revealed:\nMulti-jurisdictional corporate structure Connections between sanctioned individuals Potential shell corporation patterns Compliance risk indicators Resolution Timelines Show Logic Understanding how entities merged helps you:\nValidate that matches make sense Identify potential false positives Explain results to stakeholders Tune matching rules if needed Next Steps You\u0026rsquo;ve now completed the full watchlist mapping and analysis workflow! The final step is to reflect on key takeaways and lessons learned.\nContinue to Step 8: Key Takeaways ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/68_keytakeaways.html",
	"title": "Step 8: Key Takeaways",
	"tags": [],
	"description": "",
	"content": "Lessons Learned from Watchlist Mapping This exercise demonstrated advanced entity resolution concepts using real-world watchlist data. Here are the key lessons you should take away.\n1. Complex Data Requires Different Strategies CSV vs JSON: More Than Format Customer CSV (Exercise 1):\nFlat structure, one row = one entity Single-pass processing No relationships English-only data FTM Watchlist (Exercise 2):\nNested JSON, multiple schema types Multi-pass processing (masters ‚Üí metadata ‚Üí relationships) Explicit relationships (ownership, directorship) International data (Cyrillic, Arabic, Latin) The Lesson: Your mapping strategy must match data complexity. There\u0026rsquo;s no one-size-fits-all approach.\n2. Interactive Mapping Beats One-Shot Prompts Quality Comes from Dialogue Throughout Stage 4 (Mapping), you questioned AI assumptions:\n\u0026ldquo;Are you sure those are all the identifier types?\u0026rdquo; ‚Üí Led to discovering nested identifier records AI missed\n\u0026ldquo;Don\u0026rsquo;t companies have relationships as well?\u0026rdquo; ‚Üí Found company-to-company ownership patterns\n\u0026ldquo;Previous name should be name_org, not name_full\u0026rdquo; ‚Üí Corrected convention error that would have degraded matching\nThe Lesson: AI is knowledgeable but not infallible. Your questions and corrections produce higher quality results than passively accepting first responses.\n3. Validation Has Layers Multi-Stage Validation Catches Different Issues Validation Tool What It Checks When It Catches Errors Linter JSON syntax, required fields During development Analyzer Data source config, feature recognition, coverage Before loading Loader Actual processing, candidate matching During loading Snapshot Entity resolution quality, compression, cross-source matches After loading The Lesson: Each validation layer serves a purpose. Skipping any layer means missing potential issues that could have been caught earlier.\n4. Ask Questions BUT Manage Context The Balance: Quality vs Context Consumption Questions Improve Quality:\nVerifying identifier types found nested records AI missed Asking about company relationships discovered ownership patterns Challenging NAME_ORG correction improved matching quality But Questions Consume Context:\nEach question and answer uses tokens Long interactive sessions fill the context window The more thorough you are, the more context you use What Happened in This Exercise Interactive mapping was thorough (good!) Many questions asked to verify assumptions (good!) Data loaded successfully into Senzing (good!) AI warned: \u0026ldquo;Context ~80% full\u0026rdquo; (warning signal!) AI offered to compact the conversation (compress context to free tokens) Should have said YES to compact (action needed!) Took a chance, declined instead (bad decision!) Context hit 100% ‚Üí Session reset ‚Üí All context lost The Critical Lesson DO ask questions - interactive mapping produces better results than one-shot prompts.\nBUT be specific - \u0026ldquo;Are you sure those are all the identifier types, and how do you know?\u0026rdquo; is better than \u0026ldquo;Tell me everything about identifiers in general.\u0026rdquo;\nAND accept compact offers - When AI warns \u0026ldquo;Context ~80% full\u0026rdquo; and offers to compact:\n‚ö†Ô∏è SAY YES to the compact offer üóúÔ∏è AI compresses the conversation history ‚ú® Frees up tokens to continue in same session üîÑ Preserves context while reducing token count Understanding COMPACT vs TAKE SNAPSHOT:\nThese are COMPLETELY DIFFERENT:\nCOMPACT (AI Feature):\nAI compresses/summarizes conversation to reduce tokens Lets you continue in the same session Prevents hitting 100% context limit Has nothing to do with Senzing TAKE SNAPSHOT (Senzing Tool):\nSenzing command: sz_snapshot Analyzes entities and generates statistics Creates JSON file with resolution results Has nothing to do with AI context management In this exercise: Data was loaded, ready to take snapshot (Senzing). Context hit 80%, AI offered to compact (compress conversation). Should have said \u0026ldquo;yes, compact.\u0026rdquo; Instead declined, kept going ‚Üí hit 100% ‚Üí session reset ‚Üí all context lost.\nThe Lesson: Don\u0026rsquo;t choose between quality and safety. Ask questions AND accept compact offers when warned. At 100%, session resets automatically - no exceptions.\n5. Cross-Language Resolution is Powerful International Data is Handled Automatically Senzing Successfully Matched:\nBelarusian Cyrillic ‚Üî Russian Cyrillic name variants Cyrillic ‚Üî Latin transliterations Three phone number formats (US, Russian, international) Example: Entity 91\n\u0026ldquo;–í–ê–°–Ü–õ–¨–ï–é –ê–ª—è–∫—Å–∞–Ω–¥—Ä –ü–∞—û–ª–∞–≤—ñ—á\u0026rdquo; (Belarusian) \u0026ldquo;–í–ê–°–ò–õ–¨–ï–í –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–≤–ª–æ–≤–∏—á\u0026rdquo; (Russian) \u0026ldquo;Alexander Pavlovich Vasiliev\u0026rdquo; (English) How It Works:\nName normalization and phonetic encoding International format standardization Transliteration pattern recognition The Lesson: You don\u0026rsquo;t need to pre-normalize international data. Map it faithfully, and Senzing handles format and language variations during resolution.\n6. Multi-Pass Processing is Essential Some Data Can\u0026rsquo;t Be Processed Linearly FTM Processing Order:\nPass 1: Create Person and Company entities (masters) Pass 2: Merge Sanction metadata onto Person entities Pass 3: Add Ownership relationships Pass 4: Add Directorship relationships Why Order Matters:\nSanction records reference Person IDs (must exist first) Ownership records reference Company IDs Relationships require both entities to exist The Lesson: When source data has pointers/references, you need multi-pass processing. Identify dependencies and process in correct order.\n7. Features vs Payload vs Ignored Three Field Dispositions Serve Different Purposes Features (18 fields):\nUsed for entity resolution matching Examples: NAME_FIRST, PHONE_NUMBER, PASSPORT_NUMBER Affect which records merge together Payload (6 fields):\nStored but don\u0026rsquo;t affect matching Examples: SANCTION_PROGRAM, INCORPORATION_DATE Operational data for retrieval and display Ignored (7 fields):\nNot mapped to Senzing Examples: schema type indicator, relationship pointers, container fields Either structural or handled differently (relationships) The Lesson: Not all fields are created equal. Thoughtful disposition decisions balance matching quality with data preservation.\n8. Relationships Tell Stories REL_POINTER Networks Reveal Hidden Patterns Alexander Vasiliev Network:\nVasiliev (Russia/Belarus) ‚Üí OWNS ‚Üí Mullenkrants (Germany) ‚Üí EMPLOYS ‚Üí Siddiqui (Pakistan) What This Revealed:\nMulti-jurisdictional corporate structure Sanctioned individual controls company Second sanctioned individual in management Potential shell corporation pattern The Lesson: Entity resolution + relationship mapping = powerful risk and compliance insights. The network tells a story individual records hide.\n9. Compression Indicates Quality 42% Compression is Meaningful What It Means:\n159 source records ‚Üí 92 entities 67 records (42%) were duplicates Significant but not excessive compression Quality Indicators:\nToo low (\u0026lt;10%): Under-matching, missing duplicates Good range (30-50%): Realistic duplicate detection Too high (\u0026gt;80%): Over-matching, false positives The Lesson: Compression rate is a quick health check. Extreme values (very low or very high) warrant investigation.\n10. Cross-Source Matches Are Gold 17 Cross-Source Matches Found Hidden Connections Categories of Matches:\nCUSTOMERS + SANCTIONS (5): Watchlist hits! High-priority review. CUSTOMERS + CORP_FILINGS (8): Customers who are company officers SANCTIONS + CORP_FILINGS (4): Sanctioned individuals with companies All three sources (1): Alexander Vasiliev case The Lesson: Cross-source matches reveal hidden risks and relationships that single-source analysis misses. This is where entity resolution provides its highest value.\n11. MCP Server Enables Investigation Beyond Statistics to Understanding Snapshots provide:\nEntity counts, compression rates, cross-source matches Aggregate statistics MCP server provides:\nSpecific entity details Resolution timelines (how entities matched) Match explanations (why features scored as they did) Relationship networks (who connects to whom) The Lesson: Use snapshots for overview, MCP server for investigation. Combining both gives you complete analysis capabilities.\n12. AI-Assisted ‚â† AI-Automated Humans Make Final Decisions AI Suggested:\nInitial field mappings DATA_SOURCE codes Processing strategies You Decided:\nCorrected NAME_ORG vs NAME_FULL Verified identifier types against actual data Confirmed relationship role mappings Approved final dispositions The Lesson: AI is a knowledgeable assistant, not an autonomous agent. Your domain expertise and judgment are crucial for quality results. Question assumptions, verify against data, make final calls.\nComparing Exercise 1 vs Exercise 2 What You Should Have Learned Aspect Exercise 1 (Customers) Exercise 2 (Watchlist) Skill Developed Format Simple CSV Nested JSON Handling complex structures Workflow Step-by-step with screenshots Reference summary format Self-guided mapping Language English only Multi-language (Cyrillic, Arabic) International data handling Relationships None Ownership, Directorship Network mapping Processing Single-pass Multi-pass Dependency management AI Interaction Observed Interactive corrections Critical evaluation Context Single session Cross-session recovery Session management If you completed both exercises, you now have:\n‚úÖ Foundation in AI-assisted mapping (Exercise 1) ‚úÖ Advanced techniques for complex data (Exercise 2) ‚úÖ Context recovery strategies ‚úÖ Critical evaluation skills ‚úÖ Real-world analysis capabilities Next Steps: Apply What You\u0026rsquo;ve Learned Ready for Your Own Data You now have the knowledge and tools to:\nMap any data source to Senzing\nUnderstand the 5-stage workflow Know when to use multi-pass processing Handle international and complex data Validate thoroughly\nUse linter for structure Use analyzer for quality Interpret results correctly Analyze results\nGenerate and interpret snapshots Investigate with MCP server Identify high-value findings Work with AI effectively\nAsk questions and verify assumptions Recover from context loss Make final decisions with confidence Additional Resources Senzing Documentation:\nEntity Specification: Full feature catalog Mapping Examples: 7 worked examples Tools Reference: Complete tool documentation Workshop Materials:\nCustomer solution (Exercise 1): Reference implementation Watchlist solution (Exercise 2): Complex data example Both can serve as templates for your data Community and Support:\nSenzing Community Forum: Ask questions GitHub Issues: Report problems Documentation: Regular updates Final Thoughts Entity resolution is both science and art:\nThe Science:\nStructured workflows (5-stage mapping) Validation pipelines (linter ‚Üí analyzer ‚Üí loading) Mathematical matching algorithms The Art:\nKnowing which fields to map as features vs payload Recognizing data quality issues Interpreting resolution results Telling the story hidden in the data AI Assistance:\nAccelerates the science (faster mapping, fewer errors) Augments the art (suggests patterns, explains logic) Never replaces human judgment Congratulations! You\u0026rsquo;ve completed the watchlist mapping workshop. You\u0026rsquo;re now ready to apply AI-assisted entity resolution to your own data challenges.\nWhat\u0026rsquo;s Next? Try mapping your own data! Start with the Workflow Summary (Step 1) as a guide, use the 5-stage mapping assistant, and apply the validation and analysis techniques you\u0026rsquo;ve learned. Good luck!\n"
},
{
	"uri": "/2_setup_configuration/21_aws_guided_event/211_joinworkshopevent.html",
	"title": "Join the workshop event",
	"tags": [],
	"description": "",
	"content": "If you are joining an AWS-hosted event, you will use a platform named AWS Workshop Studio. This platform allows AWS field teams to run Workshops, Game Days, Boot camps, Immersion Days, and other events that require hands-on access to AWS accounts or resources.\nFollow the steps below to gain access to your Workshop Studio AWS account:\nStep 1: Go to Workshop Studio Access your workshop here: Workshop Studio\nStep 2:. Choose Sign-In method. Select \u0026ldquo;Email one-time-password (OTP)\u0026rdquo; for AWS guided events. Step 3: Enter your Email Enter your email and click \u0026ldquo;Send Passcode\u0026rdquo;. Step 4: Enter OTP Check your email for a message titled \u0026ldquo;Verify your AWS Training and Certification email address\u0026rdquo;. Copy the passcode and enter it here. Click Sign in. Step 5: Enter Event Access Code (If prompted) Paste the 12-character code from your organizer and click \u0026ldquo;Next.\u0026rdquo; Step 6: Agree to Terms and Join Event Review the Terms and Conditions, check the box, and click \u0026ldquo;Join event. "
},
{
	"uri": "/2_setup_configuration/21_aws_guided_event/212_accessingide.html",
	"title": "Accessing your IDE",
	"tags": [],
	"description": "",
	"content": "Access your cloud-based code-server IDE For this workshop, you\u0026rsquo;ll use Visual Studio Code running on an Amazon EC2 instance via code-server. This provides a consistent development environment with all tools pre-configured.\nOpen the Workshop Studio event dashboard\nNavigate to the Event Outputs panel at the bottom of the page.\nCopy the Password üîë - you\u0026rsquo;ll need this to authenticate.\nClick on the URL to open your cloud IDE in a new tab.\nIn the Welcome to code-server dialog, paste the password you copied and choose Submit.\nTip: If the login fails, verify you copied the complete password without extra spaces.\nYou should now see the code-server IDE interface with the file explorer on the left.\nPre-packaged components Your cloud IDE comes with all necessary tools pre-installed:\nTool Purpose Senzing SDK (version 4.0) Entity resolution engine - installed and configured with an empty database Amazon Q extension for Visual Studio Code IDE extension that provides AI-powered code suggestions, chat assistance, and MCP server support senzing-mcp-server Model Context Protocol server that connects Amazon Q to Senzing capabilities üìÅ Workshop Materials Location: Workshop files and Senzing resources are located in /home/ubuntu/\nWrap up You have successfully accessed your VS code-server IDE, which has been pre-configured for the workshop. Please move to the next section where you will authenticate with the Amazon Q Developer extension and CLI.\nNext Step: Proceed to Amazon Q Authentication Setup to connect to Amazon Q Developer.\n"
},
{
	"uri": "/2_setup_configuration/22_own_aws_account/221_configureide.html",
	"title": "Configure your IDE",
	"tags": [],
	"description": "",
	"content": "Prerequisites For this workshop, you\u0026rsquo;ll need:\nTool Purpose Visual Studio Code Integrated development environment (IDE) for writing, editing, and debugging code AWS CLI Command-line interface for AWS services, needed to interact with AWS resources Amazon Q CLI Command-line interface for Amazon Q Developer, enabling AI assistance directly from the terminal Amazon Q extension for Visual Studio Code IDE extension that provides AI-powered code suggestions, chat assistance, and security scanning Git Version control system for managing workshop code and tracking changes Setup Instructions Clone or download the workshop repository:\ngit clone https://github.com/your-org/senzing-workshop.git cd senzing-workshop Open the workshop folder in VS Code (File ‚Üí Open Folder).\nInstall the Amazon Q extension:\nYou may follow instructions at: Installing the Q Developer Extension or Click the Extensions icon in the left sidebar Search for \u0026ldquo;Amazon Q\u0026rdquo; and click Install Next Steps You\u0026rsquo;ve successfully configured your IDE with all prerequisites. Proceed to Connect using AWS Builder ID to connect to Amazon Q.\n"
},
{
	"uri": "/2_setup_configuration/25_senzingmcpsetup/251_configuremcpserver.html",
	"title": "Step 1: Configure the MCP Server",
	"tags": [],
	"description": "",
	"content": "Overview The Senzing MCP server and all required environment variables are pre-configured in your workshop environment. You just need to connect it to Amazon Q Developer.\nAccess MCP Configuration Open the Amazon Q chat panel in your IDE\nClick the tools icon in the chat interface to access MCP configuration\nSelect the plus (+) symbol to add a new MCP server Choose Configuration Scope Choose your configuration scope: Global (recommended): Saves to ~/.aws/amazonq/default.json - available for all projects Local: Saves to .amazonq/default.json in current workspace only We recommend Global scope so the Senzing MCP server is available in all your projects.\nConfigure Server Settings Fill in the MCP server configuration:\nField Value Server Name Senzing Transport stdio Command /home/ubuntu/senzing-mcp-server/launch_senzing_mcp.sh Arguments (leave empty) Timeout 60000 (60 seconds) Use the full absolute path for Command as shown above. This is the pre-installed location in your workshop environment.\nAdd Environment Variables Add Environment Variables:\nClick \u0026ldquo;Add Environment Variable\u0026rdquo; three times and enter these exact values:\nVariable Name Value SENZING_ENGINE_CONFIGURATION_JSON {\u0026quot;PIPELINE\u0026quot;:{\u0026quot;CONFIGPATH\u0026quot;:\u0026quot;/etc/opt/senzing\u0026quot;,\u0026quot;RESOURCEPATH\u0026quot;:\u0026quot;/opt/senzing/er/resources\u0026quot;,\u0026quot;SUPPORTPATH\u0026quot;:\u0026quot;/opt/senzing/data\u0026quot;},\u0026quot;SQL\u0026quot;:{\u0026quot;CONNECTION\u0026quot;:\u0026quot;sqlite3://na:na@/home/ubuntu/sz_sqlite/G2C.db\u0026quot;}} LD_LIBRARY_PATH /opt/senzing/er/lib PYTHONPATH /home/ubuntu/.local/bin:/opt/senzing/er/sdk/python Copy and paste each value exactly as shown. The SENZING_ENGINE_CONFIGURATION_JSON value should be on a single line.\nSave Configuration Save the configuration Press the Save button to store your MCP server settings.\nIf you don\u0026rsquo;t see the save button, you may see an error instead.\nMake sure you\u0026rsquo;re adding each environment variable as a separate key-value pair. Each variable name and its value should be entered individually, not combined as JSON.\nTroubleshooting If you encounter issues:\nCommand path error: Verify the path /home/ubuntu/senzing-mcp-server/launch_senzing_mcp.sh exists Environment variables error: Ensure each variable is entered separately, not as a JSON object Timeout issues: The 60-second timeout should be sufficient; if not, check Senzing installation Once you see a successful save (no error message), proceed to authorize the MCP tools.\n"
},
{
	"uri": "/2_setup_configuration/25_senzingmcpsetup/252_authorizetools.html",
	"title": "Step 2: Authorize MCP Tools",
	"tags": [],
	"description": "",
	"content": "Overview After saving your MCP server configuration, Amazon Q will automatically detect the Senzing MCP tools and prompt you to authorize them.\nAuthorization Prompt You\u0026rsquo;ll see a prompt showing all available Senzing tools:\nChoose Authorization Level Choose your authorization preference:\nAlways allow: Recommended for workshop - allows Q to use these tools automatically Ask: You\u0026rsquo;ll be prompted before each tool use (more manual but gives you control) Deny: Blocks the tool from being used Recommendation Select \u0026ldquo;Always allow\u0026rdquo; for a smoother workshop experience.\nThis setting:\nEnables Amazon Q to automatically use Senzing tools when needed Eliminates repetitive authorization prompts Creates a more natural conversational AI experience Can be changed later in Amazon Q settings if needed \u0026ldquo;Always allow\u0026rdquo; is safe in this workshop environment since you control all the data and queries. In production environments with sensitive data, you might prefer \u0026ldquo;Ask\u0026rdquo; for additional oversight.\nAuthorization Scope The authorization applies to:\nAll tools provided by the Senzing MCP server Future tool additions (if the MCP server is updated) Only this MCP server (doesn\u0026rsquo;t affect other integrations) Next Steps Once you\u0026rsquo;ve authorized the tools, you\u0026rsquo;re ready to verify the integration is working correctly!\nEnsure you\u0026rsquo;ve authorized at least some Senzing tools before proceeding to verification.\n"
},
{
	"uri": "/2_setup_configuration/25_senzingmcpsetup/253_verifyintegration.html",
	"title": "Step 3: Verify the Integration",
	"tags": [],
	"description": "",
	"content": "Overview Now that you\u0026rsquo;ve configured and authorized the Senzing MCP server, let\u0026rsquo;s verify that Amazon Q can successfully access and use the Senzing tools.\nTest 1: List Available Tools In the Amazon Q chat, ask:\nWhat MCP tools are available? These tools will become more useful once you\u0026rsquo;ve loaded entity data in later modules. For now, we\u0026rsquo;re just confirming the connection works.\nTest 2: Query Test Entity Try a test query:\nGet entity1 Expected Result You should see a response indicating that entity 1 does not exist. This is expected because you haven\u0026rsquo;t loaded any data yet.\nThe important part is that:\nAmazon Q successfully called the Senzing MCP tool The tool executed without errors You received a valid response (even though the entity doesn\u0026rsquo;t exist) A response like \u0026ldquo;entity 1 does not exist\u0026rdquo; or \u0026ldquo;no entity found\u0026rdquo; confirms the MCP server is working correctly!\nWhat This Means A successful response (even if the entity doesn\u0026rsquo;t exist) indicates:\n‚úÖ Amazon Q is connected to the Senzing MCP server ‚úÖ The MCP server can communicate with Senzing ‚úÖ Environment variables are configured correctly ‚úÖ Tools are authorized and functional\nTroubleshooting If you encounter errors:\nError Possible Cause Solution \u0026ldquo;MCP server not found\u0026rdquo; Configuration not saved Revisit Step 1 \u0026ldquo;Tool not authorized\u0026rdquo; Authorization declined Revisit Step 2 \u0026ldquo;Connection timeout\u0026rdquo; Senzing not installed Contact workshop facilitator \u0026ldquo;Database error\u0026rdquo; Environment variables incorrect Verify configuration in Step 1 Wrap Up Ensure the MCP server is configured and you can see Senzing tools in Amazon Q before proceeding.\nCongratulations! You\u0026rsquo;ve successfully connected Amazon Q Developer to the Senzing MCP server!\nWhat You Can Do Now Throughout this workshop, you\u0026rsquo;ll use this integration to:\nQuery entity resolution results conversationally Understand how Senzing identified duplicate entities Analyze relationships between resolved entities Get explanations of resolution decisions Explore your mapped data interactively Ready to Continue? You\u0026rsquo;re now ready to learn about Senzing mapping concepts!\nNext Step: Proceed to Module 3: Understanding Senzing Mapping\nAdditional Resources For more details on the Senzing MCP server:\nReview /home/ubuntu/senzing-mcp-server/AMAZON_Q_SETUP.md in your environment GitHub repository: https://github.com/jbutcher21/senzing-mcp-server Amazon Q MCP Documentation: https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/mcp-ide.html "
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/431_mapinit.html",
	"title": "Stage 1: INIT",
	"tags": [],
	"description": "",
	"content": "Stage 1: INIT - Load References Start the Mapping Assistant to load the Senzing specification and examples into Q\u0026rsquo;s context.\nLaunch the Mapping Assistant Open Amazon Q Developer (if not already open)\nTell Amazon Q: Start the senzing mapping assistant\nWhy This Matters By loading these references, the AI can:\nMake informed mapping decisions based on the official specification Cite specific sections when explaining choices Validate mappings against real examples Prevent hallucination by working from authoritative sources The assistant will now confirm: \u0026quot;‚úÖ STAGE 1 COMPLETE - Ready for INVENTORY\u0026quot;\nInteractive Learning: If you\u0026rsquo;re curious about any reference file, you can ask Q to explain it. For example: \u0026ldquo;What\u0026rsquo;s in the identifier crosswalk?\u0026rdquo;\nCheckpoint: Q should confirm all 5 reference files are loaded and the linter tool is validated.\n"
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/432_mapinventory.html",
	"title": "Stage 2: INVENTORY",
	"tags": [],
	"description": "",
	"content": "Stage 2: INVENTORY - Analyze Schema Provide the customer schema you generated in Step 2 so the AI can extract all source fields.\nProvide Your Schema Tell Amazon Q: Use the customer schema I have open\n(Make sure your customer_schema.md file is open in the editor)\nThe assistant confirms: \u0026quot;‚úÖ STAGE 2 COMPLETE - All fields enumerated\u0026quot;\nAdvance to Planning When you\u0026rsquo;re satisfied with the field extraction:\nTell Amazon Q: Yes\nCheckpoint: Q should list all 19 source fields categorized by their purpose before moving to PLANNING.\n"
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/433_mapplanning.html",
	"title": "Stage 3: PLANNING",
	"tags": [],
	"description": "",
	"content": "Stage 3: PLANNING - High-Level Strategy The AI automatically moves to planning and presents a high-level mapping strategy.\nThe Mapping Strategy STAGE 3: PLANNING - High-Level Approach\nThe AI will present its strategy:\nMaster Entity: Customer Records\nEntity represents both persons and organizations Source contains 114 individuals + 6 companies DATA_SOURCE Code:\nProposed name: CUSTOMERS Represents the source system for these records Must be uppercase per Senzing convention Entity Logic:\ncustomer_type field controls RECORD_TYPE \u0026ldquo;I\u0026rdquo; (Individual) ‚Üí RECORD_TYPE: PERSON \u0026ldquo;C\u0026rdquo; (Company) ‚Üí RECORD_TYPE: ORGANIZATION Conditional mapping based on entity type Mapping Approach:\nSingle-pass processing - One mapper handles both types Conditional field mapping: PERSON records: customer_name ‚Üí NAME_FULL (parsed) ORGANIZATION records: customer_name ‚Üí NAME_ORG (unparsed) Special Handling: Dynamic Identifiers The AI identifies the id_type/id_number/id_country pattern and plans to map:\nPASSPORT ‚Üí PASSPORT_NUMBER + PASSPORT_COUNTRY DRIVERS_LICENSE ‚Üí DRIVERS_LICENSE_NUMBER + DRIVERS_LICENSE_STATE SSN ‚Üí SSN_NUMBER NATIONAL_ID ‚Üí NATIONAL_ID_NUMBER + NATIONAL_ID_COUNTRY The assistant confirms: \u0026quot;‚úÖ STAGE 3 COMPLETE - Strategy defined\u0026quot;\nImportant: Remember, your response may not match this exactly, as the AI\u0026rsquo;s output can vary.\nAsk Questions! This is your opportunity to clarify any strategy decisions before field mapping begins.\nYou probably see something about dynamic identifiers \u0026hellip;\nAsk Amazon Q: What are dynamic identifiers?\nGive Direction! If you don\u0026rsquo;t like the data source it assigned \u0026hellip;\nTell Amazon Q: Call the data source CUSTOMER\nIf you want to only load person records \u0026hellip;\nTell Amazon Q: Filter out the companies\nAdvance to Mapping When you understand the strategy:\nTell Amazon Q: yes\nLEARNING OPPORTUNITY: The more questions you ask the more you will learn!\nCheckpoint: You and Q should agree on the high level strategy for mapping customers.\n"
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/434_mapmapping.html",
	"title": "Stage 4: MAPPING",
	"tags": [],
	"description": "",
	"content": "Stage 4: MAPPING - Field-by-Field Decisions This is the core of the mapping process. The AI will present a complete field disposition table with confidence scores.\nThe Field Disposition Table This is the most critical stage of the mapping process. Take your time here to understand each decision the AI makes.\nLearning Opportunity: This stage offers the best opportunity to understand entity resolution concepts. Don\u0026rsquo;t rush through - ask questions about anything unclear!\nAsk Questions About the Table Structure The mapping table may look confusing at first. Ask the AI to explain the key elements:\nAsk Amazon Q: What does the REF column mean?\nAsk Amazon Q: What does the confidence score indicate?\nAsk Amazon Q: What is RECORD_TYPE and why does it matter?\nAsk Amazon Q: Explain the logic for NAME_FULL vs NAME_ORG\nThe AI will explain how it uses the Senzing specification to make each decision.\nReview Mapping Decisions Carefully While the AI gets many mappings correct, it can make mistakes based on limited context from your schema.\nCommon issues to watch for:\nAddress Mapping:\nThe AI might map address to ADDR_FULL based on sample values in your schema If your address field contains only street addresses, ADDR_LINE1 may be more appropriate Example correction: Tell Amazon Q: Change the address mapping from ADDR_FULL to ADDR_LINE1 Name Parsing:\nThe AI may suggest parsing customer_name into NAME_FIRST and NAME_LAST This works well if your data is consistent, but can fail on varied formats If you prefer unparsed names: Tell Amazon Q: Map customer_name to NAME_FULL without parsing Identifier Types:\nIf your data has dynamic identifier patterns (like id_type and id_number fields), verify the AI mapped all possible type codes. The schema generator only shows the most common values.\nAsk Amazon Q: What are all the possible values for id_type in the source data?\nOnce you have the complete list, verify each code maps to the correct Senzing feature:\nSSN ‚Üí SSN_NUMBER DL ‚Üí DRIVERS_LICENSE_NUMBER PASSPORT ‚Üí PASSPORT_NUMBER NATIONAL_ID ‚Üí NATIONAL_ID_NUMBER If the AI missed any codes: Tell Amazon Q: Add mapping for id_type='CEDULA' to NATIONAL_ID_NUMBER\nDon\u0026rsquo;t Worry About Perfection: Additional validation steps will catch issues during the linter stage. Focus on understanding the major decisions and correcting obvious problems. This workflow is designed to help you iterate quickly to a working result - which is the only proof that truly matters.\nReview and Approve Once you\u0026rsquo;ve asked your questions and made any corrections:\nTell Amazon Q: The mappings look good, proceed to validation\nLinter Validation The AI will generate sample JSON records and run the Senzing linter to validate the mapping structure. The AI can usually self-correct any errors, but may ask for clarification.\nYou Can Intervene: If the AI is doing something you don\u0026rsquo;t understand, press Escape and ask questions before it continues.\nCommon validation issue - REGISTRATION_DATE:\nIf your source data has a field called registration_date, the linter may flag it because Senzing has a reserved feature called REGISTRATION_DATE (for legal business incorporation dates).\nIf this happens:\nAsk Amazon Q: Does our customer registration date qualify as the Senzing REGISTRATION_DATE feature?\nThe AI will explain that customer registration dates don\u0026rsquo;t match the Senzing feature definition. The solution is to keep it as payload and rename it:\nTell Amazon Q: Rename registration_date to CUSTOMER_SINCE_DATE in the payload\nWhen validation succeeds, you\u0026rsquo;ll see confirmation:\nThe assistant confirms: \u0026quot;‚úÖ STAGE 4 COMPLETE - Mapping validated\u0026quot;\nAdvance to Outputs When the linter validates successfully:\nTell Amazon Q: yes\nCheckpoint: Q should present a complete field disposition table with confidence scores, generate valid sample JSON, and pass linter validation before moving to OUTPUTS.\n"
},
{
	"uri": "/4_exercise1_customers/43_mapwithassistant/435_mapoutputs.html",
	"title": "Stage 5: OUTPUTS",
	"tags": [],
	"description": "",
	"content": "Stage 5: OUTPUTS - Generate Code and Docs The AI generates three production-ready files that document and implement your mapping.\nThe Generated Files STAGE 5: OUTPUTS - Generating Implementation\nThe AI creates three files in workshop/workspace/customers/:\n1. README.md - Usage Instructions\nHow to run the mapper Command-line syntax Testing guidance Prerequisites 2. customers_mapper.md - Mapping Specification\nComplete field-by-field documentation Mapping rationale for each decision Conditional logic explanations Citations to Senzing specification sections Confidence scores for each mapping 3. customers_mapper.py - Python Implementation\nProduction-ready mapper code CSV input, JSONL output Error handling Conditional logic for PERSON vs ORGANIZATION Dynamic identifier routing ~150-200 lines of well-commented Python Review the Generated Files Navigate to workshop/workspace/customers/ and open each file:\nReview customers_mapper.py:\nCan you follow the conditional logic? Does it handle both PERSON and ORGANIZATION correctly? Is the identifier routing clear? Review customers_mapper.md:\nDoes it explain all mapping decisions? Are the citations helpful for understanding? Would a new team member understand this mapping? Review README.md:\nCan you run the mapper from these instructions? Are all prerequisites listed? Is the testing guidance clear? Production Ready: These files are designed to be checked into version control. The documentation preserves the mapping knowledge for future maintenance.\nKey Takeaways You\u0026rsquo;ve completed the AI-assisted mapping process:\n‚úÖ Loaded Senzing specifications into AI context ‚úÖ Provided your source schema for analysis ‚úÖ Reviewed and approved the mapping strategy ‚úÖ Validated each field-by-field mapping decision ‚úÖ Generated production-ready mapper code and documentation The Mapping Assistant doesn\u0026rsquo;t just generate code‚Äîit teaches you about entity resolution while guiding you through validated, best-practice mappings.\nLearning Opportunity: Please ask the AI questions as you go! This teaches you what makes a valid Senzing JSON record and common mapping mistakes to avoid.\nCheckpoint: You should have three files in workshop/workspace/customers/: customers_mapper.py, customers_mapper.md, and README.md. Review them to ensure they match your requirements.\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/631_mapinit.html",
	"title": "Stage 1: INIT",
	"tags": [],
	"description": "",
	"content": "Stage 1: Initialize the Mapping Assistant The first stage loads all reference materials and tests validation tools. This gives the AI complete knowledge of Senzing\u0026rsquo;s data format, conventions, and validation requirements.\nYour Prompt Yes, let\u0026#39;s start the AI-assisted mapping workflow for ftm.jsonl Or if starting fresh:\nLoad the Senzing Mapping Assistant and help me map ftm.jsonl to Senzing format AI Response Summary The AI performs these initialization steps:\n1. Load Reference Files The AI loads five critical reference files:\nFile Purpose Status senzing_entity_specification.md Complete feature catalog (50+ features) ‚úÖ Loaded senzing_mapping_examples.md 7 shot examples with patterns ‚úÖ Loaded lint_senzing_json.py JSON structure validator ‚úÖ Loaded \u0026amp; Tested identifier_crosswalk.json 12 canonical identifiers + aliases ‚úÖ Loaded usage_type_crosswalk.json NAME, ADDRESS, PHONE usage types ‚úÖ Loaded 2. Test Validation Tools The AI confirms the linter is functional by running a self-test:\npython3 senzing/tools/lint_senzing_json.py --help Result: ‚úÖ Tool functional and ready\n3. Confirm Schema Availability The AI verifies the schema file exists and is accessible:\nworkspace/watchlist/ftm_schema.md Result: ‚úÖ Schema available with 73 records, 5 schema types, 31 fields\nWhat the AI Now Knows After Stage 1, the AI has complete knowledge of:\nSenzing Features:\nAll 50+ feature attributes (NAME, ADDRESS, identifiers, etc.) Proper usage types and formatting conventions Special features (REL_ANCHOR, REL_POINTER, RECORD_TYPE) Mapping Patterns:\n7 complete worked examples showing common patterns How to handle names, addresses, identifiers, dates When to use features vs payload attributes Identifier Standards:\nCanonical identifier names (SSN_NUMBER, PASSPORT_NUMBER, etc.) Common aliases (SSN = social_security_number = taxpayer_id) International identifier types Validation Requirements:\nRequired fields (DATA_SOURCE, RECORD_ID) JSON structure rules Feature attribute naming conventions Validation Checkpoint Verify: The AI should explicitly list all 5 files it loaded. If it skips any or says a file is unavailable, stop and troubleshoot before proceeding.\nThe AI should confirm:\n‚úÖ All 5 reference files loaded successfully ‚úÖ Linter tool is functional ‚úÖ Schema file is accessible ‚úÖ Ready to proceed to Stage 2 Key Learning Point Why separate initialization from mapping?\nThis staged approach prevents common AI mapping errors:\nHallucination prevention: AI uses actual documentation instead of outdated training data Convention adherence: AI follows current Senzing standards, not assumptions Tool readiness: Validation tools are tested before generating output Explicit knowledge: You see exactly what the AI knows before it starts making decisions Compare to Exercise 1: In the customer mapping exercise, this stage was shown with detailed screenshots of each file being loaded. Here, we summarize the result. The underlying process is identical.\nCommon Issues and Solutions Issue Cause Solution File not found error Wrong working directory Verify you\u0026rsquo;re in the workshop root directory Linter test fails Python environment issue Check Python 3 is available: python3 --version AI skips files Prompt not clear Explicitly request: \u0026ldquo;Load all 5 reference files\u0026rdquo; AI uses old knowledge Not following prompt Restart and emphasize: \u0026ldquo;Use ONLY the loaded reference files\u0026rdquo; Next Stage Once initialization is complete, proceed to Stage 2: INVENTORY where the AI extracts all fields from the FTM schema.\nContinue to Stage 2: INVENTORY ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/632_mapinventory.html",
	"title": "Stage 2: INVENTORY",
	"tags": [],
	"description": "",
	"content": "Stage 2: Extract Complete Field Inventory The AI analyzes the schema file and extracts every single field with data types and sample values. This creates a complete inventory to map in later stages.\nCritical Quality Check: This stage includes an integrity check to prevent AI hallucination. The AI must count the fields it extracted and verify the count matches what it displayed. This prevents \u0026ldquo;phantom fields\u0026rdquo; from appearing in the mapping.\nYour Prompt Proceed to Stage 2: Extract all fields from the FTM schema Or simply:\nContinue AI Response: Complete Field Inventory The AI extracts and categorizes all 31 fields from the FTM schema:\nRoot Fields (3) id - string - Unique identifier for each record schema - string - Record type (Person, Company, Sanction, Ownership, Directorship) properties - object - Container for all entity attributes Person Attributes (15) name - array of strings - Full name(s) firstName - array of strings - First name(s) middleName - array of strings - Middle name(s) lastName - array of strings - Last name(s) birthDate - array of strings - Date of birth address - array of strings - Full address strings email - array of strings - Email addresses phone - array of strings - Phone numbers nationality - array of strings - Nationality/citizenship country - array of strings - Country association passportNumber - array of strings - Passport number(s) idNumber - array of strings - National ID number(s) taxNumber - array of strings - Tax identification number(s) gender - array of strings - Gender (Male/Female) alias - array of strings - Known aliases Additional Person Name Fields (2) previousName - array of strings - Former names weakAlias - array of strings - Possible aliases (low confidence) Company Attributes (4) name - array of strings - Organization name previousName - array of strings - Former company names jurisdiction - array of strings - Country of incorporation incorporationDate - array of strings - Legal registration date Relationship Fields (7) entity - array of strings - The subject entity in a relationship owner - array of strings - The owning entity asset - array of strings - The owned entity director - array of strings - The person in a directorship organization - array of strings - The organization in a directorship role - array of strings - Management role (President, Principal, etc.) percentage - array of strings - Ownership percentage Sanction Metadata (7) program - array of strings - Sanctions program name authority - array of strings - Sanctioning authority reason - array of strings - Reason for sanction listingDate - array of strings - Date added to sanctions list startDate - array of strings - Sanction start date endDate - array of strings - Sanction end date status - array of strings - Active/Inactive Identifier-Specific Fields (Nested in some Person records) (4) holder - array of strings - Person who holds the identifier type - array of strings - Identifier type (passport, SSN, driver\u0026rsquo;s license) number - array of strings - Identifier number country - array of strings - Issuing country Integrity Checkpoint The AI performs a critical validation:\nExtracted Field Count: 31 unique fields Displayed Field Count: 31 fields shown above\n‚úÖ INTEGRITY CHECK PASSED: 31 extracted = 31 displayed (no hallucinated fields)\nWhy This Matters: Without this check, AIs sometimes \u0026ldquo;invent\u0026rdquo; fields that seem plausible but don\u0026rsquo;t actually exist in the data. This can cause mapper code to fail or produce incorrect output. Always verify this count matches!\nKey Observations 1. Arrays Everywhere Notice every field is an array of strings. FTM format supports multiple values for most attributes:\nMultiple names (legal name, aliases, previous names) Multiple addresses (home, business, historical) Multiple identifiers (passport from multiple countries) Implication for mapping: Need to iterate through arrays when mapping to Senzing.\n2. Overloaded Field Names Some field names appear in multiple schema types with different meanings:\nname - Used in Person, Company, Sanction previousName - Used in Person and Company entity - References different types depending on schema Implication for mapping: Need schema-aware mapping logic.\n3. Nested Relationship Pointers Fields like entity, owner, asset, director, organization contain IDs referencing other records, not actual data.\nImplication for mapping: Need multi-pass processing to resolve these references.\nValidation Checkpoint Before proceeding, verify:\n‚úÖ AI extracted exactly 31 fields (not 30, not 32) ‚úÖ AI performed integrity check showing counts match ‚úÖ All field categories are shown (Root, Person, Company, Relationship, Sanction, Identifier) ‚úÖ Data types are noted (mostly arrays of strings) If the count is wrong: Stop and ask the AI to re-extract fields. Review the schema file manually to identify what was missed or hallucinated.\nCommon Issues and Solutions Issue Cause Solution Wrong field count AI hallucinated or skipped fields Ask AI to re-count and verify against schema Missing field categories Incomplete schema analysis Request AI list all record types in schema No integrity check shown AI skipped validation step Explicitly request: \u0026ldquo;Verify your field count\u0026rdquo; Fields from different dataset AI using training data Remind AI: \u0026ldquo;Extract only from ftm_schema.md\u0026rdquo; Next Stage With a complete field inventory verified, proceed to Stage 3: PLANNING where you\u0026rsquo;ll determine DATA_SOURCE codes and entity type strategy.\nContinue to Stage 3: PLANNING ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/633_mapplanning.html",
	"title": "Stage 3: PLANNING",
	"tags": [],
	"description": "",
	"content": "Stage 3: Determine DATA_SOURCE and Processing Strategy Before mapping individual fields, you need to make high-level decisions about how to structure the data in Senzing. This stage determines DATA_SOURCE codes, entity types, and the overall processing strategy.\nYour Prompt Proceed to Stage 3: Determine DATA_SOURCE codes and entity types AI Initial Response The AI analyzes the five FTM schema types and proposes a strategy:\nMaster Entity Types Master entities are the primary records that will become Senzing entities:\nSchema Type Count Maps To DATA_SOURCE Person 33 Person entities SANCTIONS Company 6 Organization entities CORP_FILINGS Relationship Types Relationship records reference master entities and will be processed differently:\nSchema Type Count Processing Strategy Sanction 17 Merge as payload attributes onto Person entities Ownership 8 Convert to REL_POINTER relationships Directorship 6 Convert to REL_POINTER relationships Your Correction Teaching Moment: The AI initially said \u0026ldquo;3 master entity types\u0026rdquo; but only listed 2 (Person and Company). This is exactly the kind of error you should catch and correct!\nYour prompt:\nYou say 3 master entity types but list 2: person and company. What is the third? AI Response:\nYou\u0026#39;re absolutely correct - that was an error on my part. Let me correct the Stage 2 summary: CORRECTED: 2 master entity types (Person, Company) 3 relationship types (Sanction, Ownership, Directorship) I conflated \u0026#34;master entities\u0026#34; with \u0026#34;relationship types\u0026#34;. Thank you for catching that! Key Learning: Always verify AI counts and categorizations. Simple arithmetic errors can indicate deeper misunderstandings that will cause problems later.\nFinal Strategy DATA_SOURCE Codes Two separate DATA_SOURCE codes reflect the different domains:\nSANCTIONS\nAll Person entities (33 records) Represents watchlist/sanctions domain RECORD_ID format: sanctions-person-{id} CORP_FILINGS\nAll Company entities (6 records) Represents corporate registry domain RECORD_ID format: corp-filings-company-{id} Why separate DATA_SOURCE codes? This allows Senzing to track which system contributed which information. When a Person from SANCTIONS matches a director in CORP_FILINGS, you can see both sources in the resolved entity.\nProcessing Strategy: Multi-Pass The FTM data requires three-pass processing because some records are metadata or relationships:\nPass 1: Master Entities\nProcess all Person and Company records Create Senzing records with REL_ANCHOR Pass 2: Sanction Metadata\nMatch Sanction records to their Person entities (via entity field) Merge sanction attributes as payload onto existing Person records Pass 3: Relationships\nProcess Ownership and Directorship records Add REL_POINTER relationships to existing entities Pass 4: Identifier Records (if present in data)\nMerge identifier details onto Person entities Entity Type Determination PERSON entities:\nAll records from Person schema type RECORD_TYPE: \u0026quot;PERSON\u0026quot; in Senzing output ORGANIZATION entities:\nAll records from Company schema type RECORD_TYPE: \u0026quot;ORGANIZATION\u0026quot; in Senzing output Validation Checkpoint Before proceeding to field mapping, confirm:\n‚úÖ Two DATA_SOURCE codes identified: SANCTIONS, CORP_FILINGS ‚úÖ Two master entity types: Person (PERSON), Company (ORGANIZATION) ‚úÖ Three relationship/metadata types: Sanction, Ownership, Directorship ‚úÖ Multi-pass processing strategy understood ‚úÖ Arithmetic errors corrected (2 master types, not 3) Key Planning Decisions Explained Why SANCTIONS and CORP_FILINGS? SANCTIONS:\nPrimary source is watchlist/sanctions data Even persons not directly sanctioned are in a sanctions-related dataset Makes it clear this data comes from compliance/risk domain CORP_FILINGS:\nCompanies come from corporate registry information Separates legal entity data from person data Allows tracking which company information came from which filing Why Not All One DATA_SOURCE? You could use a single DATA_SOURCE like \u0026ldquo;FTM_WATCHLIST\u0026rdquo;, but separating by domain provides benefits:\nClearer data provenance (which system contributed what) Better cross-source matching visibility Easier debugging when issues arise More accurate record counts by source system Why Multi-Pass Processing? Single-pass processing won\u0026rsquo;t work because:\nSanction records reference Person IDs that must exist first Ownership records reference Company IDs Directorship records reference both Person and Company IDs Processing in passes ensures all referenced entities exist before relationships are added.\nCommon Issues and Solutions Issue Cause Solution Too many DATA_SOURCE codes Over-segmentation Group by logical source system, not record type Unclear entity vs metadata Confused relationship types Ask: \u0026ldquo;Does this become its own Senzing entity?\u0026rdquo; Single-pass assumption Missed relationship pointers Check for fields that reference other record IDs Arithmetic doesn\u0026rsquo;t match AI counting error Verify counts against schema file Next Stage With the high-level strategy confirmed, proceed to Stage 4: MAPPING where you\u0026rsquo;ll make field-by-field disposition decisions.\nContinue to Stage 4: MAPPING ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/634_mapmapping.html",
	"title": "Stage 4: MAPPING",
	"tags": [],
	"description": "",
	"content": "Stage 4: Field-by-Field Mapping Decisions This is the core mapping stage where every field gets a disposition: Feature (used for matching), Payload (stored but not matched), or Ignored (not needed).\nThis is where your domain expertise matters most. The AI can suggest mappings based on Senzing patterns, but you need to verify they make business sense for your use case.\nYour Prompt Show me the full mapping table for all 31 fields Or:\nProceed to Stage 4: Map each field to Senzing format AI Response: Initial Mapping Table The AI presents a complete mapping table. Here\u0026rsquo;s the summary:\nSummary Counts Features: 18 fields mapped to Senzing matching features Payload: 6 fields mapped to payload attributes (stored, not matched) Ignored: 7 fields (containers, pointers, type indicators) Key Mapping Decisions Person Entity Features Source Field Senzing Feature Notes firstName NAME_FIRST Standard name feature middleName NAME_MIDDLE Standard name feature lastName NAME_LAST Standard name feature name (full) NAME_FULL When first/middle/last not available alias NAME_FULL (usage: ALIAS) Known aliases previousName NAME_FULL (usage: MAIDEN) Former names birthDate DATE_OF_BIRTH Standard demographic gender GENDER Standard demographic address ADDR_FULL Full address strings email EMAIL_ADDRESS Contact feature phone PHONE_NUMBER Contact feature passportNumber PASSPORT_NUMBER + PASSPORT_COUNTRY Identifier feature idNumber NATIONAL_ID_NUMBER + NATIONAL_ID_COUNTRY Identifier feature taxNumber TAX_ID_NUMBER + TAX_ID_COUNTRY Identifier feature Company Entity Features Source Field Senzing Feature Notes name NAME_ORG Organization name previousName NAME_ORG (usage: PRIOR) CORRECTED (see below) jurisdiction ADDR_COUNTRY Country of incorporation incorporationDate (Payload) See discussion below Payload Attributes (Not Used for Matching) Source Field Payload Attribute Reason nationality NATIONALITY Operational data country COUNTRY Operational data weakAlias WEAK_ALIAS Low confidence, don\u0026rsquo;t match on it incorporationDate INCORPORATION_DATE Operational data Sanction fields SANCTION_* Metadata merged onto Person entities Relationship Handling Ownership and Directorship records:\nNot mapped to features or payload Converted to REL_POINTER relationships in mapper code Roles: OWNER_OF, PRINCIPAL_OF, PRESIDENT_OF Interactive Corrections During this stage, several important corrections were made based on questions and data verification:\nCorrection 1: Identifier Types Your Question:\nAre you sure those are all the identifier types, and how do you know? AI Initial Response:\nI assumed based on the schema fields (passportNumber, idNumber, taxNumber). Your Challenge:\nYou should check the actual source data, not just the schema. AI Verification: After checking actual FTM records, the AI found nested identifier records:\n1 Driver\u0026rsquo;s License record ‚Üí Maps to DRIVERS_LICENSE_NUMBER + DRIVERS_LICENSE_STATE 2 SSN records ‚Üí Maps to SSN_NUMBER Critical Lesson: Always verify AI assumptions against actual data! The schema showed general identifier fields (idNumber, passportNumber), but nested identifier records had specific types that required different Senzing features.\nCorrection 2: Company Relationships Your Question:\nI didn\u0026#39;t see any rel_pointers in the json examples, don\u0026#39;t companies have relationships as well? AI Initial Response:\nOwnership records show owner‚Üíasset (person owning company). I didn\u0026#39;t check company‚Üícompany ownership. Your Discovery: After checking actual data, you found:\nUniversal Exports Worldwide owns 3 subsidiary companies Company-to-company ownership relationships exist All use REL_POINTER_ROLE = \u0026quot;OWNER_OF\u0026quot; Key Learning: Don\u0026rsquo;t assume relationship patterns. Check actual data to find all variations:\nPerson ‚Üí Company (directorship) Person ‚Üí Company (ownership) Company ‚Üí Company (ownership) Correction 3: Relationship Roles Your Question:\nWhat are my options for assigning roles to relationship pointers? AI Response: Based on the Senzing specification, standard roles include:\nPerson ‚Üî Organization:\nEMPLOYED_BY PRINCIPAL_OF OWNER_OF DIRECTOR_OF Organization ‚Üî Organization:\nBRANCH_OF DIRECT_PARENT ULTIMATE_PARENT Your Decision:\nUse PRINCIPAL_OF for principals and PRESIDENT_OF for presidents Final Role Mappings:\nAll Ownership ‚Üí \u0026quot;OWNER_OF\u0026quot; Directorship with role=\u0026ldquo;Principal\u0026rdquo; ‚Üí \u0026quot;PRINCIPAL_OF\u0026quot; Directorship with role=\u0026ldquo;President\u0026rdquo; ‚Üí \u0026quot;PRESIDENT_OF\u0026quot; Role Precision: Using specific roles (PRESIDENT_OF vs generic DIRECTOR_OF) provides better relationship clarity and supports more precise network analysis.\nCorrection 4: Previous Names for Organizations Your Correction:\nprevious name should be name_org, not name_full AI Response:\nYou\u0026#39;re absolutely right! For organizations, previous names should use NAME_ORG, not NAME_FULL. NAME_FULL is for person names. Corrected Mapping:\nCompany previousName ‚Üí NAME_ORG with usage type PRIOR Senzing Convention: NAME_FULL is for people, NAME_ORG is for organizations. Using the wrong feature can degrade matching quality.\nQuestion 5: Company Identifiers Your Question:\nAren\u0026#39;t there any identifiers for companies? AI Response: After checking the actual FTM data, no company identifier fields were found in this dataset. Companies only have:\nOrganization name Previous names Jurisdiction (country) Incorporation date Dataset-Specific: Not all datasets have all field types. This watchlist happens to lack company identifiers (tax IDs, registration numbers). Other datasets might include them.\nComplete Mapping Table Here\u0026rsquo;s the final mapping table after all corrections:\nPerson Entities (SANCTIONS) FTM Field Senzing Disposition Feature/Attribute Usage Type id Required RECORD_ID - schema Required RECORD_TYPE = \u0026quot;PERSON\u0026quot; - firstName Feature NAME_FIRST - middleName Feature NAME_MIDDLE - lastName Feature NAME_LAST - name Feature NAME_FULL - alias Feature NAME_FULL ALIAS previousName Feature NAME_FULL MAIDEN birthDate Feature DATE_OF_BIRTH - gender Feature GENDER - address Feature ADDR_FULL - email Feature EMAIL_ADDRESS - phone Feature PHONE_NUMBER - passportNumber Feature PASSPORT_NUMBER + PASSPORT_COUNTRY - idNumber Feature NATIONAL_ID_NUMBER + NATIONAL_ID_COUNTRY - taxNumber Feature TAX_ID_NUMBER + TAX_ID_COUNTRY - nationality Payload NATIONALITY - country Payload COUNTRY - weakAlias Payload WEAK_ALIAS - Nested Identifiers SSN identifier Feature SSN_NUMBER - Driver\u0026rsquo;s License Feature DRIVERS_LICENSE_NUMBER + DRIVERS_LICENSE_STATE - Merged Sanctions program Payload SANCTION_PROGRAM - authority Payload SANCTION_AUTHORITY - reason Payload SANCTION_REASON - listingDate Payload SANCTION_LISTING_DATE - status Payload SANCTION_STATUS - Relationships REL_ANCHOR Feature REL_ANCHOR_DOMAIN + REL_ANCHOR_KEY - Ownership/Directorship Feature REL_POINTER_DOMAIN + REL_POINTER_KEY + REL_POINTER_ROLE - Company Entities (CORP_FILINGS) FTM Field Senzing Disposition Feature/Attribute Usage Type id Required RECORD_ID - schema Required RECORD_TYPE = \u0026quot;ORGANIZATION\u0026quot; - name Feature NAME_ORG - previousName Feature NAME_ORG PRIOR jurisdiction Feature ADDR_COUNTRY - incorporationDate Payload INCORPORATION_DATE - Relationships REL_ANCHOR Feature REL_ANCHOR_DOMAIN + REL_ANCHOR_KEY - Ownership Feature REL_POINTER_DOMAIN + REL_POINTER_KEY + REL_POINTER_ROLE - Validation Checkpoint Before proceeding to Stage 5, verify:\n‚úÖ All 31 fields have a disposition (Feature, Payload, or Ignored) ‚úÖ Identifier types verified against actual data (not just schema) ‚úÖ Company relationship patterns identified ‚úÖ Relationship roles assigned (OWNER_OF, PRINCIPAL_OF, PRESIDENT_OF) ‚úÖ NAME_ORG used for organization names (not NAME_FULL) ‚úÖ REL_ANCHOR and REL_POINTER strategy defined Key Learnings from This Stage 1. Verify Assumptions The AI will make reasonable assumptions, but always verify against actual data:\nIdentifier types weren\u0026rsquo;t in the schema fields - they were in nested records Company-to-company relationships weren\u0026rsquo;t obvious until checking the data 2. Domain Knowledge Matters You caught important errors that would have degraded matching:\nNAME_ORG vs NAME_FULL for organizations Specific relationship roles vs generic ones 3. Interactive Mapping is Best This wasn\u0026rsquo;t a one-shot prompt. It was an interactive dialogue:\nAI proposed mappings You questioned assumptions AI verified against data You confirmed or corrected Repeat until mapping is complete This produces much higher quality mappings than asking AI to \u0026ldquo;just do it all.\u0026rdquo;\nNext Stage With field mappings finalized, proceed to Stage 5: OUTPUTS where the AI generates mapper code, documentation, and sample JSON.\nContinue to Stage 5: OUTPUTS ‚Üí\n"
},
{
	"uri": "/6_exercise2_watchlist/63_mapwithassistant/635_mapoutputs.html",
	"title": "Stage 5: OUTPUTS",
	"tags": [],
	"description": "",
	"content": "Stage 5: Generate Mapper Code and Documentation The final stage generates three critical outputs: a mapper Python script, complete mapping documentation, and sample JSON with validation.\nYour Prompt Generate the mapper code, documentation, and sample JSON Or:\nProceed to Stage 5: Generate outputs AI Response: Three Output Files The AI generates complete, production-ready files:\n1. README.md - Usage Instructions Purpose: Quick start guide for using the mapper\nContents:\nHow to run the mapper Expected inputs and outputs Validation commands Loading instructions Example excerpt:\n# FTM Watchlist Mapper ## Usage python3 ftm_mapper.py ftm.jsonl ftm_senzing.jsonl ## Validation python3 senzing/tools/lint_senzing_json.py ftm_senzing.jsonl python3 senzing/tools/sz_json_analyzer.py ftm_senzing.jsonl -o analysis.md ## Loading sz_file_loader -f ftm_senzing.jsonl 2. ftm_mapper.md - Complete Mapping Specification Purpose: Human-readable mapping documentation (source of truth)\nContents:\nAll field mapping decisions Processing logic explanation DATA_SOURCE strategy Relationship handling approach Sample input ‚Üí output examples Why this matters:\nDocuments business logic for auditing Helps others understand mapper without reading code Serves as test specification Can be used to regenerate mapper if needed 3. ftm_mapper.py - Python Implementation Purpose: Executable mapper that transforms FTM ‚Üí Senzing JSON\nKey implementation details:\nMulti-Pass Processing # Pass 1: Process master entities (Person and Company) for record in ftm_records: if record[\u0026#39;schema\u0026#39;] == \u0026#39;Person\u0026#39;: senzing_entities[record[\u0026#39;id\u0026#39;]] = map_person(record) elif record[\u0026#39;schema\u0026#39;] == \u0026#39;Company\u0026#39;: senzing_entities[record[\u0026#39;id\u0026#39;]] = map_company(record) # Pass 2: Merge Sanction metadata onto Person entities for record in ftm_records: if record[\u0026#39;schema\u0026#39;] == \u0026#39;Sanction\u0026#39;: entity_id = record[\u0026#39;properties\u0026#39;][\u0026#39;entity\u0026#39;][0] merge_sanction(senzing_entities[entity_id], record) # Pass 3: Add Ownership relationships for record in ftm_records: if record[\u0026#39;schema\u0026#39;] == \u0026#39;Ownership\u0026#39;: owner_id = record[\u0026#39;properties\u0026#39;][\u0026#39;owner\u0026#39;][0] asset_id = record[\u0026#39;properties\u0026#39;][\u0026#39;asset\u0026#39;][0] add_rel_pointer(senzing_entities[owner_id], asset_id, \u0026#39;OWNER_OF\u0026#39;) # Pass 4: Add Directorship relationships for record in ftm_records: if record[\u0026#39;schema\u0026#39;] == \u0026#39;Directorship\u0026#39;: director_id = record[\u0026#39;properties\u0026#39;][\u0026#39;director\u0026#39;][0] org_id = record[\u0026#39;properties\u0026#39;][\u0026#39;organization\u0026#39;][0] role = map_role(record[\u0026#39;properties\u0026#39;][\u0026#39;role\u0026#39;][0]) # PRINCIPAL_OF or PRESIDENT_OF add_rel_pointer(senzing_entities[director_id], org_id, role) Array Handling FTM properties are arrays, so the mapper iterates through values:\ndef map_person(ftm_record): features = [] # Handle multiple names for first_name in ftm_record[\u0026#39;properties\u0026#39;].get(\u0026#39;firstName\u0026#39;, []): for last_name in ftm_record[\u0026#39;properties\u0026#39;].get(\u0026#39;lastName\u0026#39;, []): features.append({ \u0026#39;NAME_FIRST\u0026#39;: first_name, \u0026#39;NAME_LAST\u0026#39;: last_name }) # Handle multiple phone numbers for phone in ftm_record[\u0026#39;properties\u0026#39;].get(\u0026#39;phone\u0026#39;, []): features.append({\u0026#39;PHONE_NUMBER\u0026#39;: phone}) return { \u0026#39;DATA_SOURCE\u0026#39;: \u0026#39;SANCTIONS\u0026#39;, \u0026#39;RECORD_ID\u0026#39;: f\u0026#34;sanctions-person-{ftm_record[\u0026#39;id\u0026#39;]}\u0026#34;, \u0026#39;FEATURES\u0026#39;: features } REL_ANCHOR and REL_POINTER All master entities get a REL_ANCHOR so they can be referenced:\nfeatures.append({ \u0026#39;REL_ANCHOR_DOMAIN\u0026#39;: \u0026#39;SANCTIONS\u0026#39;, # or \u0026#39;CORP_FILINGS\u0026#39; \u0026#39;REL_ANCHOR_KEY\u0026#39;: record[\u0026#39;id\u0026#39;] }) Relationships use REL_POINTER to reference other entities:\nfeatures.append({ \u0026#39;REL_POINTER_DOMAIN\u0026#39;: \u0026#39;CORP_FILINGS\u0026#39;, \u0026#39;REL_POINTER_KEY\u0026#39;: \u0026#39;company-123\u0026#39;, \u0026#39;REL_POINTER_ROLE\u0026#39;: \u0026#39;OWNER_OF\u0026#39; }) Sample JSON Output with Validation The AI generates sample Senzing JSON and validates it with the linter.\nSample Person Entity (with merged sanction + identifier) { \u0026#34;DATA_SOURCE\u0026#34;: \u0026#34;SANCTIONS\u0026#34;, \u0026#34;RECORD_ID\u0026#34;: \u0026#34;sanctions-person-1006\u0026#34;, \u0026#34;FEATURES\u0026#34;: [ { \u0026#34;RECORD_TYPE\u0026#34;: \u0026#34;PERSON\u0026#34; }, { \u0026#34;NAME_FIRST\u0026#34;: \u0026#34;Robert\u0026#34;, \u0026#34;NAME_LAST\u0026#34;: \u0026#34;Smith Sr\u0026#34;, \u0026#34;NAME_MIDDLE\u0026#34;: \u0026#34;E\u0026#34; }, { \u0026#34;DATE_OF_BIRTH\u0026#34;: \u0026#34;1954-03-31\u0026#34; }, { \u0026#34;ADDR_FULL\u0026#34;: \u0026#34;123 Main St, Las Vegas\u0026#34; }, { \u0026#34;DRIVERS_LICENSE_NUMBER\u0026#34;: \u0026#34;112233\u0026#34;, \u0026#34;DRIVERS_LICENSE_STATE\u0026#34;: \u0026#34;NV\u0026#34; }, { \u0026#34;REL_ANCHOR_DOMAIN\u0026#34;: \u0026#34;SANCTIONS\u0026#34;, \u0026#34;REL_ANCHOR_KEY\u0026#34;: \u0026#34;sanctions-person-1006\u0026#34; } ], \u0026#34;SANCTION_PROGRAM\u0026#34;: \u0026#34;SANCTIONS\u0026#34;, \u0026#34;SANCTION_AUTHORITY\u0026#34;: \u0026#34;Sanctions Authority\u0026#34;, \u0026#34;SANCTION_REASON\u0026#34;: \u0026#34;Category: Fraud\u0026#34;, \u0026#34;SANCTION_LISTING_DATE\u0026#34;: \u0026#34;2017-01-03\u0026#34;, \u0026#34;SANCTION_STATUS\u0026#34;: \u0026#34;Active\u0026#34; } Sample Company Entity (with relationships) { \u0026#34;DATA_SOURCE\u0026#34;: \u0026#34;CORP_FILINGS\u0026#34;, \u0026#34;RECORD_ID\u0026#34;: \u0026#34;corp-filings-company-3088\u0026#34;, \u0026#34;FEATURES\u0026#34;: [ { \u0026#34;RECORD_TYPE\u0026#34;: \u0026#34;ORGANIZATION\u0026#34; }, { \u0026#34;NAME_ORG\u0026#34;: \u0026#34;Universal Exports Worldwide\u0026#34; }, { \u0026#34;NAME_ORG\u0026#34;: \u0026#34;Universal Trading Corp\u0026#34;, \u0026#34;USAGE_TYPE\u0026#34;: \u0026#34;PRIOR\u0026#34; }, { \u0026#34;ADDR_COUNTRY\u0026#34;: \u0026#34;Germany\u0026#34; }, { \u0026#34;REL_ANCHOR_DOMAIN\u0026#34;: \u0026#34;CORP_FILINGS\u0026#34;, \u0026#34;REL_ANCHOR_KEY\u0026#34;: \u0026#34;corp-filings-company-3088\u0026#34; }, { \u0026#34;REL_POINTER_DOMAIN\u0026#34;: \u0026#34;CORP_FILINGS\u0026#34;, \u0026#34;REL_POINTER_KEY\u0026#34;: \u0026#34;corp-filings-company-3089\u0026#34;, \u0026#34;REL_POINTER_ROLE\u0026#34;: \u0026#34;OWNER_OF\u0026#34; } ], \u0026#34;INCORPORATION_DATE\u0026#34;: \u0026#34;1998-05-15\u0026#34; } Linter Validation python3 senzing/tools/lint_senzing_json.py sample_output.jsonl Result: ‚úÖ PASSED - No JSON syntax errors\nAlways lint sample output during development! Finding structural errors early prevents having to fix broken mapper code later.\nValidation Checkpoint Before running the mapper on actual data, verify:\n‚úÖ README.md contains usage instructions ‚úÖ ftm_mapper.md documents all mapping decisions ‚úÖ ftm_mapper.py implements multi-pass processing ‚úÖ Sample Person JSON with merged sanction metadata ‚úÖ Sample Company JSON with REL_POINTER relationships ‚úÖ Sample output passes linter validation Common Implementation Issues Issue Cause Solution Missing relationships Single-pass processing Implement multi-pass: masters first, then relationships Array indexing errors Assuming single values Always iterate: for value in array: KeyError exceptions Missing field checks Use .get(field, []) for optional fields REL_POINTER to non-existent entity Pass ordering wrong Process targets before pointers NAME_FULL used for companies Copy-paste from Person code Use NAME_ORG for organizations Key Learnings from This Stage 1. Multi-Pass is Essential You cannot process FTM data in a single pass because:\nSanction records reference Person IDs Ownership records reference Company IDs Relationships require both entities to exist first 2. Documentation Matters The .md mapping specification is as important as the code:\nCode can have bugs Documentation explains why decisions were made Makes mapper maintainable by others 3. Validate Early Running the linter on sample output catches issues before processing 73 records:\nMissing required fields Incorrect JSON structure Typos in feature names 4. FTM Arrays Require Iteration Every property is an array, even if most records have single values:\n# WRONG - causes errors when array is empty or has multiple values name = record[\u0026#39;properties\u0026#39;][\u0026#39;firstName\u0026#39;][0] # RIGHT - handles empty, single, or multiple values for name in record[\u0026#39;properties\u0026#39;].get(\u0026#39;firstName\u0026#39;, []): # process each name Next Steps Now that you have working mapper code and documentation:\nRun the mapper on actual FTM data Validate output with linter and analyzer tools Configure data sources in Senzing Load the data into Senzing Analyze results to see entity resolution in action Continue to Step 4: Validate Mapping ‚Üí\n"
},
{
	"uri": "/categories/index.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/index.html",
	"title": "One-click Entity Resolution",
	"tags": [],
	"description": "",
	"content": "One-click Entity Resolution on AWS using Amazon Q Developer and Senzing (you can\u0026rsquo;t unsee this!) What is Entity Resolution? Entity resolution identifies when different records refer to the same real-world entity across your data sources. Without it, your organization faces:\nDuplicate customers costing 10-25% of revenue through poor experiences Compliance risks from incomplete customer views Missed opportunities from fragmented data insights Operational inefficiency from manual data cleanup Learn more about entity resolution\nTransform Your Data Operations in Hours, Not Months In this hands-on workshop, you\u0026rsquo;ll learn how to leverage AI tools like Amazon Q Developer to map, load, and explore your data with Senzing entity resolution using a structured, repeatable workflow that delivers production-ready results.\nLearning Outcomes By completing this workshop, you will:\nUnderstand how to map source data to Senzing format using the 5-stage AI-assisted workflow Know how to generate production-ready mapper code and documentation automatically Learn to validate mappings using proven Senzing tools both before and after mapping Be able to load mapped data and analyze entity resolution results Master the Senzing Mapping Assistant workflow for use with your own data sources Intended audience This workshop is designed for software developers, data engineers, and anyone interested in entity resolution who understands ETL (Extract, Transform, Load) workflows.\nYou should have:\nBasic familiarity with IDEs (Visual Studio Code or code-server) Understanding of data formats (CSV, JSON) Experience with data transformation concepts No prior Senzing experience required Workshop Details Cost: Free for AWS-hosted events\nDuration: Estimated 1-2 hours\n"
},
{
	"uri": "/tags/index.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]